{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2-jp_sample_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbImtK8f9_uW",
        "outputId": "c39c1ed3-a5dc-4f0e-84d3-65007b3b0a53"
      },
      "source": [
        "# データの永続化\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p '/content/drive/My Drive/work/'\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcup5odp-cY1",
        "outputId": "a3a563a1-8da5-4037-f70d-4e1b0e94e9d7"
      },
      "source": [
        "# ソースからのHuggingface Transformersのインストール\n",
        "!git clone https://github.com/huggingface/transformers -b v4.4.2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 74620, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 74620 (delta 19), reused 27 (delta 9), pack-reused 74583\u001b[K\n",
            "Receiving objects: 100% (74620/74620), 57.34 MiB | 7.78 MiB/s, done.\n",
            "Resolving deltas: 100% (53148/53148), done.\n",
            "Note: checking out '9f43a425fe89cfc0e9b9aa7abd7dd44bcaccd79a'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (1125/1125), done.\n",
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyKVRuEiAzgv",
        "outputId": "b4c1e433-f855-4c61-d899-fed94bf7f09e"
      },
      "source": [
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# 作業フォルダに戻る\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPeugn18BG-2",
        "outputId": "f91c45af-80d1-4862-aa79-56443d104998"
      },
      "source": [
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n",
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, datasets\n",
            "Successfully installed datasets-1.2.1 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpVz34lwBJbn",
        "outputId": "3c468f21-36df-494f-8e47-7b3f9a59d30e"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=sample_train.txt \\\n",
        "    --validation_file=sample_train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=sample_output/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 07:20:21.132854: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/25/2021 07:20:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/25/2021 07:20:26 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=sample_output/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jul25_07-20-26_8a27c85065d2, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=sample_output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 2.74MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-d10c791c1b5e5bd3 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-d10c791c1b5e5bd3/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-d10c791c1b5e5bd3/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-07-25 07:20:27,693 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeg7ojtz_\n",
            "Downloading: 100% 654/654 [00:00<00:00, 680kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-25 07:20:28,125 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|file_utils.py:1393] 2021-07-25 07:20:28,125 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:463] 2021-07-25 07:20:28,125 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:499] 2021-07-25 07:20:28,126 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-07-25 07:20:28,408 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgvzyv6_p\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 2.21MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-25 07:20:29,334 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-07-25 07:20:29,334 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-07-25 07:20:29,901 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8wnpml6j\n",
            "Downloading: 100% 153/153 [00:00<00:00, 116kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-25 07:20:30,187 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-07-25 07:20:30,187 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-07-25 07:20:30,469 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfl2gw0on\n",
            "Downloading: 100% 225/225 [00:00<00:00, 178kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-25 07:20:30,752 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|file_utils.py:1393] 2021-07-25 07:20:30,752 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-25 07:20:31,037 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-25 07:20:31,037 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-25 07:20:31,037 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-25 07:20:31,037 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-25 07:20:31,037 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-07-25 07:20:31,382 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyufmsjas\n",
            "Downloading: 100% 1.37G/1.37G [00:30<00:00, 44.4MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-25 07:21:02,601 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|file_utils.py:1393] 2021-07-25 07:21:02,601 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1051] 2021-07-25 07:21:02,601 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1167] 2021-07-25 07:21:12,030 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-07-25 07:21:12,030 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 17.51ba/s]\n",
            "100% 1/1 [00:00<00:00, 18.84ba/s]\n",
            "./transformers/examples/language-modeling/run_clm.py:334: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
            "  f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
            "07/25/2021 07:21:12 - WARNING - __main__ -   The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
            "100% 1/1 [00:00<00:00, 26.92ba/s]\n",
            "100% 1/1 [00:00<00:00, 28.65ba/s]\n",
            "[INFO|trainer.py:946] 2021-07-25 07:21:19,174 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-07-25 07:21:19,174 >>   Num examples = 14\n",
            "[INFO|trainer.py:948] 2021-07-25 07:21:19,174 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:949] 2021-07-25 07:21:19,174 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:950] 2021-07-25 07:21:19,174 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:951] 2021-07-25 07:21:19,174 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-07-25 07:21:19,174 >>   Total optimization steps = 42\n",
            "100% 42/42 [00:24<00:00,  1.73it/s][INFO|trainer.py:1129] 2021-07-25 07:21:43,817 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 24.6426, 'train_samples_per_second': 1.704, 'epoch': 3.0}\n",
            "100% 42/42 [00:24<00:00,  1.72it/s]\n",
            "[INFO|trainer.py:1558] 2021-07-25 07:21:44,229 >> Saving model checkpoint to sample_output/\n",
            "[INFO|configuration_utils.py:314] 2021-07-25 07:21:44,233 >> Configuration saved in sample_output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-07-25 07:21:49,323 >> Model weights saved in sample_output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-07-25 07:21:49,787 >> tokenizer config file saved in sample_output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-07-25 07:21:49,792 >> Special tokens file saved in sample_output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-07-25 07:21:49,801 >> Copy vocab file to sample_output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-07-25 07:21:49,809 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,809 >>   epoch                      =     3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,809 >>   init_mem_cpu_alloc_delta   =     1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,809 >>   init_mem_cpu_peaked_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   init_mem_gpu_alloc_delta   =  1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   init_mem_gpu_peaked_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_mem_cpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_mem_cpu_peaked_delta =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_mem_gpu_alloc_delta  =  3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_mem_gpu_peaked_delta =  8262MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_runtime              = 24.6426\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_samples              =      14\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:49,810 >>   train_samples_per_second   =   1.704\n",
            "07/25/2021 07:21:49 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1775] 2021-07-25 07:21:49,933 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1776] 2021-07-25 07:21:49,933 >>   Num examples = 14\n",
            "[INFO|trainer.py:1777] 2021-07-25 07:21:49,933 >>   Batch size = 1\n",
            "100% 14/14 [00:02<00:00,  6.02it/s]\n",
            "[INFO|trainer_pt_utils.py:656] 2021-07-25 07:21:53,024 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   epoch                     =     3.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_loss                 =  2.5756\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_mem_cpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_mem_cpu_peaked_delta =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_mem_gpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_mem_gpu_peaked_delta =   542MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_runtime              =  2.9667\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_samples              =      14\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   eval_samples_per_second   =   4.719\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-25 07:21:53,024 >>   perplexity                = 13.1394\n",
            "CPU times: user 1.14 s, sys: 286 ms, total: 1.43 s\n",
            "Wall time: 2min 18s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdn6tR5vDi3B",
        "outputId": "c1d2c836-a0a5-4361-b7c2-94c82bdab051"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sample_output/\")\n",
        "\n",
        "# 推論\n",
        "input = tokenizer.encode(\"おはよう、お兄ちゃん。\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=8)\n",
        "print(tokenizer.batch_decode(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['おはよう、お兄ちゃん。</s> 今日のお兄ちゃんは、昨夜、少しだけ... こんにちは、お兄ちゃん。 お兄ちゃんは...、...、今夜...、...、オナニーして...、... こんにちは、お兄ちゃん。 今夜、お兄ちゃんと...、オナニーして...、... こんにちは、お兄ちゃん。 お兄ちゃんは...、今晩、オナニーして...、...、オ', 'おはよう、お兄ちゃん。</s> いいものを見つけちゃった!「 いーお。 」</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>', 'おはよう、お兄ちゃん。</s> pic.twitter.com/7h4fvhz6uu 今日は、お兄ちゃんの部屋にある私のエロ画像を貼っていくよ! <unk> <unk> <unk> <unk> <unk> <unk> 画像の上にマウスカーソルを持っていくと、画像の拡大/縮小、回転、拡大、移動、移動、縮小の操作画面が表示されるね。クリックするとその画像についての解説が表示されるよ。※もし画像にマウスオーバー', 'おはよう、お兄ちゃん。</s> 昨日の「お兄ちゃんと二人でごはん」、お兄ちゃんの...</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>', 'おはよう、お兄ちゃん。</s> 今日はなんのお日... お兄ちゃん。 昨日はお兄ちゃん。 今日はお兄ちゃん。 それでは、1日に何回イっ... お兄ちゃん。 俺は今日も学校に行って来た。 そこで、俺のこと友達と勘違いしてたお兄ちゃんが俺のことを名前で呼ん... お兄ちゃん。 昨日はお兄ちゃん。 今日はお兄ちゃん。 それ', 'おはよう、お兄ちゃん。</s> 。今日も1日頑張ろうね♪ #グラドル自画撮り部 #けなこ https://t.co/1j8f5emo5f おはようみそらゆうお兄ちゃん!本日は『お兄ちゃんはエッチなには抵抗がないけど...』のプレミアムグラビアdvdが2018年4月20日に発売です。ちなみに、私服を着てますが実は...。', 'おはよう、お兄ちゃん。</s> 今日から2日間 私は夏休みをいただこう......かな? 今日はお兄ちゃんのお誕生日! お兄ちゃんの日なので、今日の出来事を報告しちゃうね?......あ、もちろん、これからもお兄ちゃんの事よろしくね! 実はね......お兄ちゃんには秘密の契約がいるの......。ちょっとまって、聞いてよ♪.........えへへ、私、まだ秘密の契約して', 'おはよう、お兄ちゃん。</s> 今日も頑張っとるわ。 #けものフレンズ #けもフレ #けもフレ2 pic.twitter.com/g5qwqofhb6 「『けものフレンズ』2期、今年の2月か3月あたりからやってない?」 — けものフレンズ@けもフレ大好き (@kemo_anime) 2017年2月5日']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSmS9FJtEIpD",
        "outputId": "2f3fa570-c099-45d3-aec4-955be9440210"
      },
      "source": [
        "# 推論\n",
        "input = tokenizer.encode(\"私が負けるわけないしね。\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=8)\n",
        "print(tokenizer.batch_decode(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['私が負けるわけないしね。</s> こんなこと言う人達、大して負けてるわけないよね。 私の知ってる人ってあんまりいないと思うけど(笑い いや、いやぁ、結構いると思うよ。(照) いろんな意味で結構頑張ってるし、すごく努力していて、周りを見ると尊敬するしね。 俺もいろんな男に付き合ってたりするけど、やっぱり女の子って', '私が負けるわけないしね。</s> 。今年も、お疲れ様でしたー! いや、実は私は負けるだろうなぁ～(-_-;)...てか負けたら、、、(-_-;) いや、、、、、おれ、負けてるじゃん(爆笑)</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>', '私が負けるわけないしね。</s> >>262は今のところそういう感情はないけどな。 >>363 で、そういうのは、そういう感情を表に出さないような人たちにもやっかまれるので、このスレには来んのが良い。 あ、今度行ったら店の名前とか電話番号とかは教えられないよ? 今度会うことになったんだけど、お兄さんってどんな人がいるの? お姉ちゃんなんだから、お兄', '私が負けるわけないしね。</s> 。( ・∀・)イイネ!! 私はあなたが大好きだし、あなたと話せたら、絶対元気になれるのー♪ 私はまだまだ勉強が足りません、お父さんお母さん、ごめんなさいね......。 そうすればお父さんお母さんもお母さんも安心だし、私のお兄ちゃんもお兄ちゃん大好きだから!......でも......私、そんなお', '私が負けるわけないしね。</s> あぁ、そうだぁ。。 でもね、今日の話は、ホントにショッキングな話だよ。。。 でもね、これから先の人生にはどんなことが待っているかわからないからね。 だから、これから先の自分の人生には、何が待ってるのかしら? このサイトを見てくれてる人は、今がどうなるかなんて知らないよね? 例えばね。。 このサイトに登録して、あなたは', '私が負けるわけないしね。</s> ( <unk>.. <unk> )=3 フン そういえば、「私の好きなアイドルはどこですか?」と尋ねられたら、私は即座に答えちゃいます。いや・・・答えたい人いますか?このコミュに来てる方全員・・・私です(笑)。 「今日(8/1)は、お二人が結婚記念日です」・・・で、何の日だっけ?', '私が負けるわけないしね。</s> やるべきことを、その人がちゃんとやる。 私が勝てるわけがない。 負けるのは当たり前? じゃあ、なんで負けたのよ。 負けたら悲しいし、悔しさで苦しむわ。 私なんて、絶対にお父さんに勝つことはできないから...</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>', '私が負けるわけないしね。</s> 私だって、やらないといけないんですよっっ!!!!!!!! そうそう、うちのメイドちゃんにも、あのー今度<unk> 狩りに行こうって誘ってきたりするんですよ。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuQ0mLTwFIII"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}