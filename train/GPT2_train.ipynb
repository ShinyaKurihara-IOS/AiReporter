{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHfd98YKpAa"
      },
      "source": [
        "以下の設定になっていること  \n",
        "\n",
        "ランタイム > ランタイムのタイプを変更  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  \n",
        "\n",
        "編集 > ノートブックの設定  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZUtGFrIE0N",
        "outputId": "3c88f531-5ea1-4207-f870-b28f3d557d5b"
      },
      "source": [
        "# googleドライブをマウント\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "# 表示されるリンクをクリックして、アクセスを許可して、最後に表示される文字列を以下の入力欄に入れる"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUPHBC4zKgyR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6Phq7IINCz",
        "outputId": "ec2af6c1-5491-4f60-fbd8-a1ff875ffe2f"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7WZHPnIqSv",
        "outputId": "cb21dd60-ed24-4373-aff2-a33e131cace6"
      },
      "source": [
        "# importで使う必要があるので、インストールがランタイム切れるごとに必要\n",
        "# インストール後にランタイムの再起動を行わないとT5Tokenizerが見つからない\n",
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# ドライブに保存してるものでインストール\n",
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers\n",
            "Collecting datasets==1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.5.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.41.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, datasets\n",
            "Successfully installed datasets-1.2.1 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWjtNtvnRTPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c155d443-9753-4fe7-b44e-f6d39a7ed4e2"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tepd4-4WCb"
      },
      "source": [
        "import json\n",
        "path_w = 'train.txt'\n",
        "with open(path_w, mode='w') as f_out:\n",
        "  with open('output_0001.json') as f_in:\n",
        "    json_text = json.load(f_in)\n",
        "    for v_result in json_text.values():\n",
        "      for v_alternatives in v_result:\n",
        "        for val_list in v_alternatives.values():\n",
        "          if isinstance(val_list, list):\n",
        "            for val in val_list:\n",
        "              # print(val.get('transcript'))\n",
        "              f_out.write(val.get('transcript'))\n",
        "              f_out.write('。')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtynWz0u4IT"
      },
      "source": [
        "CLM（Causal Language Modeling）: GPT、GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKmxERxt7Io"
      },
      "source": [
        "・model_name_or_path: モデルのチェックポイント（モデルを最初から学習しない場合）  \n",
        "・model_type: モデルの種別（モデルを最初から学習する場合）  \n",
        "・config_name: コンフィグ名（model_nameと同じでない場合）  \n",
        "・tokenizer_name: トークナイザー名（model_nameと同じでない場合）  \n",
        "・cache_dir: キャッシュフォルダ  \n",
        "・use_fast_tokenizer: Fastトークナイザーを使用するかどうか  \n",
        "・model_revision: 使用するモデルの特定のバージョン  \n",
        "・use_auth_token: 「transformers-cli login」の実行時に生成されたトークンを使用するかどうか  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0KKz6auJXj"
      },
      "source": [
        "・dataset_name: データセット名  \n",
        "・dataset_config_name: データセットのコンフィグ名  \n",
        "・train_file: 学習データ（テキストファイル）  \n",
        "・validation_file: 検証データ（テキストファイル）  \n",
        "・overwrite_cache: キャッシュの上書き  \n",
        "・validation_split_percentage: 学習データから使われる検証データの割合（検証データがない場合）  \n",
        "・max_seq_length: トークン化後の最大合計入力シーケンス長  \n",
        "・preprocessing_num_workers: 前処理に使用するプロセス数  \n",
        "・block_size: トークン化後のオプションの入力シーケンス長  \n",
        "・max_train_samples: 学習データの最大数  \n",
        "・max_val_samples: 検証データの最大数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9LIvEaz6Pst"
      },
      "source": [
        "    # GPT2のモデルファイルを指定\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    # 学習ファイル\n",
        "    --train_file=train.txt \\\n",
        "    # 評価データファイル\n",
        "    --validation_file=train.txt \\\n",
        "    # トレーニングを実施する\n",
        "    --do_train \\\n",
        "    # 評価を実施する\n",
        "    --do_eval \\\n",
        "    # 学習回数（エポック数）\n",
        "    --num_train_epochs=30 \\\n",
        "    # チェックポイントの保存間隔\n",
        "    --save_steps=5000 \\\n",
        "    # チェックポイントの保持数\n",
        "    --save_total_limit=3 \\\n",
        "    # T5Tokenizer.model_max_length=1024をチャンクサイズとして使用するかblock_sizeで指定するかを設定する（設定しないとtokenizerの超大なサイズから1024になる）メモリに乗るように調整する必要がある（バッチサイズとの兼ね合い）\n",
        "    --block_size=512 \\\n",
        "    # GPU1つあたりの学習バッチサイズ\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    # GPU1つあたりの評価バッチサイズ\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    # モデルとチェックポイントの出力先\n",
        "    --output_dir=output/ \\\n",
        "    # 出力先の上書きの許可\n",
        "    --overwrite_output_dir=True \\\n",
        "    # T5Tokenizerで高速化ライブラリがあれば使用する\n",
        "    --use_fast_tokenizer=False\n",
        "\n",
        "### 学習する回数\n",
        "エポック数 * (データ文字数 / ブロックサイズ):1つの学習データ / バッチサイズ  \n",
        "    エポック数 = 1  \n",
        "    データ文字数 = 22142  \n",
        "    データの文字数だと数字が合わないので、tokenizeした結果の形態素数による？\n",
        "    ブロックサイズ = 512  \n",
        "    データ文字数 / ブロックサイズ = 43  \n",
        "    バッチサイズ = 1  \n",
        "    Total optimization steps = 29  \n",
        "\n",
        "\n",
        "バッチサイズ = 2  \n",
        "バッチサイズを2にすることで、トータルの実行回数が減っている  \n",
        "Total optimization steps = 15  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zS2rIpZIfXB",
        "outputId": "01f525d5-01c5-46d6-988d-8091bba99969"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs=100 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --block_size=256 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --output_dir=output/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-28 15:11:30.908292: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "06/28/2021 15:11:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "06/28/2021 15:11:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jun28_15-11-32_2d4200e574ad, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c3c137b81c3400d3/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:463] 2021-06-28 15:11:33,537 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:499] 2021-06-28 15:11:33,538 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-06-28 15:11:34,227 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-06-28 15:11:34,227 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-06-28 15:11:34,227 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-06-28 15:11:34,227 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-06-28 15:11:34,227 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1051] 2021-06-28 15:11:34,424 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1167] 2021-06-28 15:11:44,209 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-06-28 15:11:44,209 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c3c137b81c3400d3/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-49df1d5d244ca13a.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c3c137b81c3400d3/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-cea5b14669d19140.arrow\n",
            "[INFO|trainer.py:946] 2021-06-28 15:11:47,574 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-06-28 15:11:47,574 >>   Num examples = 7\n",
            "[INFO|trainer.py:948] 2021-06-28 15:11:47,574 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:949] 2021-06-28 15:11:47,574 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:950] 2021-06-28 15:11:47,574 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:951] 2021-06-28 15:11:47,574 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-06-28 15:11:47,574 >>   Total optimization steps = 200\n",
            "100% 200/200 [01:23<00:00,  2.45it/s][INFO|trainer.py:1129] 2021-06-28 15:13:11,398 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 83.8243, 'train_samples_per_second': 2.386, 'epoch': 100.0}\n",
            "100% 200/200 [01:23<00:00,  2.39it/s]\n",
            "[INFO|trainer.py:1558] 2021-06-28 15:13:11,730 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:314] 2021-06-28 15:13:11,735 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-06-28 15:13:16,847 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-06-28 15:13:16,852 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-06-28 15:13:16,856 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-06-28 15:13:16,864 >> Copy vocab file to output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-06-28 15:13:16,870 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   epoch                      =   100.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   init_mem_cpu_alloc_delta   =     1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   init_mem_cpu_peaked_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   init_mem_gpu_alloc_delta   =  1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   init_mem_gpu_peaked_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   train_mem_cpu_alloc_delta  =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,870 >>   train_mem_cpu_peaked_delta =     0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,871 >>   train_mem_gpu_alloc_delta  =  3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,871 >>   train_mem_gpu_peaked_delta =  4657MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,871 >>   train_runtime              = 83.8243\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,871 >>   train_samples              =       7\n",
            "[INFO|trainer_pt_utils.py:661] 2021-06-28 15:13:16,871 >>   train_samples_per_second   =   2.386\n",
            "CPU times: user 924 ms, sys: 295 ms, total: 1.22 s\n",
            "Wall time: 1min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu4AvG5Ik-p"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_K7EOdMPVZ",
        "outputId": "4404c596-68f7-4e69-bf60-16f14018e0a9"
      },
      "source": [
        "# 推論\n",
        "input = tokenizer.encode(\"青コーナー\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=8)\n",
        "print(tokenizer.batch_decode(output))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['青コーナー</s> 天井まで20m 青コーナーから3人が 降りてきました。 赤コーナーから2人が降りてきました。 青コーナーから2人 降りてきました。 青コーナーから2人 降りてきました。 赤コーナーから2人 降りてきました。 赤コーナーから1人 降りてきました。 赤コーナーから2人 [...] 青コーナーから2人 降りてきました。', '青コーナー</s> スタートです。左の茶道ていくらですか? 香水は数珠を持っていますか? 赤コーナー はい いいえ 無記名です。お点前しませんが ご苦労様でした。次はいらしてください。御席へとご案内します。お菓子をお出しします。手水鉢をお示ししますのでご自由にお飲みください。 青コーナー はい いいえ', '青コーナー</s> 雨にぬれても 濡れても 強くなれるよ 僕は負けない [dvd]出版社/メーカー: ポニーキャニオン■発売日: 2008/09/21■値段: <unk> 6,480(税込) ■発売場所: dvd・cdショップ ■商品紹介: ■青コーナー 雨にぬれても 濡れても 強くなれるよ 僕は負けない ■青コーナー 雨にぬれても', '青コーナー</s> 1 ○ 2 ○ 3 ○ 4 ○ 5 ○ 6 三十路コーナー 1 ○ 2 ○ 3 ○ 4 ○ 5 ○ 6 ○ 7 19 青コーナー 1 ○ 3 ○ 4 ○ 4 ○ 5 ○ 6 ○ 7 9 青コーナー 1 ○ 2 ○ 3 ○ 4 ○ 5 ○ 7 19 赤コーナー 1 ○ 2 ○', '青コーナー</s> 対 馬連4点 的中おめでとうございます。今年の函館2歳ステークスにふさわしいレースでした。来年も注目していきましょう。(大沼博) 函館2歳s 1〜4着まで馬券になっています。オッズの変動も大きいですね。ここぞという時にしっかり結果を出しています。来年の函館2歳sも注目していきましょう。(大沼博) 函館2歳s', '青コーナー</s> 何事もなかったかのように再開して、2連勝もあってリングに戻りましたが、やはりあの重量挙げまくりのシーンはなかなか見ていて気持ちよかったですね 武尊 そうですねあの時、手数を増やして配達に回ってたおかげでリングにもしっかりと顔を出してるし手数もどんどん出していったのでそれで勝負強いんだなというのを見せられたのでよかったですねあの試合からどれだけ自分のボクシングが出せるかっていうのが', '青コーナー</s> : 確かにそうだね。それでは、実際にトレードをする中でどのようなところがポイントになってくるのでしょうか。 大西さん : そうですね。しっかり落ち着いて、相手の動きを探しながら狙って欲しいですね。自分の得意なところや特徴を出していって徐々に相手の動きを探しながら狙って欲しいです。 大西さん : それでは最後に、最近注目されている仮想通貨投資法をひとつ。「レバレッジ」という投資法です', '青コーナー</s> 左のジャブを作り出していくのは誰だ!?3人とも右のジャブもしっかり出せるようにな! 青コーナー 左のジャブを作り出していくのは誰だ!?3人とも右のジャブもしっかり出せるようにな! 赤コーナー 左のジャブを作り出していくのは誰だ!?3人とも右のジャブもしっかり出せるようにな! 青コーナー 左のジャブを作り出していくのは']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7NySwjBMRlR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMHnJ1-7QlIa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB5AQ9pQoyR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}