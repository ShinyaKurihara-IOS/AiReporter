{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHfd98YKpAa"
      },
      "source": [
        "以下の設定になっていること  \n",
        "\n",
        "ランタイム > ランタイムのタイプを変更  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  \n",
        "\n",
        "編集 > ノートブックの設定  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZUtGFrIE0N",
        "outputId": "e33356e4-9d5b-49bb-c32f-9690dca7e9b0"
      },
      "source": [
        "# googleドライブをマウント\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "# 表示されるリンクをクリックして、アクセスを許可して、最後に表示される文字列を以下の入力欄に入れる"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6Phq7IINCz",
        "outputId": "caa4c502-4538-430a-e6c8-a60e75bc8e0b"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7WZHPnIqSv",
        "outputId": "b094a3e9-06b3-49b4-d023-474e52030f53"
      },
      "source": [
        "# importで使う必要があるので、インストールがランタイム切れるごとに必要\n",
        "# インストール後にランタイムの再起動を行わないとT5Tokenizerが見つからない\n",
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# ドライブに保存してるものでインストール\n",
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n",
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, tqdm, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWjtNtvnRTPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec9af73-6f77-4575-f6eb-575f68d8d80b"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tepd4-4WCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtynWz0u4IT"
      },
      "source": [
        "CLM（Causal Language Modeling）: GPT、GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKmxERxt7Io"
      },
      "source": [
        "・model_name_or_path: モデルのチェックポイント（モデルを最初から学習しない場合）  \n",
        "・model_type: モデルの種別（モデルを最初から学習する場合）  \n",
        "・config_name: コンフィグ名（model_nameと同じでない場合）  \n",
        "・tokenizer_name: トークナイザー名（model_nameと同じでない場合）  \n",
        "・cache_dir: キャッシュフォルダ  \n",
        "・use_fast_tokenizer: Fastトークナイザーを使用するかどうか  \n",
        "・model_revision: 使用するモデルの特定のバージョン  \n",
        "・use_auth_token: 「transformers-cli login」の実行時に生成されたトークンを使用するかどうか  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0KKz6auJXj"
      },
      "source": [
        "・dataset_name: データセット名  \n",
        "・dataset_config_name: データセットのコンフィグ名  \n",
        "・train_file: 学習データ（テキストファイル）  \n",
        "・validation_file: 検証データ（テキストファイル）  \n",
        "・overwrite_cache: キャッシュの上書き  \n",
        "・validation_split_percentage: 学習データから使われる検証データの割合（検証データがない場合）  \n",
        "・max_seq_length: トークン化後の最大合計入力シーケンス長  \n",
        "・preprocessing_num_workers: 前処理に使用するプロセス数  \n",
        "・block_size: トークン化後のオプションの入力シーケンス長  \n",
        "・max_train_samples: 学習データの最大数  \n",
        "・max_val_samples: 検証データの最大数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9LIvEaz6Pst"
      },
      "source": [
        "    # GPT2のモデルファイルを指定\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    # 学習ファイル\n",
        "    --train_file=train.txt \\\n",
        "    # 評価データファイル\n",
        "    --validation_file=train.txt \\\n",
        "    # トレーニングを実施する\n",
        "    --do_train \\\n",
        "    # 評価を実施する\n",
        "    --do_eval \\\n",
        "    # 学習回数（エポック数）\n",
        "    --num_train_epochs=30 \\\n",
        "    # チェックポイントの保存間隔\n",
        "    --save_steps=5000 \\\n",
        "    # チェックポイントの保持数\n",
        "    --save_total_limit=3 \\\n",
        "    # T5Tokenizer.model_max_length=1024をチャンクサイズとして使用するかblock_sizeで指定するかを設定する（設定しないとtokenizerの超大なサイズから1024になる）メモリに乗るように調整する必要がある（バッチサイズとの兼ね合い）\n",
        "    --block_size=512 \\\n",
        "    # GPU1つあたりの学習バッチサイズ\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    # GPU1つあたりの評価バッチサイズ\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    # モデルとチェックポイントの出力先\n",
        "    --output_dir=output/ \\\n",
        "    # 出力先の上書きの許可\n",
        "    --overwrite_output_dir=True \\\n",
        "    # T5Tokenizerで高速化ライブラリがあれば使用する\n",
        "    --use_fast_tokenizer=False\n",
        "\n",
        "### 学習する回数\n",
        "エポック数 * (データ文字数 / ブロックサイズ):1つの学習データ / バッチサイズ  \n",
        "    エポック数 = 1  \n",
        "    データ文字数 = 22142  \n",
        "    データの文字数だと数字が合わないので、tokenizeした結果の形態素数による？\n",
        "    ブロックサイズ = 512  \n",
        "    データ文字数 / ブロックサイズ = 43  \n",
        "    バッチサイズ = 1  \n",
        "    Total optimization steps = 29  \n",
        "\n",
        "\n",
        "バッチサイズ = 2  \n",
        "バッチサイズを2にすることで、トータルの実行回数が減っている  \n",
        "Total optimization steps = 15  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zS2rIpZIfXB",
        "outputId": "d5b29a9e-c54a-4b0c-b66d-a6186ae54ad2"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs=300 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --block_size=256 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --output_dir=output/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 12:28:23.411625: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/28/2021 12:28:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/28/2021 12:28:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=300.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jul28_12-28-31_69031f40117e, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 1.75MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-3a09e1cc42b81adc (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-3a09e1cc42b81adc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-3a09e1cc42b81adc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:33,887 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdq2xc46w\n",
            "Downloading: 100% 654/654 [00:00<00:00, 511kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:34,252 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:34,252 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:463] 2021-07-28 12:28:34,253 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:499] 2021-07-28 12:28:34,253 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:34,602 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0jw6co9f\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 15.1MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:34,760 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:34,760 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:35,455 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp541vzxg5\n",
            "Downloading: 100% 153/153 [00:00<00:00, 121kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:35,795 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:35,796 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:36,143 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsiz4u53j\n",
            "Downloading: 100% 225/225 [00:00<00:00, 161kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:36,487 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:36,487 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,828 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:37,243 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpczfjp7x_\n",
            "Downloading: 100% 1.37G/1.37G [00:24<00:00, 56.7MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:29:01,463 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:29:01,463 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1051] 2021-07-28 12:29:01,464 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1167] 2021-07-28 12:29:10,624 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-07-28 12:29:10,624 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 2/2 [00:00<00:00, 22.17ba/s]\n",
            "100% 2/2 [00:00<00:00, 24.12ba/s]\n",
            "[INFO|trainer.py:946] 2021-07-28 12:29:18,838 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-07-28 12:29:18,838 >>   Num examples = 67\n",
            "[INFO|trainer.py:948] 2021-07-28 12:29:18,838 >>   Num Epochs = 300\n",
            "[INFO|trainer.py:949] 2021-07-28 12:29:18,839 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:950] 2021-07-28 12:29:18,839 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:951] 2021-07-28 12:29:18,839 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-07-28 12:29:18,839 >>   Total optimization steps = 5100\n",
            "{'loss': 0.7468, 'learning_rate': 4.5098039215686275e-05, 'epoch': 29.41}\n",
            "{'loss': 0.031, 'learning_rate': 4.0196078431372555e-05, 'epoch': 58.82}\n",
            "{'loss': 0.0212, 'learning_rate': 3.529411764705883e-05, 'epoch': 88.24}\n",
            "{'loss': 0.0174, 'learning_rate': 3.0392156862745097e-05, 'epoch': 117.65}\n",
            "{'loss': 0.016, 'learning_rate': 2.5490196078431373e-05, 'epoch': 147.06}\n",
            "{'loss': 0.0151, 'learning_rate': 2.058823529411765e-05, 'epoch': 176.47}\n",
            "{'loss': 0.0143, 'learning_rate': 1.568627450980392e-05, 'epoch': 205.88}\n",
            "{'loss': 0.0137, 'learning_rate': 1.0784313725490197e-05, 'epoch': 235.29}\n",
            "{'loss': 0.0133, 'learning_rate': 5.882352941176471e-06, 'epoch': 264.71}\n",
            "{'loss': 0.0132, 'learning_rate': 9.80392156862745e-07, 'epoch': 294.12}\n",
            " 98% 5000/5100 [22:24<00:26,  3.79it/s][INFO|trainer.py:1558] 2021-07-28 12:51:43,570 >> Saving model checkpoint to output/checkpoint-5000\n",
            "[INFO|configuration_utils.py:314] 2021-07-28 12:51:43,574 >> Configuration saved in output/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-07-28 12:51:49,125 >> Model weights saved in output/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-07-28 12:51:49,130 >> tokenizer config file saved in output/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-07-28 12:51:49,133 >> Special tokens file saved in output/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-07-28 12:51:49,140 >> Copy vocab file to output/checkpoint-5000/spiece.model\n",
            "100% 5100/5100 [23:28<00:00,  3.89it/s][INFO|trainer.py:1129] 2021-07-28 12:52:47,825 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1408.9868, 'train_samples_per_second': 3.62, 'epoch': 300.0}\n",
            "100% 5100/5100 [23:28<00:00,  3.62it/s]\n",
            "[INFO|trainer.py:1558] 2021-07-28 12:52:48,103 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:314] 2021-07-28 12:52:48,473 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-07-28 12:53:05,525 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-07-28 12:53:05,968 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-07-28 12:53:06,289 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-07-28 12:53:06,758 >> Copy vocab file to output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-07-28 12:53:07,210 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   epoch                      =     300.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   init_mem_cpu_alloc_delta   =       1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   init_mem_cpu_peaked_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   init_mem_gpu_alloc_delta   =    1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   init_mem_gpu_peaked_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_cpu_alloc_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_cpu_peaked_delta =     126MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_gpu_alloc_delta  =    3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_gpu_peaked_delta =    4657MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_runtime              = 1408.9868\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_samples              =        67\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_samples_per_second   =      3.62\n",
            "CPU times: user 17.4 s, sys: 3.3 s, total: 20.7 s\n",
            "Wall time: 26min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu4AvG5Ik-p"
      },
      "source": [
        "# from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# # トークナイザーとモデルの準備\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"output/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xllJ0rN9Fuyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab6fa01-fa0a-4af4-eb20-e55c12a1914c"
      },
      "source": [
        "# もちろんだが、Autoでも直指定でも同じ結果にはなっている\n",
        "# https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
        "from transformers import T5Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# GPU判定\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"output/\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(32000, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbp9X4mv2yJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt93bvvwv22j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_K7EOdMPVZ",
        "outputId": "ed6651c1-6de1-424b-969f-1e63ed888986"
      },
      "source": [
        "# 推論\n",
        "# https://huggingface.co/blog/how-to-generate\n",
        "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate\n",
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブ 残り10秒!\n",
            "左のジャブ 1ラウンド10左のフック!\n",
            "左のジャブ 1ラウンド2r\n",
            "左のジャブ 残り2分間という戦いになったが、しっかりお互いがパンチを出し合う中での戦いになった。\n",
            "左のジャブ 残り2分間、そのボクサー人生にとって、これ以上ないような勝利をつかみたいと話していた。\n",
            "左のジャブ 残り2分間という戦いになりました。\n",
            "左のジャブ このストレート!\n",
            "左のジャブ 残り2分間という戦いになりました。\n",
            "左のジャブ 残り 2分間という戦いになるが、しっかりガードして右のストレート!\n",
            "左のジャブ 次は右のストレート右のフック!右のストレートも返していく!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB5AQ9pQoyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16009c1-f79f-4af6-e416-9ccfb55e4398"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック このストレートどうでしょうか?\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-VUAXfoEE9K",
        "outputId": "4f0a4078-268f-45cc-bcdd-8c4d66c6dfd1"
      },
      "source": [
        "input = tokenizer.encode(\"ボディ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボディ このダウンを奪って一気に行こうかというところから、もう一度ギアを落としました。\n",
            "ボディ このダウンフォース、かなり頑張ってますね。\n",
            "ボディ 得意の右アッパー!\n",
            "ボディ 選手は152cmで体重が46キロですから、およそ5倍の重りがかかっているということになります。\n",
            "ボディ 選手は3回戦で行われます。\n",
            "ボディ このダウンを奪って一気に行こうかというところから、もう一度ギアを落としました。\n",
            "ボディ ただ、選手はそれだけではありませんでした。\n",
            "ボディ このストレートのまいは、まさにのですね。\n",
            "ボディ 選手は25歳これがキャリア10戦目。\n",
            "ボディ これが4戦目。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zPc_EPFEJi_",
        "outputId": "e597e595-4670-48b8-fbf8-94dd36fc95c1"
      },
      "source": [
        "input = tokenizer.encode(\"右のストレート\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のストレート さあ今度はの右。\n",
            "右のストレート 少し行き過ぎになったか。\n",
            "右のストレート ここはガードを固めたほうが良いかなと。\n",
            "右のストレート いまパンチをもらっているです。\n",
            "右のストレート 少しが下がったように見えました。\n",
            "右のストレート ここはガードを固めたガードを固めた左のジャブ。\n",
            "右のストレート さあ第2ラウンドが始まりました。\n",
            "右のストレート 少しクラっときたか。\n",
            "右のストレート さん、ボディがいいんで、ボディからのフックの返しが、さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。\n",
            "右のストレート 少しクラっときたか。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHIHS0H7EMe1",
        "outputId": "224ab7e6-1f9b-4880-8a94-6b49a5cd0fa7"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上から がを打ちました。\n",
            "ガードの上から この距離から両者引きませんね。\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から この距離で両者全く引きませんね。\n",
            "ガードの上から これよりいよいよ今日の試合のメインイベントお届けします\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から でもしっかり手数を出して。\n",
            "ガードの上から 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "ガードの上から これはの距離か?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4H6oqsAKrlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxas9R2LKrt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiAl1wGnF6kf"
      },
      "source": [
        "tokenizerの中身を確認  \n",
        "\\<s\\>の意味合いを表示  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYhyTw7Ea7-",
        "outputId": "8daf467e-f137-4f24-960e-71f20db23a62"
      },
      "source": [
        "# model.generateの結果はtokenizerのindexベクトル\n",
        "output[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    9,  5682,    10, 12276,     2,     9,     0,    20,  2115,    18,\n",
              "         5456,  2199,     7,    80,     0,    10,   819,     8,     2,     2,\n",
              "            2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym78sIbQFIMn",
        "outputId": "0e4f0dcc-3a88-4653-d17f-f3ce7328ac4b"
      },
      "source": [
        "# 記号の意味\n",
        "tokenizer.all_special_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNCWHrgWG9bY",
        "outputId": "3574002a-6b91-4aab-b276-e7e660b76436"
      },
      "source": [
        "# 記号に対応するindex\n",
        "tokenizer.all_special_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0, 5, 3, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCg-F5fAFlaa"
      },
      "source": [
        "{\"bos_token\": \"\\<s\\>\", \"eos_token\": \"\\</s\\>\", \"unk_token\": \"<unk>\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}  \n",
        "bos_token: 文の先頭（Begin of sequence token）  \n",
        "eos_token: 文のおしり（End of Sequence token）  \n",
        "unk_token: IDに変換できない文字（Unknown token）  \n",
        "sep_token: 文と文を区切り目（The separator token）  \n",
        "pad_token: パッディング（The token used for padding）  \n",
        "cls_token: 分類用（cls_token）  \n",
        "mask_token: マスク（The token used for masking values）  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5xE9Myg0WR"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode  \n",
        "sequences : torch.Tensorの配列を入力値として指定  \n",
        "  トークン化された入力IDのリスト  \n",
        "skip_special_tokens : デコード時に特殊なトークンを削除するかどうか(eos_tokenとかを消す)(デフォルト:False)  \n",
        "clean_up_tokenization_spaces : トークン化スペースをクリーンアップするかどうか(デフォルト:True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqZhkicXiBP8"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode  \n",
        "\n",
        "text (str, List[str] or List[int]) – 入力文字列  \n",
        "\n",
        "text_pair (str, List[str] or List[int], optional) – ペアとなるもう一つを入力する場合のオプション  \n",
        "\n",
        "add_special_tokens (bool, optional, defaults to True) – 上記で定義していない特別なトークンをモデルに適用するか\n",
        "\n",
        "padding (bool, str or PaddingStrategy, optional, defaults to False) –パディングして入力シーケンスを揃える場合  \n",
        "\n",
        "truncation (bool, str or TruncationStrategy, optional, defaults to False) –逆に長過ぎる場合に、一定の長さに揃える場合\n",
        "\n",
        "max_length (int, optional) –トランケーション・パディングで使用するオプション\n",
        "\n",
        "stride (int, optional, defaults to 0) – max_lengthで切り捨てられたのを調整する  \n",
        "\n",
        "is_split_into_words (bool, optional, defaults to False) – 単語分割が既にされている場合True\n",
        "\n",
        "pad_to_multiple_of (int, optional) – 指定された値の倍数になるようにシーケンスをパッドする  \n",
        "\n",
        "return_tensors (str or TensorType, optional) – python整数のリストの代わりにテンソルを返す  \n",
        "'tf': Return TensorFlow tf.constant objects.  \n",
        "'pt': Return PyTorch torch.Tensor objects.  \n",
        "'np': Return Numpy np.ndarray objects.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZXgdwCNECfn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paoQ0wFiNsyp"
      },
      "source": [
        "* https://huggingface.co/blog/how-to-generate  \n",
        "* https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate  \n",
        "* https://note.com/npaka/n/n5d296d8ae26d  \n",
        "* https://note.com/npaka/n/n96dde45fdf8d  \n",
        "\n",
        "### GPT2LMHeadModel.generateのオプションを確認  \n",
        "\n",
        "input_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) – 入力シーケンス  \n",
        "\n",
        "max_length (int, optional, defaults to model.config.max_length) – 生成されるシーケンスの最大長を指定（学習に使用した256の長さが良さそう）\n",
        "\n",
        "max_new_tokens (int, optional, defaults to None) – 現在のトークン数に関係なく、生成されるシーケンスの最大長を指定\n",
        "\n",
        "min_length (int, optional, defaults to 10) – 生成されるシーケンスの最小の長さ\n",
        "\n",
        "do_sample (bool, optional, defaults to False) – 単語予測にサンプリングを入れてランダム性を導入する。（デフォルトは greedy decoding の生成）  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d  \n",
        "\n",
        "early_stopping (bool, optional, defaults to False) – ビーム探索で、num_beams個の文が生成された時点で、ビーム探索を終了するかどうか  \n",
        "\n",
        "num_beams (int, optional, defaults to 1) – ビームサーチを行うビームの数。1はビームサーチを行わないことを意味します。  \n",
        "\n",
        "temperature (float, optional, defaults to 1.0) – 次のトークンの確率をモジュール化するために使用される値です。温度（デフォルト1、推奨0.7〜1.0）ボルツマン分布のパラメータ。小さい値ではランダムな補完が減り，0では決まりきった繰り返しの文になる。大きい値ではより様々な補完がされる。  \n",
        "\n",
        "top_k (int, optional, defaults to 50) – top-k-filteringのために保持する最高確率の語彙トークンの数です。確率が大きめな候補からサンプリングしてランダム性を導入する際の候補を何個にするか。40が一般的に良い値  \n",
        "\n",
        "top_p (float, optional, defaults to 1.0) – 生成テキストを累積確率に制限 (0で制限なし) float < 1に設定すると、top_p以上の確率を持つ最も確率の高いトークンのみが生成のために保持されます。  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d#top-p-(nucleus)-sampling  \n",
        "\n",
        "repetition_penalty (float, optional, defaults to 1.0) – 反復ペナルティのパラメータです。1.0はペナルティなし。すでに生成された単語や文脈に属する単語にペナルティを与えるために使用することができます。反復防止にはかなり効果的ですが、異なるモデルやユースケースには非常に敏感なようで、議論がある。  \n",
        "\n",
        "pad_token_id (int, optional) – PADトークンを指定\n",
        "\n",
        "bos_token_id (int, optional) – bosトークンを指定\n",
        "\n",
        "eos_token_id (int, optional) – eosトークンを指定\n",
        "\n",
        "length_penalty (float, optional, defaults to 1.0) – 長さに対する指数関数的なペナルティ。1.0はペナルティがないことを意味します。1.0未満の値を設定すると、モデルは短い配列を生成するようになり、1.0以上の値を設定すると、モデルは長い配列を生成するようになります。\n",
        "\n",
        "no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、そのサイズのngramはすべて一度しか発生しません。最も一般的な n-grams ペナルティは、すでに見た n-gramsを作る可能性のある次の単語の確率を 0 に手動で設定することで、n-gramsが 2 回出現しないようにするものです。  \n",
        "\n",
        "encoder_no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、encoder_input_idsに出現したそのサイズのすべてのngramは、decoder_input_idsには出現しません。\n",
        "\n",
        "bad_words_ids (List[List[int]], optional) – 生成してはいけないトークンのidのリスト。トークンのIDは以下で確認  \n",
        "tokenizer(bad_word, add_prefix_space=True).input_ids  \n",
        "\n",
        "num_return_sequences (int, optional, defaults to 1) – バッチ内の各要素について、独立して計算された戻り値の配列の数(返却される結果の数)。返されるべき最高得点のBeamの数を設定します。ただし、num_return_sequences <= num_beams とします。\n",
        "\n",
        "max_time (float, optional, defaults to None) – 計算の実行を許可する最大時間を秒単位で指定します。割り当てられた時間が経過しても、生成は現在のパスを終了します。  \n",
        "\n",
        "attention_mask (torch.LongTensor of shape (batch_size, sequence_length), optional) – パディングされたトークンのインデックスに対してアテンションを行わないようにするためのマスクです。マスクの値は [0, 1] で、マスクされていないトークンには 1、マスクされたトークンには 0 です。提供されていない場合は、パッドトークンをマスクするinput_idsと同じ形のテンソルがデフォルトになります。attentionで予測するための配列を作るので、マスクすると候補に出なくなる。入力シーケンスに対して同じ長さで指定する\n",
        "\n",
        "decoder_start_token_id (int, optional) – エンコーダ・デコーダモデルがbosとは異なるトークンでデコードを開始した場合、そのトークンのid。\n",
        "\n",
        "use_cache – (bool, optional, defaults to True): 過去の最後のキー／バリューの注目度（モデルに該当する場合）を利用して、デコーディングを高速化するかどうか。  \n",
        "\n",
        "num_beam_groups (int, optional, defaults to 1) – num_beamsを分割するグループの数（ビームの異なるグループ間の多様性を確保するため）。\n",
        "\n",
        "diversity_penalty (float, optional, defaults to 0.0) – この値は、ある時点で他のグループのビームと同じトークンを生成した場合、ビームのスコアから差し引かれます。なお、ダイバーシティペナルティは、グループビーム検索が有効な場合にのみ有効です。  \n",
        "\n",
        "prefix_allowed_tokens_fn – (Callable[[int, torch.Tensor], List[int]], optional):提供された場合、この関数は、各ステップで許可されたトークンのみにビーム検索を制約します。提供されない場合、制約は適用されません。この関数は2つの引数をとります：バッチID batch_id と input_id です。これは、バッチID batch_idと以前に生成されたトークンinput_idsを条件として、次の生成ステップで許可されたトークンのリストを返さなければなりません。この引数は、「自己回帰的実体検索」で説明されているように、接頭辞を条件とした制約付き生成に役立ちます。  \n",
        "\n",
        "output_attentions (bool, optional, defaults to False) – すべてのアテンションレイヤーのアテンションテンソルを返すかどうか。  \n",
        "\n",
        "output_hidden_states (bool, optional, defaults to False) – すべてのレイヤーの隠れた状態を返すかどうか。  \n",
        "\n",
        "output_scores (bool, optional, defaults to False) – 予測スコアを返すかどうか。  \n",
        "\n",
        "return_dict_in_generate (bool, optional, defaults to False) – 単なるタプルではなく、ModelOutputを返すかどうか。  \n",
        "\n",
        "forced_bos_token_id (int, optional) – decoder_start_token_idの後に、最初に生成されるトークンとして強制的に使用するトークンのidです。mBARTのような多言語モデルで、最初に生成されるトークンがターゲット言語のトークンである必要がある場合に便利です。（一番最初に生成される単語を指定してしまう。）  \n",
        "\n",
        "forced_eos_token_id (int, optional) – max_lengthに達したときに、最後に生成されたトークンとして強制的に使用するトークンのidです。(最後をわかりやすくして、途中で切られたのを知らせる)  \n",
        "\n",
        "remove_invalid_values (bool, optional) – 生成方法がクラッシュするのを防ぐために、モデルの可能性のあるnanとinfの出力を削除するかどうか。remove_invalid_valuesを使うと生成が遅くなることに注意してください。  \n",
        "\n",
        "synced_gpus (bool, optional, defaults to False) – max_lengthまでwhileループを続けて実行するかどうか  \n",
        "\n",
        "最新の研究により、単純な Beam Search や Greedy Search が同じ単語列の繰り返しを発生させてしまうのは、decoding に問題があるのではなくモデルの学習自体に問題があるとされています。また、Top-K や Top-p のようなサンプリングによる decoding であってもそうした単語列の繰り返しは発生しうるそうです。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIarpN7BN1HJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisPBDCsN1OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d59b4b-ae65-40fd-ff13-812bd6ea68ed"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n",
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n",
            "左のフック</s> この距離で左のストレート! <unk> の右! 距離が詰まる! 右のストレート! <unk> がストレートを打ち返していく!</s>\n",
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n",
            "左のフック</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!</s> </s> </s>\n",
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n",
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n",
            "左のフック</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!</s> </s> </s>\n",
            "左のフック</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!</s> </s> </s>\n",
            "左のフック</s> これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS8z5iRxN1Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664f787e-f28f-4e9e-9d94-87220b48c81a"
      },
      "source": [
        "input = tokenizer.encode(\"ラッシュラッシュ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ラッシュラッシュ</s> このラッシュのシーンですけれども、ちょっとよろけたところで、左に右パンツあって行きます。そして、右のストレートも出していきます。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> しかし、<unk> も再び世界に向けての挑戦となっていますから、ここで負けてはいけませんよね。 <unk> 選手にとってはここでの負けは厳しいですね。</s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> しかし<unk> も今あっと言わせるシーン作りましたね。いいラッシュましたね。今のはもう狙ってたと見ていいですか?</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> 会場からは大きな拍手、女子フライ級4回戦ファイナルラウンドへと入っていきます。 左のジャブ、残り2分間という戦いになります。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> このラッシュのシーンですけれども、ちょっと後ろに二歩三歩下がったりも見えますけれども。 足の位置も悪くなっているので。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> しかし<unk> はそれを許さない。今度は<unk> の右ボディ! ラッシュ打っているときにボディ打たれると効くと思います。 右!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> このラッシュのシーンですけれども、少し後ろに二歩三歩下がったりも見えますけれども。 これは足の位置が上がって、パンチが当たるというのも見えますね。</s>\n",
            "ラッシュラッシュ</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> このラッシュのシーンですけれども、ちょっとよろけたところで左に右パンツあって行きますよね。あれ、あれがいい判断だと思います。</s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkkVVbvvsWG",
        "outputId": "d1f58c3b-ae5b-450a-a946-e5711a4318c0"
      },
      "source": [
        "input = tokenizer.encode(\"ダウン\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ダウン</s> ここまでは落ち着いた試合展開。第4ラウンドに入っていきます。 第4ラウンドに入っていきます。 <unk> Cの速いこの左のジャブ、左のジャブからの右、そして右のジャブ、右のストレートを繰り出していったのが<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> ダウン 右のフック!左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> が勝利を収めました。 第6ラウンドに入りました。 終盤の戦いになってきます。 試合は残り6分。 食べないスタミナに自信があるってのがよく分かる試合ですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> さん、ボディがいいんで、ボディからのフックの返しが、<unk> さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。主導権をどちらが取っていくかという第2ラウンド。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> 獲得 1 <unk> 2 勝利<unk> 3<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "ダウン</s> 入ってきたところにカウンターを返していくという展開もありますね。 第3ラウンド、ダウンを食らった直後のラウンドは、ちょっと今までの戦い方に疑問を抱きながらという風に見えましたけれども。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> 獲得 <unk> 得意の右アッパー! 今の右のアッパーはどうでしょうか? ちょっと効いたかなと思います。反応が遅れました。ボディに打ち込んでいく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> 獲得した勝ち点は10。順位は変わらず。 <unk> 、今試合の終盤にかけて自分のペースをクシングを貫いた<unk> です。 第3ラウンドまでが終わっています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUggffnvsdo",
        "outputId": "00402415-e357-4fc4-d9b7-2bdbdaa86c52"
      },
      "source": [
        "input = tokenizer.encode(\"あーとここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 右のフック!左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah8NV8oNwuO8",
        "outputId": "fbba3552-68a5-4aed-9848-d18bf903dacc"
      },
      "source": [
        "input = tokenizer.encode(\"ここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.85, temperature=0.7, min_length=64, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。 そうですね。足の位置も悪くなっているので。 ショートレンジのパンチ、ボディに打ち込んでいく青森。 青森がムキになった時左下にお客さん入ってしまったので戻りになってしまったからですね</s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しクラっときたか。足が止まっている。<unk> がそこに合わせて打っていく。ガードが下がっている。 <unk> がプレスかけ始めましたね。 少しパンチをもらったが、返して行きます<unk> です。ガードが下がってきた。 <unk> がプレスかけ始めましたね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少し聞いただけではどのような試合になるか全く想像がつきませんね。 第1ラウンド、左のジャブを打って行ったのは<unk> です。 24歳、ベテランの選手です。 <unk> がどう入ってくるかの点もポイントですね 。 キャリア49戦目、35歳の<unk> 、負ければもう引退も覚悟しなくてはならないという、そんなことも話していました。</s>\n",
            "ここで倒れた</s> 少し聞いただけでも、そのダメージはすごく伝わってきます。 少し聞いただけでも、そのダメージはすごく伝わってきます。 第1ラウンド、ダウンを食らった直後のラウンドは、ちょっと今までの戦い方に疑問を抱きながらという風に見えましたけれども。 今もうちょっと開き直ってますね。いいと思います今のままで。足も動いてると思うんで。</s>\n",
            "ここで倒れた</s> 少しふらついた!そしてパンチを貰った!さあ今度は<unk> が返していく! <unk> 選手もいいボディを打ちましたね。 この試合は後半に入ります。 第4ラウンドに入りました。 <unk> Cの速いこの左のジャブ、左のジャブからの右、そして右のジャブ、右のストレートを繰り出していったのが<unk> です。</s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しクラっときたか。。。 そして少しずつ距離を詰めていく。 右のフック!左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。<unk> 選手もしっかりガードを固めてあの<unk> 選手のあのそれとちょっと気をつけてて欲しいですね。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> この距離でパンチを出せるか! 残りは5秒! さあ今度は<unk> も体をくっつけて行く! 右のストレート! <unk> も打ち返します! 残りは5秒! 両者打ち合いました。 最後まで打ち合いました! この両者のスタイルでもありましたが、少しパンチをもらったが、返して行きます<unk> です。</s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しふらついた! やはり前に出ますね。 今のは<unk> も狙っていたと思います。 どんどん行くでしょうね。<unk> 選手。 距離を詰めて行きます。 パンチを<unk> が集めてきました。 今のは<unk> も狙っていたと思います。 どんどん行くでしょうね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。 そうですね。足の位置も悪くなっているので。 ショートレンジのパンチ、ボディに打ち込んでいく青森。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しペースが落ちましたが、最後までしっかり走りきりました<unk> です。 終盤の戦いになってきます第7ラウンド。試合は残り6分。 食べないスタミナに自信があるってのがよく分かる試合ですね。 第7ラウンドまで終了しています。 二人の一戦は最終ラウンドへと移っていきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXts53TaxMU1",
        "outputId": "4170eb30-d1ce-4974-89ec-3ff1a7b757d1"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上から連打</s> 少しパンチを貰ったか<unk> が下がりました。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 左のジャブ!右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 残り1分。 高い勝率を誇る<unk> に対して、<unk> は序盤からしっかり自分のボクシングを進めてきて、序盤からしっかり自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分のボクシングを進めてきて、序盤からどんどん自分の\n",
            "ガードの上から連打</s> 残り1分。 高い勝率を誇る<unk> に対して、<unk> は序盤からしっかり自分のボクシングを進めてきて、序盤からパンチを集める。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 残り1分。 高い勝率を誇る<unk> に対して、<unk> は序盤からしっかり自分のボクシングを進めてきて、中盤終盤にかけてそのボクシングのスタイルを崩さなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 少しパンチを貰ったか<unk> がガードの上から連打し、少しガードが下がってきた。 少しパンチを貰った<unk> が、少し距離を詰めてきました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 少しパンチを貰ったか<unk> が下がりました。 返して行きます<unk> です。 が、<unk> も今あっと言わせるシーン作りましたね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 少しパンチを貰ったか<unk> が下がりました。 攻勢を強めてきました。 <unk> もパンチ力を持っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 少しパンチを貰ったか<unk> がガードを固めました。 残り1分。 高い勝率を誇る<unk> に対して、<unk> は序盤からしっかり自分のボクシングを進めてきて、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj4_nF4yxt4q",
        "outputId": "0a877002-da15-4928-ebd2-2af314309c03"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたノックアウトです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 少しクラっときたか。 <unk> が来るところにジャブを入れるだけで止まりはするので。 残りは10秒切りました。</s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート!こちらもしっかり手数を出して。</s> </s>\n",
            "倒れたノックアウトです。</s> 一気に試合がうごくというところ、左に右パンツあって行きます。 左のストレート! 残り1分。</s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErD4cWrSyard",
        "outputId": "d3c3fa22-462a-411e-c6ef-6c332ce4a640"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたKOです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "倒れた<unk> です。</s> 少しペースが落ちましたが、最後までしっかり食べました。<unk> 選手もいいボディを打ちましたね。 終盤の戦いになりましたが、最後までしっかり<unk> はパンチを打ちました。</s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 少し落ち着いたように見えますね。<unk> 選手。 少しずつ距離が縮まってきて、<unk> のパンチが当たるというのも見えてきました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 少し落ち着いた様子にも見えますが、<unk> には気をつけて欲しいですね。 終盤の戦いになってきます第7ラウンド</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 少し動きが止まってきたか。。 再び世界に向けての挑戦となっています。 56キロ契約8回戦で行われます</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 22歳、キャリア10戦目。もうすでに後はどう終わるかだということを話しています。 22歳の若武者相手にキャリアの多さ、引き出しの多さ、今日そこまでは存分に見せてきました。</s>\n",
            "倒れた<unk> です。</s> 22歳、キャリア10戦目。もうすでに後はどう終わるかだということを話しています。 右のジャブ。そして入ってきたところに左を合わせるといったパンチですかね。</s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 22歳、ベテランと若手! <unk> の右。 キャリア10戦目。もうすでに後はどう終わるかだということを話しています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 終盤の戦いになってきます第7ラウンド。 試合は残り6分。 食べないスタミナに自信があるってのがよく分かる試合ですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 少し落ち着いた様子の<unk> です。 復帰戦ですから、序盤からしっかり自分のボクシングを進めてきて、 中盤終盤にかけて自分のボクシングを進めてきて、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjlq2F41zC2j",
        "outputId": "e1e687e2-3785-4dba-91fd-0e1bebf2c65b"
      },
      "source": [
        "input = tokenizer.encode(\"右のフックから左右連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のフックから左右連打</s> ちょっとクラっときたか。<unk> の右、<unk> の右、左右、左右振り分けて行きます。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のフックから左右連打</s> ちょっとクラっときたか。<unk> の右、<unk> の右、左右、左右振り分けて行きます。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 少し<unk> が下がったように見えました。 左のストレート! <unk> が下がった! 左のボディ!</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のフックから左右連打</s> ちょっとクラっときたか。<unk> の右、<unk> の右、左右、左右振り分けて行きます。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 左のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlzLienS-drA",
        "outputId": "007b8904-7dcd-4ca4-8fd3-ca469a6ae4a8"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上からボディー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右!今<unk> 狙ってましたか。 いいジャブが出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右のストレート右のフック!右!今<unk> 狙ってましたか!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右のストレート!右!右のジャブ!右のストレート顔面をとらえた強いパンチ。右が非常に伸びている。右のストレートです。</s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右のストレート右のフック!右!今<unk> 狙ってましたか!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右のジャブ。左のフック。右!右のストレート右!右のフック!引かない!引かせません!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右!右のジャブ!今<unk> は右を返して行きます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 第7ラウンドも残り1分を切っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右のストレート右のフック!右!今<unk> 狙ってましたか!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右!今<unk> 選手はジャブを出していますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> この距離から両者引きませんね。 右!右のジャブ。右のストレート右のフック!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sQQtEbd-pXu",
        "outputId": "c12ac381-a8de-470d-c180-e894b90e11cb"
      },
      "source": [
        "input = tokenizer.encode(\"ジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ジャブ</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 右のストレート!右のストレート顔面をとらえた強いパンチ!右が非常に伸びている。右のストレートです! 今パンチを貰ってますが、少し<unk> が下がったように見えます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> <unk> ・<unk> が このラウンド 残り10秒となりました。 左のストレート! <unk> 選手は中盤にかけて<unk> 勝利、このコロナ禍での応援に来てくださる方に対して、面白い試合をしたいと、言っていますが。</s>\n",
            "ジャブ</s> さあ、この辺りの入りですけれども、<unk> さん、どんなとこに注目されてますか? 前回は<unk> さんの巻き上げに冷静に対処した<unk> さんましたが、今回はどうでしょうか?</s> </s> </s> </s> </s>\n",
            "ジャブ</s> 右のストレート右のフック右のフック<unk> さん 残り10秒 左のストレート!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 右のストレート!左のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> さあ、第6ラウンドに入っています。 左のストレート! <unk> 選手は中盤にかけて<unk> 勝利、このコロナ禍での応援に来てくださる方に対して、面白い試合をしたいと、言っていますが。</s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> さあ、この辺りの入りですけれども、<unk> さん、どんなとこに注目されてますか? ジャブそれと右のストレートを打っていきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW_A-HPXOvhl",
        "outputId": "4f8e5766-9928-4a4b-9b4b-63df7f0ebae9"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブ</s> このストレートどうでしょうか? このストレートどうでしょうか?右のフック、アッパー! 今のこの体のフリというかリズムの感じを見てると、<unk> 選手はしっかり落ち着いて試合展開ができているなと思いました。</s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 第3ラウンド、右のストレート! <unk> 選手は中盤にかけて<unk> 勝利、このコロナ禍での応援に来てくださる方に対して、面白い試合をしたいと、言っていますが。</s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 第7ラウンドに試合が入りました。 <unk> 、左のジャブ、残り2分間という戦いになりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 今回は<unk> を狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 第1ラウンド、左のフック! <unk> を<unk> Cして行きました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り2分間という戦いになりました。 第7ラウンドも残り1分を切っています。 終盤の戦い、ワンツーと打っていく。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 残り10秒。 第7ラウンドに試合が入りました。 <unk> 対<unk> の一戦、左のジャブ、右のストレートを繰り出していったのが<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2BheEMBO0Md",
        "outputId": "5efbdda8-335a-484c-d3c9-3c6035511f98"
      },
      "source": [
        "input = tokenizer.encode(\"右のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。右のストレート!右のストレート!今フック入りましたよね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。右のストレート!右のストレート顔面をとらえた強いパンチ。右が非常に伸びている。右のストレートです。</s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。右のストレート<unk> も返して行きます。ボディいいですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。右のストレート!右のストレート顔面をとらえた強いパンチ。右が非常に伸びている。右のストレートです。</s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか? はいそれとかよりも今少し後ろに体重が移動しました。左のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。かなりこのラウンド当たってますよね。右のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> このストレートどうでしょうか?いいジャブだと思います。右のストレート!右のストレート顔面をとらえた強いパンチ。右が非常に伸びている。右のストレートです。</s> </s> </s> </s> </s>\n",
            "右のジャブ</s> 対する<unk> です。いいジャブが出ていますね。お互いいいジャブだと思います。右のストレート! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のジャブ</s> このストレートどうでしょうか? このストレートどうでしょうか?右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のジャブ</s> このストレートどうでしょうか? はいそれとかよりも今少し後ろに体重が移動しました。左のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0lu5HoO28H",
        "outputId": "1893a159-3f95-460b-d6e5-0f7e1fca2009"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブからワンツー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "左のジャブからワンツー</s> 右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijuN8PUdPaik",
        "outputId": "f27e892f-1a05-46bf-96e0-e57920af9059"
      },
      "source": [
        "input = tokenizer.encode(\"左のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のアッパー</s> いま、第二ラウンドのゴングが鳴りました。 この辺りがやっぱり<unk> 本人が話していた、戦うたびに自分の動きとか可能性を感じてしまうというところなんでしょうね。</s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 前の手が重要になってくると思うので、<unk> 選手はサウスポー同士の戦いどのように見ていますか。</s>\n",
            "左のアッパー</s> <unk> が返す! この距離で左! <unk> の右のアッパー! ここはガードを固めた<unk>! 左のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。。。 右のストレート右のアッパー! ここはガードを固めています<unk> です。</s> </s> </s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 右のストレート! これより第4ラウンド、試合は中盤に移っていきます。</s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 この辺り、序盤の立ち上がりですが、<unk> さんはサウスポー同士の戦いどのように見ていますか。</s>\n",
            "左のアッパー</s> いま、第二ラウンドのゴングが鳴りました。 持ち味は非常に強いパンチが印象的なこの長井香織です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 この辺り、序盤の立ち上がりですが、<unk> さんはサウスポー同士の戦いどのように見ていますか。</s>\n",
            "左のアッパー</s> ここはガードを固めたほうがいいですね。ガードを固めたらジャブどんどん出して。ジャブでシャットアウトした方がいいです。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> が来るところにジャブを入れるだけで止まりはするので。 右のストレート右のアッパー! ここはガードを固めています<unk> です。</s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbGej1nPuhc",
        "outputId": "858566e2-4b03-4a8b-ab7f-46ccffe5981b"
      },
      "source": [
        "input = tokenizer.encode(\"右のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のアッパー</s> ここはガードを固めたほうがいいですね。ガードを固めたほうがいい相手は<unk> です。 右のアッパー!ガードを固めた<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはガードを固めたほうがいいですね。ガードを固めた方が<unk> はやりやすくなります。 返して行きます<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 少しクラっときたか。足の位置も悪くなっているので。 そうですね。足の位置も悪くなっているので。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 少しリズムがよくなってきたか。左のストレート! 残り2分間、ポイントになる有効打ををどんどん出せるかどうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 少しパンチが当たりましたでしょうか<unk> 。 連打を浴びるシーンもありますが、少しガードが下がってきたでしょうか<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 少しクラっときたか。<unk> の右。 <unk> の右。 ガードの上からでもしっかり手数を出して。最後は気持ちの勝負だ。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはガードを固めた左のフック!右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "右のアッパー</s> ここはガードを固めたいところ。ここはボディを返していく。さあ距離が近くなる! 右のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはガードを固めたほうがいいですね。ガードを固めたら今度は右のアッパー! ここはガードを固めたほうがいいですね。ガードを固めたら今度は右のアッパー!</s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj7ohJTrRIeW",
        "outputId": "2a98fd23-1f03-4338-a86d-432302bddcee"
      },
      "source": [
        "input = tokenizer.encode(\"アッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "アッパー</s> ました。<unk> にも輝いた選手です。 今のこの経歴紹介を聞いていても、<unk> 選手のここまでの戦歴、長さを感じますし、逆に<unk> 選手のシンプルな強さというのは感じますよね。</s>\n",
            "アッパー</s> これが4戦目。キャリア49戦目。ベテランと若手! <unk> 選手はこのペースで全然いいと思いますね。どんどん前に出る、<unk> 選手。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> これが4戦目。これまでデビューしてから負けを知りません。<unk> 、今リングインです。 地元の観客からは、大きな拍手が送られています。<unk> これが4戦目。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> <unk> 得意の右アッパー! 今の右のアッパー! あれからどういう風に変化しているのか、すごく楽しみです。 <unk> 得意の右アッパー!</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> いま、第二ラウンドのゴングが鳴りました。 この左のジャブ、アッパー! 持ち味は非常に強いパンチが印象的なこの長井香織です。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtGG0ILqRVbY",
        "outputId": "e6f0b030-5fd1-400c-d4a0-08ef16c0a421"
      },
      "source": [
        "input = tokenizer.encode(\"カウンター\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "カウンター</s> <unk> が<unk> を破っています。 第7ラウンドを終えて、非常に見応えのある試合が続いています。 ただ、第2ラウンドに<unk> は一つダウンを貰ってるんですよね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> が後半に入るにつれて、自分のペースに持って行こうとする<unk> です。 第4ラウンド終了です。 <unk> のパンチをもらっているシーンもありましたが、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> さん、ボディがいいんで、ボディからのフックの返しが、<unk> さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> が<unk> を破っています。 第7ラウンドに入っています。 左のストレート! <unk> 選手は中盤にかけて<unk> 勝利、このコロナ禍での応援に来てくださる方に対して、面白い試合をしたいと、言っていますが。</s>\n",
            "カウンター</s> <unk> さん、ボディがいいんで、ボディからのフックの返しが、<unk> さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> さん、ボディがいいんで、ボディからのフックの返しが、<unk> さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> この辺りがやっぱり<unk> 本人が話していた、戦うたびに自分の動きとか可能性を感じてしまうというところなんでしょうね。 可能性だらけですよね。お互いそうだと思うんですが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> この辺りがやっぱり<unk> 本人が話していた、戦うたびに自分の動きとか可能性を感じてしまうというところなんでしょうね。 可能性だらけですよね。お互いそうだと思うんですが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> さん、ボディがいいんで、ボディからのフックの返しが、<unk> さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> この辺りがやっぱり<unk> 本人が話していた、戦うたびに自分の動きとか可能性を感じてしまうというところなんでしょうね。 可能性だらけですよね。お互いそうだと思うんですが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piw5DZQ3UdGW",
        "outputId": "abb9ae93-36f9-45cf-9409-ffed74a797b5"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレートから左のフック!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合は残り6分。どんどん前に出るの<unk> 。右のストレート!近い距離になってしっかり手数を出して。最後は気持ちの勝負です。</s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は左のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレートから左のフック!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 右のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレートから左のフック!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレート!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレートから左のフック!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 左のフック!右のストレート右のフック!右のストレート<unk> も返していく! 試合展開は右のストレート!右のストレート<unk> も返していく!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_uAXTaYirD",
        "outputId": "b4a9a564-6d93-423b-8b73-1414ccd9045a"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "試合展開について</s> 序盤から、パンチを出せるか、気持ちとスタミナでカバーできるだけの自信があると話していました。ただ、序盤からどんどん全力で行っても、自分の強みはスタミナだと。</s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド、ここまで序盤は<unk> が落ち着いて、ただ強気のボクシングを見せている。</s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 序盤の話にもありましたが、右の前手の使い方ですが、<unk> はいいジャブが出ていますね。</s> </s> </s> </s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド、ダウンを食らった直後のラウンドは、ちょっと今までの戦い方に疑問を抱きながらという風に見えましたけれども。</s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド、ダウンを食らった直後のラウンドは、ちょっと今までの戦い方に疑問を抱きながらという風に見えましたけれども。</s>\n",
            "試合展開について</s> 左のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 右のジャブを打っていって、どんどん自分のペースに持って行こうとする<unk> です。 第3ラウンド前半が終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS9RLYW2YehG",
        "outputId": "e05b3be0-f0ed-45ff-b499-acd40a53fec6"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシング\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボクシング</s> <unk> 対<unk> の一戦。 2017年の3月にプロテストを合格して順調に勝利を掴めコロナの影響でまた私よりなくならなかったそしてプロ3戦目で<unk> との一戦になりました</s> </s>\n",
            "ボクシング</s> 左のジャブ。左のフック!右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s>\n",
            "ボクシング</s> この6ラウンドの戦い方なんですが、<unk> さんは、どういうふうに考えますか? 王者決定戦で、1/3が回ったっていう風になりますか?</s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> これは新世代だ。リング上、お互いがお互いをいなす。 第4ラウンド終了です。 <unk> 、ダウンをもらいました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 56キロ契約8回戦で行われます、<unk> 対<unk> の一戦です。 左のジャブを打っていったのは、赤のグローブ<unk> 、<unk> ジム所属です。</s> </s> </s> </s> </s>\n",
            "ボクシング</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 少しパンチを貰ったか<unk> が終わりにしようか。 なるほど。 <unk> が終わりにしようか。 なるほど。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> <unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDqzn1wXWXro",
        "outputId": "abd82f92-558c-42e7-994c-c105088c24d0"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボクシングの試合について</s> 左のジャブ。 そして右のストレート右のフック! 試合時間が短いという中で、手数もどんどん出していきたい、両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。 そして右のストレート右のフック! 試合全体を通して改めてこの<unk> か選手のボクシングはどう映りましたか?</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。 そして右のストレート右のフック! 試合時間が短いという中で、手数もどんどん出していきたい、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック! これ<unk> 選手の右ですね。 距離が近い!右!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック!右のストレート<unk> も返していく! 試合全体を通して改めてこの<unk> か選手のボクシングはどう映りましたか?</s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s>\n",
            "ボクシングの試合について</s> 左のジャブ。そして右のストレート右のフック!右のストレート<unk> も返していく! こういうところ気持ちのぶつかり合い、しっかりパンチも出ていますね。</s> </s>\n",
            "ボクシングの試合について</s> まず第1ラウンドをその日に始めて、 その日に相手がどういった出方をしてくるか、 見切ったと言ってもいいんですかね?</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8DlV3bYWia9",
        "outputId": "46210114-3a3e-43e1-b80c-a55b77506b6d"
      },
      "source": [
        "input = tokenizer.encode(\"今回のボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "今回のボクシングの試合について</s> <unk> さんは試合開始からして手数も出ている中で、<unk> さんのパンチが当たるというのもありますし、そういうのも狙ってっていますか?</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは中盤にかけて自分のペースをクシングを貫いていきました。 そのあたり、戦略というのはどうでしょうか? 自分のペースに持って行こうと、自分のスタイル崩してもやったので、このまま行けたらいいかなとは思いますね。</s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> 選手は判定に移りますが、どうですか、これ判定に移りますが。 少し<unk> のパンチをもらっているシーンもありましたが、大盛りに対して<unk> はちょっとパンチが当たりましたね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは試合開始からして手数も出ている中で、<unk> さんのパンチをもらっているシーンもありましたね。 少し<unk> さんのパンチが当たるというところも出ました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> 選手は中盤にかけて自分のペースをクシングを貫いていきました。 右のジャブ!そして左のフック!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは序盤からパンチを集めて、どんどん手数を出してという試合展開になっていますね。 ジャブが顔面を捉えました<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは試合全体を通して改めてこの<unk> か選手のボクシングはどう映りましたか? すごい接近戦がうまいですし、いろんなうまさが詰まったボクシングを展開しましたね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは試合序盤から、うまいボクシングを見せているなと思いました。 ただ、序盤からどんどん全力で行っても、<unk> 選手がスタミナが切れてきたのか、体が流れ始めてきているので、そうなってくると、<unk> 選手のペースになっていきそうな気がしますね。</s>\n",
            "今回のボクシングの試合について</s> <unk> さんは序盤から、うまいボクシングを見せているなと思いました。 第3ラウンドまでが終わっています。 <unk> さんは序盤からどんどん全力で行っても、自分の強みはスタミナだと。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> さんは試合開始からして手数も出ている中で、<unk> さんのパンチをもらっているシーンもありましたね。 どうですか、この辺り、序盤のラッシュのシーンですが、<unk> さんはよく出ていますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5QCrAIYJxp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}