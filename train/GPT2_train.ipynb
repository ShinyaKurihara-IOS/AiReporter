{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6296207f23be4ba3b1b336979035d37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7bb0019d4de5453388bff7fe9cb7ac37",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77819a14d5e3426493102a66b60d2685",
              "IPY_MODEL_3d7d953d1d6a43e4b38fbe103127b87e"
            ]
          }
        },
        "7bb0019d4de5453388bff7fe9cb7ac37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77819a14d5e3426493102a66b60d2685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f5d6115c5fdc4947b9a93a8b4de5e7a5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 805634,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 805634,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6477fb64df2743f882f88c22a9e383ef"
          }
        },
        "3d7d953d1d6a43e4b38fbe103127b87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d33c12bf1bf84b5d92a4755302e4b4a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 806k/806k [00:35&lt;00:00, 22.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1856485cae54cfbac7b77e455d932b3"
          }
        },
        "f5d6115c5fdc4947b9a93a8b4de5e7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6477fb64df2743f882f88c22a9e383ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d33c12bf1bf84b5d92a4755302e4b4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1856485cae54cfbac7b77e455d932b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bdc585aa97d64bfe8c17e72f045b14cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f53cb2a4331647e2a1f270d2428804a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_79a6a04590ba4f3087d3e16c3022cb3a",
              "IPY_MODEL_7f9a4e53de3645dfb41d1865f18491d7"
            ]
          }
        },
        "f53cb2a4331647e2a1f270d2428804a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79a6a04590ba4f3087d3e16c3022cb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5666d7673d2345f3a7da7ef9133ae8cf",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 153,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 153,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a4b57dbf4c54b98866e1b3747906adf"
          }
        },
        "7f9a4e53de3645dfb41d1865f18491d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_adb3ebad2b3e45ffb864412e1d4d5c3e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 153/153 [00:03&lt;00:00, 47.0B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b062db46ef4747bf9a316fd974a61d1d"
          }
        },
        "5666d7673d2345f3a7da7ef9133ae8cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a4b57dbf4c54b98866e1b3747906adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adb3ebad2b3e45ffb864412e1d4d5c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b062db46ef4747bf9a316fd974a61d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9426e0b8cea44a17a4df51c2dcf5c76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c351f86e02c04cac9716d956eb62e8a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_402f8ba77acc4145b6da3fc8e1e55f2c",
              "IPY_MODEL_6864022e47c44125bbc286304705643d"
            ]
          }
        },
        "c351f86e02c04cac9716d956eb62e8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "402f8ba77acc4145b6da3fc8e1e55f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_105eecbd39624c2395a4017cf1fd3236",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 225,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 225,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9dec9b67e7f4ac89dc32aef074f3802"
          }
        },
        "6864022e47c44125bbc286304705643d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72370e26f6544b70997a0104d7977563",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 225/225 [00:01&lt;00:00, 125B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e1050cea292424da8df21bd76f3859a"
          }
        },
        "105eecbd39624c2395a4017cf1fd3236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9dec9b67e7f4ac89dc32aef074f3802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72370e26f6544b70997a0104d7977563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e1050cea292424da8df21bd76f3859a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHfd98YKpAa"
      },
      "source": [
        "以下の設定になっていること  \n",
        "\n",
        "ランタイム > ランタイムのタイプを変更  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  \n",
        "\n",
        "編集 > ノートブックの設定  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZUtGFrIE0N",
        "outputId": "87eb39f6-fbc6-4001-b917-b3df3d918f82"
      },
      "source": [
        "# googleドライブをマウント\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "# 表示されるリンクをクリックして、アクセスを許可して、最後に表示される文字列を以下の入力欄に入れる"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6Phq7IINCz",
        "outputId": "791d0158-3bef-4cae-8a3c-4671f4cace94"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7WZHPnIqSv",
        "outputId": "dc000eb5-0294-4d35-b9ca-ee1e0cf3b94d"
      },
      "source": [
        "# importで使う必要があるので、インストールがランタイム切れるごとに必要\n",
        "# インストール後にランタイムの再起動を行わないとT5Tokenizerが見つからない\n",
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# ドライブに保存してるものでインストール\n",
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n",
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, datasets\n",
            "Successfully installed datasets-1.2.1 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWjtNtvnRTPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2152a46-5a0f-421e-ef6a-376ee786a8b5"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tepd4-4WCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtynWz0u4IT"
      },
      "source": [
        "CLM（Causal Language Modeling）: GPT、GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKmxERxt7Io"
      },
      "source": [
        "・model_name_or_path: モデルのチェックポイント（モデルを最初から学習しない場合）  \n",
        "・model_type: モデルの種別（モデルを最初から学習する場合）  \n",
        "・config_name: コンフィグ名（model_nameと同じでない場合）  \n",
        "・tokenizer_name: トークナイザー名（model_nameと同じでない場合）  \n",
        "・cache_dir: キャッシュフォルダ  \n",
        "・use_fast_tokenizer: Fastトークナイザーを使用するかどうか  \n",
        "・model_revision: 使用するモデルの特定のバージョン  \n",
        "・use_auth_token: 「transformers-cli login」の実行時に生成されたトークンを使用するかどうか  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0KKz6auJXj"
      },
      "source": [
        "・dataset_name: データセット名  \n",
        "・dataset_config_name: データセットのコンフィグ名  \n",
        "・train_file: 学習データ（テキストファイル）  \n",
        "・validation_file: 検証データ（テキストファイル）  \n",
        "・overwrite_cache: キャッシュの上書き  \n",
        "・validation_split_percentage: 学習データから使われる検証データの割合（検証データがない場合）  \n",
        "・max_seq_length: トークン化後の最大合計入力シーケンス長  \n",
        "・preprocessing_num_workers: 前処理に使用するプロセス数  \n",
        "・block_size: トークン化後のオプションの入力シーケンス長  \n",
        "・max_train_samples: 学習データの最大数  \n",
        "・max_val_samples: 検証データの最大数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9LIvEaz6Pst"
      },
      "source": [
        "    # GPT2のモデルファイルを指定\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    # 学習ファイル\n",
        "    --train_file=train.txt \\\n",
        "    # 評価データファイル\n",
        "    --validation_file=train.txt \\\n",
        "    # トレーニングを実施する\n",
        "    --do_train \\\n",
        "    # 評価を実施する\n",
        "    --do_eval \\\n",
        "    # 学習回数（エポック数）\n",
        "    --num_train_epochs=30 \\\n",
        "    # チェックポイントの保存間隔\n",
        "    --save_steps=5000 \\\n",
        "    # チェックポイントの保持数\n",
        "    --save_total_limit=3 \\\n",
        "    # T5Tokenizer.model_max_length=1024をチャンクサイズとして使用するかblock_sizeで指定するかを設定する（設定しないとtokenizerの超大なサイズから1024になる）メモリに乗るように調整する必要がある（バッチサイズとの兼ね合い）\n",
        "    --block_size=512 \\\n",
        "    # GPU1つあたりの学習バッチサイズ\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    # GPU1つあたりの評価バッチサイズ\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    # モデルとチェックポイントの出力先\n",
        "    --output_dir=output/ \\\n",
        "    # 出力先の上書きの許可\n",
        "    --overwrite_output_dir=True \\\n",
        "    # T5Tokenizerで高速化ライブラリがあれば使用する\n",
        "    --use_fast_tokenizer=False\n",
        "\n",
        "### 学習する回数\n",
        "エポック数 * (データ文字数 / ブロックサイズ):1つの学習データ / バッチサイズ  \n",
        "    エポック数 = 1  \n",
        "    データ文字数 = 22142  \n",
        "    データの文字数だと数字が合わないので、tokenizeした結果の形態素数による？\n",
        "    ブロックサイズ = 512  \n",
        "    データ文字数 / ブロックサイズ = 43  \n",
        "    バッチサイズ = 1  \n",
        "    Total optimization steps = 29  \n",
        "\n",
        "\n",
        "バッチサイズ = 2  \n",
        "バッチサイズを2にすることで、トータルの実行回数が減っている  \n",
        "Total optimization steps = 15  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zS2rIpZIfXB",
        "outputId": "d5b29a9e-c54a-4b0c-b66d-a6186ae54ad2"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs=300 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --block_size=256 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --output_dir=output/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 12:28:23.411625: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/28/2021 12:28:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/28/2021 12:28:32 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output/, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=300.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Jul28_12-28-31_69031f40117e, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 1.75MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-3a09e1cc42b81adc (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-3a09e1cc42b81adc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-3a09e1cc42b81adc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:33,887 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdq2xc46w\n",
            "Downloading: 100% 654/654 [00:00<00:00, 511kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:34,252 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:34,252 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:463] 2021-07-28 12:28:34,253 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.6837b013c474e795389ec2ae1d273d297dec5b156a42fdc819053b1fc8d86982\n",
            "[INFO|configuration_utils.py:499] 2021-07-28 12:28:34,253 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:34,602 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0jw6co9f\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 15.1MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:34,760 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:34,760 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:35,455 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp541vzxg5\n",
            "Downloading: 100% 153/153 [00:00<00:00, 121kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:35,795 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:35,796 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:36,143 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsiz4u53j\n",
            "Downloading: 100% 225/225 [00:00<00:00, 161kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:28:36,487 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:28:36,487 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,828 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.170e22986e7aeec289e1039c35e3e98e7c30b748b2dcba20ad5425a788c3d78f\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-07-28 12:28:36,829 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-07-28 12:28:37,243 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpczfjp7x_\n",
            "Downloading: 100% 1.37G/1.37G [00:24<00:00, 56.7MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-07-28 12:29:01,463 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|file_utils.py:1393] 2021-07-28 12:29:01,463 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1051] 2021-07-28 12:29:01,464 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.c74ec66be237ef806c57a59fff7b47b3a5f5ed1d91cb9463152da68ee9a5154b\n",
            "[INFO|modeling_utils.py:1167] 2021-07-28 12:29:10,624 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-07-28 12:29:10,624 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 2/2 [00:00<00:00, 22.17ba/s]\n",
            "100% 2/2 [00:00<00:00, 24.12ba/s]\n",
            "[INFO|trainer.py:946] 2021-07-28 12:29:18,838 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-07-28 12:29:18,838 >>   Num examples = 67\n",
            "[INFO|trainer.py:948] 2021-07-28 12:29:18,838 >>   Num Epochs = 300\n",
            "[INFO|trainer.py:949] 2021-07-28 12:29:18,839 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:950] 2021-07-28 12:29:18,839 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:951] 2021-07-28 12:29:18,839 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-07-28 12:29:18,839 >>   Total optimization steps = 5100\n",
            "{'loss': 0.7468, 'learning_rate': 4.5098039215686275e-05, 'epoch': 29.41}\n",
            "{'loss': 0.031, 'learning_rate': 4.0196078431372555e-05, 'epoch': 58.82}\n",
            "{'loss': 0.0212, 'learning_rate': 3.529411764705883e-05, 'epoch': 88.24}\n",
            "{'loss': 0.0174, 'learning_rate': 3.0392156862745097e-05, 'epoch': 117.65}\n",
            "{'loss': 0.016, 'learning_rate': 2.5490196078431373e-05, 'epoch': 147.06}\n",
            "{'loss': 0.0151, 'learning_rate': 2.058823529411765e-05, 'epoch': 176.47}\n",
            "{'loss': 0.0143, 'learning_rate': 1.568627450980392e-05, 'epoch': 205.88}\n",
            "{'loss': 0.0137, 'learning_rate': 1.0784313725490197e-05, 'epoch': 235.29}\n",
            "{'loss': 0.0133, 'learning_rate': 5.882352941176471e-06, 'epoch': 264.71}\n",
            "{'loss': 0.0132, 'learning_rate': 9.80392156862745e-07, 'epoch': 294.12}\n",
            " 98% 5000/5100 [22:24<00:26,  3.79it/s][INFO|trainer.py:1558] 2021-07-28 12:51:43,570 >> Saving model checkpoint to output/checkpoint-5000\n",
            "[INFO|configuration_utils.py:314] 2021-07-28 12:51:43,574 >> Configuration saved in output/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-07-28 12:51:49,125 >> Model weights saved in output/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-07-28 12:51:49,130 >> tokenizer config file saved in output/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-07-28 12:51:49,133 >> Special tokens file saved in output/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-07-28 12:51:49,140 >> Copy vocab file to output/checkpoint-5000/spiece.model\n",
            "100% 5100/5100 [23:28<00:00,  3.89it/s][INFO|trainer.py:1129] 2021-07-28 12:52:47,825 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1408.9868, 'train_samples_per_second': 3.62, 'epoch': 300.0}\n",
            "100% 5100/5100 [23:28<00:00,  3.62it/s]\n",
            "[INFO|trainer.py:1558] 2021-07-28 12:52:48,103 >> Saving model checkpoint to output/\n",
            "[INFO|configuration_utils.py:314] 2021-07-28 12:52:48,473 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-07-28 12:53:05,525 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-07-28 12:53:05,968 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-07-28 12:53:06,289 >> Special tokens file saved in output/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-07-28 12:53:06,758 >> Copy vocab file to output/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-07-28 12:53:07,210 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   epoch                      =     300.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   init_mem_cpu_alloc_delta   =       1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,210 >>   init_mem_cpu_peaked_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   init_mem_gpu_alloc_delta   =    1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   init_mem_gpu_peaked_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_cpu_alloc_delta  =       0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_cpu_peaked_delta =     126MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_gpu_alloc_delta  =    3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_mem_gpu_peaked_delta =    4657MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_runtime              = 1408.9868\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_samples              =        67\n",
            "[INFO|trainer_pt_utils.py:661] 2021-07-28 12:53:07,211 >>   train_samples_per_second   =      3.62\n",
            "CPU times: user 17.4 s, sys: 3.3 s, total: 20.7 s\n",
            "Wall time: 26min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu4AvG5Ik-p"
      },
      "source": [
        "# from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# # トークナイザーとモデルの準備\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"output/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xllJ0rN9Fuyk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "6296207f23be4ba3b1b336979035d37d",
            "7bb0019d4de5453388bff7fe9cb7ac37",
            "77819a14d5e3426493102a66b60d2685",
            "3d7d953d1d6a43e4b38fbe103127b87e",
            "f5d6115c5fdc4947b9a93a8b4de5e7a5",
            "6477fb64df2743f882f88c22a9e383ef",
            "d33c12bf1bf84b5d92a4755302e4b4a4",
            "f1856485cae54cfbac7b77e455d932b3",
            "bdc585aa97d64bfe8c17e72f045b14cc",
            "f53cb2a4331647e2a1f270d2428804a8",
            "79a6a04590ba4f3087d3e16c3022cb3a",
            "7f9a4e53de3645dfb41d1865f18491d7",
            "5666d7673d2345f3a7da7ef9133ae8cf",
            "4a4b57dbf4c54b98866e1b3747906adf",
            "adb3ebad2b3e45ffb864412e1d4d5c3e",
            "b062db46ef4747bf9a316fd974a61d1d",
            "9426e0b8cea44a17a4df51c2dcf5c76d",
            "c351f86e02c04cac9716d956eb62e8a9",
            "402f8ba77acc4145b6da3fc8e1e55f2c",
            "6864022e47c44125bbc286304705643d",
            "105eecbd39624c2395a4017cf1fd3236",
            "f9dec9b67e7f4ac89dc32aef074f3802",
            "72370e26f6544b70997a0104d7977563",
            "6e1050cea292424da8df21bd76f3859a"
          ]
        },
        "outputId": "e12aaeb5-2302-4e1c-cf05-864cd0d2cb74"
      },
      "source": [
        "# もちろんだが、Autoでも直指定でも同じ結果にはなっている\n",
        "# https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
        "from transformers import T5Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"output/\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6296207f23be4ba3b1b336979035d37d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=805634.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdc585aa97d64bfe8c17e72f045b14cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=153.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9426e0b8cea44a17a4df51c2dcf5c76d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=225.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_K7EOdMPVZ",
        "outputId": "ed6651c1-6de1-424b-969f-1e63ed888986"
      },
      "source": [
        "# 推論\n",
        "# https://huggingface.co/blog/how-to-generate\n",
        "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate\n",
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブ 残り10秒!\n",
            "左のジャブ 1ラウンド10左のフック!\n",
            "左のジャブ 1ラウンド2r\n",
            "左のジャブ 残り2分間という戦いになったが、しっかりお互いがパンチを出し合う中での戦いになった。\n",
            "左のジャブ 残り2分間、そのボクサー人生にとって、これ以上ないような勝利をつかみたいと話していた。\n",
            "左のジャブ 残り2分間という戦いになりました。\n",
            "左のジャブ このストレート!\n",
            "左のジャブ 残り2分間という戦いになりました。\n",
            "左のジャブ 残り 2分間という戦いになるが、しっかりガードして右のストレート!\n",
            "左のジャブ 次は右のストレート右のフック!右のストレートも返していく!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB5AQ9pQoyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16009c1-f79f-4af6-e416-9ccfb55e4398"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック このストレートどうでしょうか?\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-VUAXfoEE9K",
        "outputId": "4f0a4078-268f-45cc-bcdd-8c4d66c6dfd1"
      },
      "source": [
        "input = tokenizer.encode(\"ボディ\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボディ このダウンを奪って一気に行こうかというところから、もう一度ギアを落としました。\n",
            "ボディ このダウンフォース、かなり頑張ってますね。\n",
            "ボディ 得意の右アッパー!\n",
            "ボディ 選手は152cmで体重が46キロですから、およそ5倍の重りがかかっているということになります。\n",
            "ボディ 選手は3回戦で行われます。\n",
            "ボディ このダウンを奪って一気に行こうかというところから、もう一度ギアを落としました。\n",
            "ボディ ただ、選手はそれだけではありませんでした。\n",
            "ボディ このストレートのまいは、まさにのですね。\n",
            "ボディ 選手は25歳これがキャリア10戦目。\n",
            "ボディ これが4戦目。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zPc_EPFEJi_",
        "outputId": "e597e595-4670-48b8-fbf8-94dd36fc95c1"
      },
      "source": [
        "input = tokenizer.encode(\"右のストレート\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のストレート さあ今度はの右。\n",
            "右のストレート 少し行き過ぎになったか。\n",
            "右のストレート ここはガードを固めたほうが良いかなと。\n",
            "右のストレート いまパンチをもらっているです。\n",
            "右のストレート 少しが下がったように見えました。\n",
            "右のストレート ここはガードを固めたガードを固めた左のジャブ。\n",
            "右のストレート さあ第2ラウンドが始まりました。\n",
            "右のストレート 少しクラっときたか。\n",
            "右のストレート さん、ボディがいいんで、ボディからのフックの返しが、さんの素早いスピードの打ち終わりを狙ってるイメージで、いいじゃないですか。\n",
            "右のストレート 少しクラっときたか。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHIHS0H7EMe1",
        "outputId": "224ab7e6-1f9b-4880-8a94-6b49a5cd0fa7"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上から がを打ちました。\n",
            "ガードの上から この距離から両者引きませんね。\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から この距離で両者全く引きませんね。\n",
            "ガードの上から これよりいよいよ今日の試合のメインイベントお届けします\n",
            "ガードの上から この距離で両者引きませんね。\n",
            "ガードの上から でもしっかり手数を出して。\n",
            "ガードの上から 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。\n",
            "ガードの上から これはの距離か?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4H6oqsAKrlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxas9R2LKrt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiAl1wGnF6kf"
      },
      "source": [
        "tokenizerの中身を確認  \n",
        "\\<s\\>の意味合いを表示  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYhyTw7Ea7-",
        "outputId": "8daf467e-f137-4f24-960e-71f20db23a62"
      },
      "source": [
        "# model.generateの結果はtokenizerのindexベクトル\n",
        "output[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    9,  5682,    10, 12276,     2,     9,     0,    20,  2115,    18,\n",
              "         5456,  2199,     7,    80,     0,    10,   819,     8,     2,     2,\n",
              "            2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym78sIbQFIMn",
        "outputId": "0e4f0dcc-3a88-4653-d17f-f3ce7328ac4b"
      },
      "source": [
        "# 記号の意味\n",
        "tokenizer.all_special_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNCWHrgWG9bY",
        "outputId": "3574002a-6b91-4aab-b276-e7e660b76436"
      },
      "source": [
        "# 記号に対応するindex\n",
        "tokenizer.all_special_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0, 5, 3, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCg-F5fAFlaa"
      },
      "source": [
        "{\"bos_token\": \"\\<s\\>\", \"eos_token\": \"\\</s\\>\", \"unk_token\": \"<unk>\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}  \n",
        "bos_token: 文の先頭（Begin of sequence token）  \n",
        "eos_token: 文のおしり（End of Sequence token）  \n",
        "unk_token: IDに変換できない文字（Unknown token）  \n",
        "sep_token: 文と文を区切り目（The separator token）  \n",
        "pad_token: パッディング（The token used for padding）  \n",
        "cls_token: 分類用（cls_token）  \n",
        "mask_token: マスク（The token used for masking values）  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5xE9Myg0WR"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode  \n",
        "sequences : torch.Tensorの配列を入力値として指定  \n",
        "  トークン化された入力IDのリスト  \n",
        "skip_special_tokens : デコード時に特殊なトークンを削除するかどうか(eos_tokenとかを消す)(デフォルト:False)  \n",
        "clean_up_tokenization_spaces : トークン化スペースをクリーンアップするかどうか(デフォルト:True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqZhkicXiBP8"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode  \n",
        "\n",
        "text (str, List[str] or List[int]) – 入力文字列  \n",
        "\n",
        "text_pair (str, List[str] or List[int], optional) – ペアとなるもう一つを入力する場合のオプション  \n",
        "\n",
        "add_special_tokens (bool, optional, defaults to True) – 上記で定義していない特別なトークンをモデルに適用するか\n",
        "\n",
        "padding (bool, str or PaddingStrategy, optional, defaults to False) –パディングして入力シーケンスを揃える場合  \n",
        "\n",
        "truncation (bool, str or TruncationStrategy, optional, defaults to False) –逆に長過ぎる場合に、一定の長さに揃える場合\n",
        "\n",
        "max_length (int, optional) –トランケーション・パディングで使用するオプション\n",
        "\n",
        "stride (int, optional, defaults to 0) – max_lengthで切り捨てられたのを調整する  \n",
        "\n",
        "is_split_into_words (bool, optional, defaults to False) – 単語分割が既にされている場合True\n",
        "\n",
        "pad_to_multiple_of (int, optional) – 指定された値の倍数になるようにシーケンスをパッドする  \n",
        "\n",
        "return_tensors (str or TensorType, optional) – python整数のリストの代わりにテンソルを返す  \n",
        "'tf': Return TensorFlow tf.constant objects.  \n",
        "'pt': Return PyTorch torch.Tensor objects.  \n",
        "'np': Return Numpy np.ndarray objects.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZXgdwCNECfn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paoQ0wFiNsyp"
      },
      "source": [
        "* https://huggingface.co/blog/how-to-generate  \n",
        "* https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate  \n",
        "* https://note.com/npaka/n/n5d296d8ae26d  \n",
        "* https://note.com/npaka/n/n96dde45fdf8d  \n",
        "\n",
        "### GPT2LMHeadModel.generateのオプションを確認  \n",
        "\n",
        "input_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) – 入力シーケンス  \n",
        "\n",
        "max_length (int, optional, defaults to model.config.max_length) – 生成されるシーケンスの最大長を指定（学習に使用した256の長さが良さそう）\n",
        "\n",
        "max_new_tokens (int, optional, defaults to None) – 現在のトークン数に関係なく、生成されるシーケンスの最大長を指定\n",
        "\n",
        "min_length (int, optional, defaults to 10) – 生成されるシーケンスの最小の長さ\n",
        "\n",
        "do_sample (bool, optional, defaults to False) – 単語予測にサンプリングを入れてランダム性を導入する。（デフォルトは greedy decoding の生成）  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d  \n",
        "\n",
        "early_stopping (bool, optional, defaults to False) – ビーム探索で、num_beams個の文が生成された時点で、ビーム探索を終了するかどうか  \n",
        "\n",
        "num_beams (int, optional, defaults to 1) – ビームサーチを行うビームの数。1はビームサーチを行わないことを意味します。  \n",
        "\n",
        "temperature (float, optional, defaults to 1.0) – 次のトークンの確率をモジュール化するために使用される値です。温度（デフォルト1、推奨0.7〜1.0）ボルツマン分布のパラメータ。小さい値ではランダムな補完が減り，0では決まりきった繰り返しの文になる。大きい値ではより様々な補完がされる。  \n",
        "\n",
        "top_k (int, optional, defaults to 50) – top-k-filteringのために保持する最高確率の語彙トークンの数です。確率が大きめな候補からサンプリングしてランダム性を導入する際の候補を何個にするか。40が一般的に良い値  \n",
        "\n",
        "top_p (float, optional, defaults to 1.0) – 生成テキストを累積確率に制限 (0で制限なし) float < 1に設定すると、top_p以上の確率を持つ最も確率の高いトークンのみが生成のために保持されます。  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d#top-p-(nucleus)-sampling  \n",
        "\n",
        "repetition_penalty (float, optional, defaults to 1.0) – 反復ペナルティのパラメータです。1.0はペナルティなし。すでに生成された単語や文脈に属する単語にペナルティを与えるために使用することができます。反復防止にはかなり効果的ですが、異なるモデルやユースケースには非常に敏感なようで、議論がある。  \n",
        "\n",
        "pad_token_id (int, optional) – PADトークンを指定\n",
        "\n",
        "bos_token_id (int, optional) – bosトークンを指定\n",
        "\n",
        "eos_token_id (int, optional) – eosトークンを指定\n",
        "\n",
        "length_penalty (float, optional, defaults to 1.0) – 長さに対する指数関数的なペナルティ。1.0はペナルティがないことを意味します。1.0未満の値を設定すると、モデルは短い配列を生成するようになり、1.0以上の値を設定すると、モデルは長い配列を生成するようになります。\n",
        "\n",
        "no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、そのサイズのngramはすべて一度しか発生しません。最も一般的な n-grams ペナルティは、すでに見た n-gramsを作る可能性のある次の単語の確率を 0 に手動で設定することで、n-gramsが 2 回出現しないようにするものです。  \n",
        "\n",
        "encoder_no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、encoder_input_idsに出現したそのサイズのすべてのngramは、decoder_input_idsには出現しません。\n",
        "\n",
        "bad_words_ids (List[List[int]], optional) – 生成してはいけないトークンのidのリスト。トークンのIDは以下で確認  \n",
        "tokenizer(bad_word, add_prefix_space=True).input_ids  \n",
        "\n",
        "num_return_sequences (int, optional, defaults to 1) – バッチ内の各要素について、独立して計算された戻り値の配列の数(返却される結果の数)。返されるべき最高得点のBeamの数を設定します。ただし、num_return_sequences <= num_beams とします。\n",
        "\n",
        "max_time (float, optional, defaults to None) – 計算の実行を許可する最大時間を秒単位で指定します。割り当てられた時間が経過しても、生成は現在のパスを終了します。  \n",
        "\n",
        "attention_mask (torch.LongTensor of shape (batch_size, sequence_length), optional) – パディングされたトークンのインデックスに対してアテンションを行わないようにするためのマスクです。マスクの値は [0, 1] で、マスクされていないトークンには 1、マスクされたトークンには 0 です。提供されていない場合は、パッドトークンをマスクするinput_idsと同じ形のテンソルがデフォルトになります。attentionで予測するための配列を作るので、マスクすると候補に出なくなる。入力シーケンスに対して同じ長さで指定する\n",
        "\n",
        "decoder_start_token_id (int, optional) – エンコーダ・デコーダモデルがbosとは異なるトークンでデコードを開始した場合、そのトークンのid。\n",
        "\n",
        "use_cache – (bool, optional, defaults to True): 過去の最後のキー／バリューの注目度（モデルに該当する場合）を利用して、デコーディングを高速化するかどうか。  \n",
        "\n",
        "num_beam_groups (int, optional, defaults to 1) – num_beamsを分割するグループの数（ビームの異なるグループ間の多様性を確保するため）。\n",
        "\n",
        "diversity_penalty (float, optional, defaults to 0.0) – この値は、ある時点で他のグループのビームと同じトークンを生成した場合、ビームのスコアから差し引かれます。なお、ダイバーシティペナルティは、グループビーム検索が有効な場合にのみ有効です。  \n",
        "\n",
        "prefix_allowed_tokens_fn – (Callable[[int, torch.Tensor], List[int]], optional):提供された場合、この関数は、各ステップで許可されたトークンのみにビーム検索を制約します。提供されない場合、制約は適用されません。この関数は2つの引数をとります：バッチID batch_id と input_id です。これは、バッチID batch_idと以前に生成されたトークンinput_idsを条件として、次の生成ステップで許可されたトークンのリストを返さなければなりません。この引数は、「自己回帰的実体検索」で説明されているように、接頭辞を条件とした制約付き生成に役立ちます。  \n",
        "\n",
        "output_attentions (bool, optional, defaults to False) – すべてのアテンションレイヤーのアテンションテンソルを返すかどうか。  \n",
        "\n",
        "output_hidden_states (bool, optional, defaults to False) – すべてのレイヤーの隠れた状態を返すかどうか。  \n",
        "\n",
        "output_scores (bool, optional, defaults to False) – 予測スコアを返すかどうか。  \n",
        "\n",
        "return_dict_in_generate (bool, optional, defaults to False) – 単なるタプルではなく、ModelOutputを返すかどうか。  \n",
        "\n",
        "forced_bos_token_id (int, optional) – decoder_start_token_idの後に、最初に生成されるトークンとして強制的に使用するトークンのidです。mBARTのような多言語モデルで、最初に生成されるトークンがターゲット言語のトークンである必要がある場合に便利です。（一番最初に生成される単語を指定してしまう。）  \n",
        "\n",
        "forced_eos_token_id (int, optional) – max_lengthに達したときに、最後に生成されたトークンとして強制的に使用するトークンのidです。(最後をわかりやすくして、途中で切られたのを知らせる)  \n",
        "\n",
        "remove_invalid_values (bool, optional) – 生成方法がクラッシュするのを防ぐために、モデルの可能性のあるnanとinfの出力を削除するかどうか。remove_invalid_valuesを使うと生成が遅くなることに注意してください。  \n",
        "\n",
        "synced_gpus (bool, optional, defaults to False) – max_lengthまでwhileループを続けて実行するかどうか  \n",
        "\n",
        "最新の研究により、単純な Beam Search や Greedy Search が同じ単語列の繰り返しを発生させてしまうのは、decoding に問題があるのではなくモデルの学習自体に問題があるとされています。また、Top-K や Top-p のようなサンプリングによる decoding であってもそうした単語列の繰り返しは発生しうるそうです。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIarpN7BN1HJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisPBDCsN1OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b0379f-01f2-4554-bf79-c2381b6da396"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\")\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック 今回はを狙いたいと話していましたが、気持ちのぶつかり合いだ。 距離を詰めてお互い打ち合う!\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!\n",
            "左のフック これがもっと距離が近くなると当たるというところなんですが、なかなか捉えられません。 少しリズムは良くってきた。 左のボディです!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS8z5iRxN1Tn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}