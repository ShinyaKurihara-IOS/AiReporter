{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train_all.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHfd98YKpAa"
      },
      "source": [
        "以下の設定になっていること  \n",
        "\n",
        "ランタイム > ランタイムのタイプを変更  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  \n",
        "\n",
        "編集 > ノートブックの設定  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZUtGFrIE0N",
        "outputId": "4e5df25a-cec8-480f-d453-ce43d56128ef"
      },
      "source": [
        "# googleドライブをマウント\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "# 表示されるリンクをクリックして、アクセスを許可して、最後に表示される文字列を以下の入力欄に入れる"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6Phq7IINCz",
        "outputId": "e93274a2-c89d-4084-ad61-45b1a6e43966"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7WZHPnIqSv",
        "outputId": "21bae2aa-e054-42de-cb96-abb6644824ee"
      },
      "source": [
        "# importで使う必要があるので、インストールがランタイム切れるごとに必要\n",
        "# インストール後にランタイムの再起動を行わないとT5Tokenizerが見つからない\n",
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# ドライブに保存してるものでインストール\n",
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n",
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 92.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, tqdm, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWjtNtvnRTPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfa8519-91c1-407b-a03f-95f85b470181"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tepd4-4WCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtynWz0u4IT"
      },
      "source": [
        "CLM（Causal Language Modeling）: GPT、GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKmxERxt7Io"
      },
      "source": [
        "・model_name_or_path: モデルのチェックポイント（モデルを最初から学習しない場合）  \n",
        "・model_type: モデルの種別（モデルを最初から学習する場合）  \n",
        "・config_name: コンフィグ名（model_nameと同じでない場合）  \n",
        "・tokenizer_name: トークナイザー名（model_nameと同じでない場合）  \n",
        "・cache_dir: キャッシュフォルダ  \n",
        "・use_fast_tokenizer: Fastトークナイザーを使用するかどうか  \n",
        "・model_revision: 使用するモデルの特定のバージョン  \n",
        "・use_auth_token: 「transformers-cli login」の実行時に生成されたトークンを使用するかどうか  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0KKz6auJXj"
      },
      "source": [
        "・dataset_name: データセット名  \n",
        "・dataset_config_name: データセットのコンフィグ名  \n",
        "・train_file: 学習データ（テキストファイル）  \n",
        "・validation_file: 検証データ（テキストファイル）  \n",
        "・overwrite_cache: キャッシュの上書き  \n",
        "・validation_split_percentage: 学習データから使われる検証データの割合（検証データがない場合）  \n",
        "・max_seq_length: トークン化後の最大合計入力シーケンス長  \n",
        "・preprocessing_num_workers: 前処理に使用するプロセス数  \n",
        "・block_size: トークン化後のオプションの入力シーケンス長  \n",
        "・max_train_samples: 学習データの最大数  \n",
        "・max_val_samples: 検証データの最大数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9LIvEaz6Pst"
      },
      "source": [
        "    # GPT2のモデルファイルを指定\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    # 学習ファイル\n",
        "    --train_file=train.txt \\\n",
        "    # 評価データファイル\n",
        "    --validation_file=train.txt \\\n",
        "    # トレーニングを実施する\n",
        "    --do_train \\\n",
        "    # 評価を実施する\n",
        "    --do_eval \\\n",
        "    # 学習回数（エポック数）\n",
        "    --num_train_epochs=30 \\\n",
        "    # チェックポイントの保存間隔\n",
        "    --save_steps=5000 \\\n",
        "    # チェックポイントの保持数\n",
        "    --save_total_limit=3 \\\n",
        "    # T5Tokenizer.model_max_length=1024をチャンクサイズとして使用するかblock_sizeで指定するかを設定する（設定しないとtokenizerの超大なサイズから1024になる）メモリに乗るように調整する必要がある（バッチサイズとの兼ね合い）\n",
        "    --block_size=512 \\\n",
        "    # GPU1つあたりの学習バッチサイズ\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    # GPU1つあたりの評価バッチサイズ\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    # モデルとチェックポイントの出力先\n",
        "    --output_dir=output/ \\\n",
        "    # 出力先の上書きの許可\n",
        "    --overwrite_output_dir=True \\\n",
        "    # T5Tokenizerで高速化ライブラリがあれば使用する\n",
        "    --use_fast_tokenizer=False\n",
        "\n",
        "### 学習する回数\n",
        "エポック数 * (データ文字数 / ブロックサイズ):1つの学習データ / バッチサイズ  \n",
        "    エポック数 = 1  \n",
        "    データ文字数 = 22142  \n",
        "    データの文字数だと数字が合わないので、tokenizeした結果の形態素数による？\n",
        "    ブロックサイズ = 512  \n",
        "    データ文字数 / ブロックサイズ = 43  \n",
        "    バッチサイズ = 1  \n",
        "    Total optimization steps = 29  \n",
        "\n",
        "\n",
        "バッチサイズ = 2  \n",
        "バッチサイズを2にすることで、トータルの実行回数が減っている  \n",
        "Total optimization steps = 15  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zS2rIpZIfXB",
        "outputId": "64432c25-4c76-47cd-b08a-465f7aef4ffc"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train_all.txt \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs=500 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --block_size=256 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --output_dir=output_all/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/29/2021 12:01:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "08/29/2021 12:01:46 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output_all/, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=500.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Aug29_12-01-46_4f86858edcd0, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output_all/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 2.79MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-6ca254177ec09072 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-6ca254177ec09072/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-6ca254177ec09072/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-08-29 12:01:49,015 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbu8m9giv\n",
            "Downloading: 100% 799/799 [00:00<00:00, 853kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 12:01:49,274 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|file_utils.py:1393] 2021-08-29 12:01:49,274 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:463] 2021-08-29 12:01:49,274 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:499] 2021-08-29 12:01:49,275 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-08-29 12:01:49,547 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppyfniben\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 25.2MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 12:01:49,964 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-08-29 12:01:49,964 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-08-29 12:01:50,481 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6097qp_l\n",
            "Downloading: 100% 153/153 [00:00<00:00, 154kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 12:01:50,739 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-08-29 12:01:50,739 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-08-29 12:01:50,999 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz19seo70\n",
            "Downloading: 100% 282/282 [00:00<00:00, 296kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 12:01:51,258 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|file_utils.py:1393] 2021-08-29 12:01:51,258 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 12:01:51,513 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 12:01:51,513 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 12:01:51,513 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 12:01:51,513 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-08-29 12:01:51,513 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-08-29 12:01:51,828 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph2vb4_90\n",
            "Downloading: 100% 1.37G/1.37G [00:19<00:00, 71.0MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-08-29 12:02:11,178 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|file_utils.py:1393] 2021-08-29 12:02:11,178 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1051] 2021-08-29 12:02:11,178 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1167] 2021-08-29 12:02:19,313 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-08-29 12:02:19,313 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 4/4 [00:00<00:00,  9.27ba/s]\n",
            "100% 4/4 [00:00<00:00,  9.10ba/s]\n",
            "[INFO|trainer.py:946] 2021-08-29 12:02:27,235 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-08-29 12:02:27,235 >>   Num examples = 453\n",
            "[INFO|trainer.py:948] 2021-08-29 12:02:27,235 >>   Num Epochs = 500\n",
            "[INFO|trainer.py:949] 2021-08-29 12:02:27,235 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:950] 2021-08-29 12:02:27,235 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:951] 2021-08-29 12:02:27,236 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-08-29 12:02:27,236 >>   Total optimization steps = 57000\n",
            "{'loss': 3.6264, 'learning_rate': 4.956140350877193e-05, 'epoch': 4.39}\n",
            "{'loss': 2.1614, 'learning_rate': 4.912280701754386e-05, 'epoch': 8.77}\n",
            "{'loss': 1.0541, 'learning_rate': 4.868421052631579e-05, 'epoch': 13.16}\n",
            "{'loss': 0.4346, 'learning_rate': 4.824561403508772e-05, 'epoch': 17.54}\n",
            "{'loss': 0.1877, 'learning_rate': 4.780701754385965e-05, 'epoch': 21.93}\n",
            "{'loss': 0.1021, 'learning_rate': 4.736842105263158e-05, 'epoch': 26.32}\n",
            "{'loss': 0.0706, 'learning_rate': 4.6929824561403515e-05, 'epoch': 30.7}\n",
            "{'loss': 0.0564, 'learning_rate': 4.649122807017544e-05, 'epoch': 35.09}\n",
            "{'loss': 0.0488, 'learning_rate': 4.605263157894737e-05, 'epoch': 39.47}\n",
            "{'loss': 0.045, 'learning_rate': 4.56140350877193e-05, 'epoch': 43.86}\n",
            "  9% 5000/57000 [1:14:27<12:53:55,  1.12it/s][INFO|trainer.py:1558] 2021-08-29 13:16:54,480 >> Saving model checkpoint to output_all/checkpoint-5000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 13:16:54,485 >> Configuration saved in output_all/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 13:16:58,943 >> Model weights saved in output_all/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 13:16:58,948 >> tokenizer config file saved in output_all/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 13:16:58,951 >> Special tokens file saved in output_all/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 13:16:58,957 >> Copy vocab file to output_all/checkpoint-5000/spiece.model\n",
            "{'loss': 0.0404, 'learning_rate': 4.517543859649123e-05, 'epoch': 48.25}\n",
            "{'loss': 0.0423, 'learning_rate': 4.473684210526316e-05, 'epoch': 52.63}\n",
            "{'loss': 0.0369, 'learning_rate': 4.429824561403509e-05, 'epoch': 57.02}\n",
            "{'loss': 0.0346, 'learning_rate': 4.3859649122807014e-05, 'epoch': 61.4}\n",
            "{'loss': 0.0331, 'learning_rate': 4.342105263157895e-05, 'epoch': 65.79}\n",
            "{'loss': 0.0316, 'learning_rate': 4.298245614035088e-05, 'epoch': 70.18}\n",
            "{'loss': 0.0308, 'learning_rate': 4.254385964912281e-05, 'epoch': 74.56}\n",
            "{'loss': 0.0322, 'learning_rate': 4.210526315789474e-05, 'epoch': 78.95}\n",
            "{'loss': 0.0296, 'learning_rate': 4.166666666666667e-05, 'epoch': 83.33}\n",
            "{'loss': 0.0286, 'learning_rate': 4.12280701754386e-05, 'epoch': 87.72}\n",
            " 18% 10000/57000 [2:29:17<11:44:28,  1.11it/s][INFO|trainer.py:1558] 2021-08-29 14:31:44,618 >> Saving model checkpoint to output_all/checkpoint-10000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 14:31:44,621 >> Configuration saved in output_all/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 14:31:49,442 >> Model weights saved in output_all/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 14:31:49,446 >> tokenizer config file saved in output_all/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 14:31:49,449 >> Special tokens file saved in output_all/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 14:31:49,454 >> Copy vocab file to output_all/checkpoint-10000/spiece.model\n",
            "{'loss': 0.0285, 'learning_rate': 4.078947368421053e-05, 'epoch': 92.11}\n",
            "{'loss': 0.0274, 'learning_rate': 4.0350877192982455e-05, 'epoch': 96.49}\n",
            "{'loss': 0.0263, 'learning_rate': 3.991228070175439e-05, 'epoch': 100.88}\n",
            "{'loss': 0.0259, 'learning_rate': 3.9473684210526316e-05, 'epoch': 105.26}\n",
            "{'loss': 0.0262, 'learning_rate': 3.9035087719298244e-05, 'epoch': 109.65}\n",
            "{'loss': 0.0265, 'learning_rate': 3.859649122807018e-05, 'epoch': 114.04}\n",
            "{'loss': 0.0258, 'learning_rate': 3.815789473684211e-05, 'epoch': 118.42}\n",
            "{'loss': 0.0253, 'learning_rate': 3.771929824561404e-05, 'epoch': 122.81}\n",
            "{'loss': 0.0245, 'learning_rate': 3.728070175438597e-05, 'epoch': 127.19}\n",
            "{'loss': 0.0245, 'learning_rate': 3.6842105263157895e-05, 'epoch': 131.58}\n",
            " 26% 15000/57000 [3:44:07<10:26:47,  1.12it/s][INFO|trainer.py:1558] 2021-08-29 15:46:35,090 >> Saving model checkpoint to output_all/checkpoint-15000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 15:46:35,094 >> Configuration saved in output_all/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 15:46:39,658 >> Model weights saved in output_all/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 15:46:39,662 >> tokenizer config file saved in output_all/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 15:46:39,665 >> Special tokens file saved in output_all/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 15:46:39,670 >> Copy vocab file to output_all/checkpoint-15000/spiece.model\n",
            "{'loss': 0.0271, 'learning_rate': 3.640350877192983e-05, 'epoch': 135.96}\n",
            "{'loss': 0.0238, 'learning_rate': 3.5964912280701756e-05, 'epoch': 140.35}\n",
            "{'loss': 0.0237, 'learning_rate': 3.5526315789473684e-05, 'epoch': 144.74}\n",
            "{'loss': 0.0225, 'learning_rate': 3.508771929824561e-05, 'epoch': 149.12}\n",
            "{'loss': 0.024, 'learning_rate': 3.4649122807017546e-05, 'epoch': 153.51}\n",
            "{'loss': 0.0217, 'learning_rate': 3.421052631578947e-05, 'epoch': 157.89}\n",
            "{'loss': 0.022, 'learning_rate': 3.377192982456141e-05, 'epoch': 162.28}\n",
            "{'loss': 0.0226, 'learning_rate': 3.3333333333333335e-05, 'epoch': 166.67}\n",
            "{'loss': 0.0219, 'learning_rate': 3.289473684210527e-05, 'epoch': 171.05}\n",
            "{'loss': 0.0213, 'learning_rate': 3.24561403508772e-05, 'epoch': 175.44}\n",
            " 35% 20000/57000 [4:59:04<9:12:51,  1.12it/s][INFO|trainer.py:1558] 2021-08-29 17:01:32,346 >> Saving model checkpoint to output_all/checkpoint-20000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 17:01:32,351 >> Configuration saved in output_all/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 17:01:36,912 >> Model weights saved in output_all/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 17:01:36,916 >> tokenizer config file saved in output_all/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 17:01:36,919 >> Special tokens file saved in output_all/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 17:01:36,924 >> Copy vocab file to output_all/checkpoint-20000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 17:02:06,997 >> Deleting older checkpoint [output_all/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 0.0205, 'learning_rate': 3.2017543859649124e-05, 'epoch': 179.82}\n",
            "{'loss': 0.0207, 'learning_rate': 3.157894736842105e-05, 'epoch': 184.21}\n",
            "{'loss': 0.0205, 'learning_rate': 3.1140350877192986e-05, 'epoch': 188.6}\n",
            "{'loss': 0.0198, 'learning_rate': 3.0701754385964913e-05, 'epoch': 192.98}\n",
            "{'loss': 0.0192, 'learning_rate': 3.0263157894736844e-05, 'epoch': 197.37}\n",
            "{'loss': 0.0186, 'learning_rate': 2.9824561403508772e-05, 'epoch': 201.75}\n",
            "{'loss': 0.0188, 'learning_rate': 2.9385964912280706e-05, 'epoch': 206.14}\n",
            "{'loss': 0.0176, 'learning_rate': 2.8947368421052634e-05, 'epoch': 210.53}\n",
            "{'loss': 0.0208, 'learning_rate': 2.850877192982456e-05, 'epoch': 214.91}\n",
            "{'loss': 0.0198, 'learning_rate': 2.8070175438596492e-05, 'epoch': 219.3}\n",
            " 44% 25000/57000 [6:14:03<7:59:03,  1.11it/s][INFO|trainer.py:1558] 2021-08-29 18:16:31,407 >> Saving model checkpoint to output_all/checkpoint-25000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 18:16:31,411 >> Configuration saved in output_all/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 18:16:36,111 >> Model weights saved in output_all/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 18:16:36,115 >> tokenizer config file saved in output_all/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 18:16:37,903 >> Special tokens file saved in output_all/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 18:16:37,911 >> Copy vocab file to output_all/checkpoint-25000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 18:17:05,305 >> Deleting older checkpoint [output_all/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 0.02, 'learning_rate': 2.7631578947368426e-05, 'epoch': 223.68}\n",
            "{'loss': 0.0145, 'learning_rate': 2.7192982456140354e-05, 'epoch': 228.07}\n",
            "{'loss': 0.0128, 'learning_rate': 2.675438596491228e-05, 'epoch': 232.46}\n",
            "{'loss': 0.0129, 'learning_rate': 2.6315789473684212e-05, 'epoch': 236.84}\n",
            "{'loss': 0.0105, 'learning_rate': 2.5877192982456143e-05, 'epoch': 241.23}\n",
            "{'loss': 0.0094, 'learning_rate': 2.5438596491228074e-05, 'epoch': 245.61}\n",
            "{'loss': 0.0081, 'learning_rate': 2.5e-05, 'epoch': 250.0}\n",
            "{'loss': 0.0076, 'learning_rate': 2.456140350877193e-05, 'epoch': 254.39}\n",
            "{'loss': 0.0078, 'learning_rate': 2.412280701754386e-05, 'epoch': 258.77}\n",
            "{'loss': 0.0067, 'learning_rate': 2.368421052631579e-05, 'epoch': 263.16}\n",
            " 53% 30000/57000 [7:29:00<6:46:04,  1.11it/s][INFO|trainer.py:1558] 2021-08-29 19:31:28,164 >> Saving model checkpoint to output_all/checkpoint-30000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 19:31:28,167 >> Configuration saved in output_all/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 19:31:34,265 >> Model weights saved in output_all/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 19:31:34,269 >> tokenizer config file saved in output_all/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 19:31:34,272 >> Special tokens file saved in output_all/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 19:31:34,277 >> Copy vocab file to output_all/checkpoint-30000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 19:32:04,990 >> Deleting older checkpoint [output_all/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 0.0064, 'learning_rate': 2.324561403508772e-05, 'epoch': 267.54}\n",
            "{'loss': 0.007, 'learning_rate': 2.280701754385965e-05, 'epoch': 271.93}\n",
            "{'loss': 0.0063, 'learning_rate': 2.236842105263158e-05, 'epoch': 276.32}\n",
            "{'loss': 0.0059, 'learning_rate': 2.1929824561403507e-05, 'epoch': 280.7}\n",
            "{'loss': 0.0061, 'learning_rate': 2.149122807017544e-05, 'epoch': 285.09}\n",
            "{'loss': 0.0059, 'learning_rate': 2.105263157894737e-05, 'epoch': 289.47}\n",
            "{'loss': 0.0054, 'learning_rate': 2.06140350877193e-05, 'epoch': 293.86}\n",
            "{'loss': 0.005, 'learning_rate': 2.0175438596491227e-05, 'epoch': 298.25}\n",
            "{'loss': 0.0055, 'learning_rate': 1.9736842105263158e-05, 'epoch': 302.63}\n",
            "{'loss': 0.0052, 'learning_rate': 1.929824561403509e-05, 'epoch': 307.02}\n",
            " 61% 35000/57000 [8:44:02<5:01:43,  1.22it/s][INFO|trainer.py:1558] 2021-08-29 20:46:30,291 >> Saving model checkpoint to output_all/checkpoint-35000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 20:46:30,295 >> Configuration saved in output_all/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 20:46:35,148 >> Model weights saved in output_all/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 20:46:35,152 >> tokenizer config file saved in output_all/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 20:46:35,155 >> Special tokens file saved in output_all/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 20:46:35,170 >> Copy vocab file to output_all/checkpoint-35000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 20:47:03,834 >> Deleting older checkpoint [output_all/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 0.0052, 'learning_rate': 1.885964912280702e-05, 'epoch': 311.4}\n",
            "{'loss': 0.0049, 'learning_rate': 1.8421052631578947e-05, 'epoch': 315.79}\n",
            "{'loss': 0.0047, 'learning_rate': 1.7982456140350878e-05, 'epoch': 320.18}\n",
            "{'loss': 0.0048, 'learning_rate': 1.7543859649122806e-05, 'epoch': 324.56}\n",
            "{'loss': 0.005, 'learning_rate': 1.7105263157894737e-05, 'epoch': 328.95}\n",
            "{'loss': 0.0047, 'learning_rate': 1.6666666666666667e-05, 'epoch': 333.33}\n",
            "{'loss': 0.0046, 'learning_rate': 1.62280701754386e-05, 'epoch': 337.72}\n",
            "{'loss': 0.0045, 'learning_rate': 1.5789473684210526e-05, 'epoch': 342.11}\n",
            "{'loss': 0.0044, 'learning_rate': 1.5350877192982457e-05, 'epoch': 346.49}\n",
            "{'loss': 0.0045, 'learning_rate': 1.4912280701754386e-05, 'epoch': 350.88}\n",
            " 70% 40000/57000 [9:59:03<4:14:58,  1.11it/s][INFO|trainer.py:1558] 2021-08-29 22:01:31,107 >> Saving model checkpoint to output_all/checkpoint-40000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 22:01:31,111 >> Configuration saved in output_all/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 22:01:35,998 >> Model weights saved in output_all/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 22:01:36,002 >> tokenizer config file saved in output_all/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 22:01:36,005 >> Special tokens file saved in output_all/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 22:01:36,010 >> Copy vocab file to output_all/checkpoint-40000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 22:02:04,207 >> Deleting older checkpoint [output_all/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 0.0045, 'learning_rate': 1.4473684210526317e-05, 'epoch': 355.26}\n",
            "{'loss': 0.0044, 'learning_rate': 1.4035087719298246e-05, 'epoch': 359.65}\n",
            "{'loss': 0.0044, 'learning_rate': 1.3596491228070177e-05, 'epoch': 364.04}\n",
            "{'loss': 0.0043, 'learning_rate': 1.3157894736842106e-05, 'epoch': 368.42}\n",
            "{'loss': 0.0043, 'learning_rate': 1.2719298245614037e-05, 'epoch': 372.81}\n",
            "{'loss': 0.0042, 'learning_rate': 1.2280701754385964e-05, 'epoch': 377.19}\n",
            "{'loss': 0.0042, 'learning_rate': 1.1842105263157895e-05, 'epoch': 381.58}\n",
            "{'loss': 0.0041, 'learning_rate': 1.1403508771929824e-05, 'epoch': 385.96}\n",
            "{'loss': 0.0041, 'learning_rate': 1.0964912280701754e-05, 'epoch': 390.35}\n",
            "{'loss': 0.004, 'learning_rate': 1.0526315789473684e-05, 'epoch': 394.74}\n",
            " 79% 45000/57000 [11:14:03<2:59:50,  1.11it/s][INFO|trainer.py:1558] 2021-08-29 23:16:31,294 >> Saving model checkpoint to output_all/checkpoint-45000\n",
            "[INFO|configuration_utils.py:314] 2021-08-29 23:16:31,298 >> Configuration saved in output_all/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-29 23:16:35,754 >> Model weights saved in output_all/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-29 23:16:35,758 >> tokenizer config file saved in output_all/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-29 23:16:35,761 >> Special tokens file saved in output_all/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-29 23:16:36,035 >> Copy vocab file to output_all/checkpoint-45000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-29 23:17:05,712 >> Deleting older checkpoint [output_all/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 0.004, 'learning_rate': 1.0087719298245614e-05, 'epoch': 399.12}\n",
            "{'loss': 0.0041, 'learning_rate': 9.649122807017545e-06, 'epoch': 403.51}\n",
            "{'loss': 0.004, 'learning_rate': 9.210526315789474e-06, 'epoch': 407.89}\n",
            "{'loss': 0.0039, 'learning_rate': 8.771929824561403e-06, 'epoch': 412.28}\n",
            "{'loss': 0.0039, 'learning_rate': 8.333333333333334e-06, 'epoch': 416.67}\n",
            "{'loss': 0.0039, 'learning_rate': 7.894736842105263e-06, 'epoch': 421.05}\n",
            "{'loss': 0.0039, 'learning_rate': 7.456140350877193e-06, 'epoch': 425.44}\n",
            "{'loss': 0.0038, 'learning_rate': 7.017543859649123e-06, 'epoch': 429.82}\n",
            "{'loss': 0.0038, 'learning_rate': 6.578947368421053e-06, 'epoch': 434.21}\n",
            "{'loss': 0.0038, 'learning_rate': 6.140350877192982e-06, 'epoch': 438.6}\n",
            " 88% 50000/57000 [12:29:04<1:44:39,  1.11it/s][INFO|trainer.py:1558] 2021-08-30 00:31:32,416 >> Saving model checkpoint to output_all/checkpoint-50000\n",
            "[INFO|configuration_utils.py:314] 2021-08-30 00:31:32,419 >> Configuration saved in output_all/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-30 00:31:37,445 >> Model weights saved in output_all/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-30 00:31:37,450 >> tokenizer config file saved in output_all/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-30 00:31:37,507 >> Special tokens file saved in output_all/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-30 00:31:37,513 >> Copy vocab file to output_all/checkpoint-50000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-30 00:32:10,260 >> Deleting older checkpoint [output_all/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 0.0037, 'learning_rate': 5.701754385964912e-06, 'epoch': 442.98}\n",
            "{'loss': 0.0037, 'learning_rate': 5.263157894736842e-06, 'epoch': 447.37}\n",
            "{'loss': 0.0037, 'learning_rate': 4.824561403508772e-06, 'epoch': 451.75}\n",
            "{'loss': 0.0038, 'learning_rate': 4.3859649122807014e-06, 'epoch': 456.14}\n",
            "{'loss': 0.0036, 'learning_rate': 3.9473684210526315e-06, 'epoch': 460.53}\n",
            "{'loss': 0.0036, 'learning_rate': 3.5087719298245615e-06, 'epoch': 464.91}\n",
            "{'loss': 0.0036, 'learning_rate': 3.070175438596491e-06, 'epoch': 469.3}\n",
            "{'loss': 0.0036, 'learning_rate': 2.631578947368421e-06, 'epoch': 473.68}\n",
            "{'loss': 0.0035, 'learning_rate': 2.1929824561403507e-06, 'epoch': 478.07}\n",
            "{'loss': 0.0035, 'learning_rate': 1.7543859649122807e-06, 'epoch': 482.46}\n",
            " 96% 55000/57000 [13:44:09<30:07,  1.11it/s][INFO|trainer.py:1558] 2021-08-30 01:46:36,885 >> Saving model checkpoint to output_all/checkpoint-55000\n",
            "[INFO|configuration_utils.py:314] 2021-08-30 01:46:36,889 >> Configuration saved in output_all/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-30 01:46:42,255 >> Model weights saved in output_all/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-30 01:46:42,265 >> tokenizer config file saved in output_all/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-30 01:46:42,268 >> Special tokens file saved in output_all/checkpoint-55000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-30 01:46:42,276 >> Copy vocab file to output_all/checkpoint-55000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-08-30 01:47:10,942 >> Deleting older checkpoint [output_all/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 0.0035, 'learning_rate': 1.3157894736842106e-06, 'epoch': 486.84}\n",
            "{'loss': 0.0035, 'learning_rate': 8.771929824561404e-07, 'epoch': 491.23}\n",
            "{'loss': 0.0034, 'learning_rate': 4.385964912280702e-07, 'epoch': 495.61}\n",
            "{'loss': 0.0034, 'learning_rate': 0.0, 'epoch': 500.0}\n",
            "100% 57000/57000 [14:14:31<00:00,  1.37it/s][INFO|trainer.py:1129] 2021-08-30 02:16:58,484 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 51271.2481, 'train_samples_per_second': 1.112, 'epoch': 500.0}\n",
            "100% 57000/57000 [14:14:31<00:00,  1.11it/s]\n",
            "[INFO|trainer.py:1558] 2021-08-30 02:16:58,835 >> Saving model checkpoint to output_all/\n",
            "[INFO|configuration_utils.py:314] 2021-08-30 02:16:58,839 >> Configuration saved in output_all/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-08-30 02:17:04,168 >> Model weights saved in output_all/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-08-30 02:17:04,171 >> tokenizer config file saved in output_all/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-08-30 02:17:04,174 >> Special tokens file saved in output_all/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-08-30 02:17:04,180 >> Copy vocab file to output_all/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-08-30 02:17:04,186 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   epoch                      =      500.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   init_mem_cpu_alloc_delta   =        1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   init_mem_gpu_alloc_delta   =     1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   train_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   train_mem_cpu_peaked_delta =      126MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   train_mem_gpu_alloc_delta  =     3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   train_mem_gpu_peaked_delta =     4657MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,186 >>   train_runtime              = 51271.2481\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,187 >>   train_samples              =        453\n",
            "[INFO|trainer_pt_utils.py:661] 2021-08-30 02:17:04,187 >>   train_samples_per_second   =      1.112\n",
            "CPU times: user 4min 45s, sys: 49.3 s, total: 5min 34s\n",
            "Wall time: 14h 16min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu4AvG5Ik-p"
      },
      "source": [
        "# from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# # トークナイザーとモデルの準備\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"output/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xllJ0rN9Fuyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c353adc-7c3b-45fa-e501-da14c77224ab"
      },
      "source": [
        "# もちろんだが、Autoでも直指定でも同じ結果にはなっている\n",
        "# https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
        "from transformers import T5Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# GPU判定\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"output_all/\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(32000, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbp9X4mv2yJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt93bvvwv22j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_K7EOdMPVZ",
        "outputId": "cb92648a-8347-42c2-c7c4-5d790be092b3"
      },
      "source": [
        "# 推論\n",
        "# https://huggingface.co/blog/how-to-generate\n",
        "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate\n",
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブ チャット打ってきました続けデビュー戦ですが落ち着いた表情そして観客の拍手に応えるといったシーンもありました試合は十分といったところでしょう連打しますもしふらつきましたが大丈夫でしょうかでのパンチ集めて行きました。\n",
            "左のジャブ やはりこの辺りは手数を増やしていく。大切なのはどういったところですか?\n",
            "左のジャブ ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。\n",
            "左のジャブ やはりこの辺りは手数を増やしていく。大切なのはどういったところですか?\n",
            "左のジャブ プレッシャーをかけに来たのかと思われたのか右のストレートコンビネーションを見せてくれましたやり服大きなパンチが出ましたがこれは空を切りました。\n",
            "左のジャブ やはりスピード感のある選手ですね。\n",
            "左のジャブ コンビネーションを打つ時はしっかり踏み込んでパンツ売って行きますいらです左のボディお手数を増やしていったらという声も上がるほどなかなか折笠の捉えられないボクシング右のストレートコンビネーションで打ってきました。\n",
            "左のジャブ やはりこの辺りは手数を増やしていく。大切なのはどういったところですか?\n",
            "左のジャブ ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。\n",
            "左のジャブ いきなり若干角度を変えながら右を出していきました今度はの左です。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB5AQ9pQoyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ef2dbc-d0c4-4f98-feb5-8f86d0672cdf"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングという\n",
            "左のフック 赤コーナーよりサカタケントがこれから入場をして来ようとしています。\n",
            "左のフック 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "左のフック しゃべる相手の動きを制します浜田ですただ前に前に入ろうとするのは堀これがデビュー戦です思い切って戦っているほりです前に出ようというところ歌詞でプレッシャーをかけて混ぜていく未来のジャムが入りました。\n",
            "左のフック ボディに打ち込んでいく、そして、ショートレンジのパンチ。が入ってきてもパンチで応戦して、逆に押し返しているのがですかね。\n",
            "左のフック スピード感のある選手ですね。\n",
            "左のフック みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングという\n",
            "左のフック 赤コーナーよりサカタケントがこれから入場をして来ようとしています。\n",
            "左のフック 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "左のフック 少しふらついた!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-VUAXfoEE9K",
        "outputId": "c922d0e2-34b9-4fa3-e25b-941a1152c598"
      },
      "source": [
        "input = tokenizer.encode(\"ボディ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボディ 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。\n",
            "ボディ 左でジャブを出していく青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。\n",
            "ボディ 左のフック!左のボディ!右のストレート!も返していく!\n",
            "ボディ 左のフック!左!ひかない!も引かない!右左!さあ今度はの逆襲!\n",
            "ボディ 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。\n",
            "ボディ 左でじわじわと詰めていきましたこの辺りの戦略というのはどうでしょうか距離に持って行きますがそうなるとかんだとしても入らせたくないジャムなども栗あってくるかもしれませんがここで右のストレートなたんだ左から右の番地を出して行きます右のボディを返して言ったのは長いです左のジャブが早いコナカですただこの間だが今回話していたのはその前に出るだけではなくていいとこ引いたような頭は\n",
            "ボディ 左のフックを返してきました小西みかとも感じてるから第2ラウンド終了です。\n",
            "ボディ 左のフックを見せました千葉です。\n",
            "ボディ 左で距離を取りますヒヤリ。\n",
            "ボディ 左のフックを返していった!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zPc_EPFEJi_",
        "outputId": "f80d8451-c530-47ce-e903-23634e01da6d"
      },
      "source": [
        "input = tokenizer.encode(\"右のストレート\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 少し変化がありましたか。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n",
            "右のストレート パリ最後まで体重が落ちませんしっかりて画像出して前に前にプレッシャーをかけて行きたいと言っていましたが残り10秒おまけに声出していいのなら明日の久保にもっと  ロッテに対する終了しましたがこちらが西桐生糸です22歳岐阜の横好きジム所属です今日が13戦目。\n",
            "右のストレート 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHIHS0H7EMe1",
        "outputId": "7bf8035d-827f-4485-dce1-46b73652ac00"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上から 早くもコンビネーション!\n",
            "ガードの上から じわじわと圧力をかけていきますです。\n",
            "ガードの上から じわじわと圧力をかけていくのは守り青葉。\n",
            "ガードの上から いきなり大きなパンチを打ち込んでいきますです。\n",
            "ガードの上から みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングという\n",
            "ガードの上から じわじわと五十嵐のプレッシャーをかけていきます。\n",
            "ガードの上から じわじわと圧をかけて行きますです。\n",
            "ガードの上から じわじわと五十嵐にプレッシャーをかけていきます。\n",
            "ガードの上から じわじわと腸だけでいかがですか?\n",
            "ガードの上から じわっと詰めていきましたです。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4H6oqsAKrlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxas9R2LKrt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiAl1wGnF6kf"
      },
      "source": [
        "tokenizerの中身を確認  \n",
        "\\<s\\>の意味合いを表示  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYhyTw7Ea7-",
        "outputId": "8daf467e-f137-4f24-960e-71f20db23a62"
      },
      "source": [
        "# model.generateの結果はtokenizerのindexベクトル\n",
        "output[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    9,  5682,    10, 12276,     2,     9,     0,    20,  2115,    18,\n",
              "         5456,  2199,     7,    80,     0,    10,   819,     8,     2,     2,\n",
              "            2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym78sIbQFIMn",
        "outputId": "0e4f0dcc-3a88-4653-d17f-f3ce7328ac4b"
      },
      "source": [
        "# 記号の意味\n",
        "tokenizer.all_special_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNCWHrgWG9bY",
        "outputId": "3574002a-6b91-4aab-b276-e7e660b76436"
      },
      "source": [
        "# 記号に対応するindex\n",
        "tokenizer.all_special_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0, 5, 3, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCg-F5fAFlaa"
      },
      "source": [
        "{\"bos_token\": \"\\<s\\>\", \"eos_token\": \"\\</s\\>\", \"unk_token\": \"<unk>\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}  \n",
        "bos_token: 文の先頭（Begin of sequence token）  \n",
        "eos_token: 文のおしり（End of Sequence token）  \n",
        "unk_token: IDに変換できない文字（Unknown token）  \n",
        "sep_token: 文と文を区切り目（The separator token）  \n",
        "pad_token: パッディング（The token used for padding）  \n",
        "cls_token: 分類用（cls_token）  \n",
        "mask_token: マスク（The token used for masking values）  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5xE9Myg0WR"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode  \n",
        "sequences : torch.Tensorの配列を入力値として指定  \n",
        "  トークン化された入力IDのリスト  \n",
        "skip_special_tokens : デコード時に特殊なトークンを削除するかどうか(eos_tokenとかを消す)(デフォルト:False)  \n",
        "clean_up_tokenization_spaces : トークン化スペースをクリーンアップするかどうか(デフォルト:True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqZhkicXiBP8"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode  \n",
        "\n",
        "text (str, List[str] or List[int]) – 入力文字列  \n",
        "\n",
        "text_pair (str, List[str] or List[int], optional) – ペアとなるもう一つを入力する場合のオプション  \n",
        "\n",
        "add_special_tokens (bool, optional, defaults to True) – 上記で定義していない特別なトークンをモデルに適用するか\n",
        "\n",
        "padding (bool, str or PaddingStrategy, optional, defaults to False) –パディングして入力シーケンスを揃える場合  \n",
        "\n",
        "truncation (bool, str or TruncationStrategy, optional, defaults to False) –逆に長過ぎる場合に、一定の長さに揃える場合\n",
        "\n",
        "max_length (int, optional) –トランケーション・パディングで使用するオプション\n",
        "\n",
        "stride (int, optional, defaults to 0) – max_lengthで切り捨てられたのを調整する  \n",
        "\n",
        "is_split_into_words (bool, optional, defaults to False) – 単語分割が既にされている場合True\n",
        "\n",
        "pad_to_multiple_of (int, optional) – 指定された値の倍数になるようにシーケンスをパッドする  \n",
        "\n",
        "return_tensors (str or TensorType, optional) – python整数のリストの代わりにテンソルを返す  \n",
        "'tf': Return TensorFlow tf.constant objects.  \n",
        "'pt': Return PyTorch torch.Tensor objects.  \n",
        "'np': Return Numpy np.ndarray objects.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZXgdwCNECfn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paoQ0wFiNsyp"
      },
      "source": [
        "* https://huggingface.co/blog/how-to-generate  \n",
        "* https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate  \n",
        "* https://note.com/npaka/n/n5d296d8ae26d  \n",
        "* https://note.com/npaka/n/n96dde45fdf8d  \n",
        "\n",
        "### GPT2LMHeadModel.generateのオプションを確認  \n",
        "\n",
        "input_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) – 入力シーケンス  \n",
        "\n",
        "max_length (int, optional, defaults to model.config.max_length) – 生成されるシーケンスの最大長を指定（学習に使用した256の長さが良さそう）\n",
        "\n",
        "max_new_tokens (int, optional, defaults to None) – 現在のトークン数に関係なく、生成されるシーケンスの最大長を指定\n",
        "\n",
        "min_length (int, optional, defaults to 10) – 生成されるシーケンスの最小の長さ\n",
        "\n",
        "do_sample (bool, optional, defaults to False) – 単語予測にサンプリングを入れてランダム性を導入する。（デフォルトは greedy decoding の生成）  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d  \n",
        "\n",
        "early_stopping (bool, optional, defaults to False) – ビーム探索で、num_beams個の文が生成された時点で、ビーム探索を終了するかどうか  \n",
        "\n",
        "num_beams (int, optional, defaults to 1) – ビームサーチを行うビームの数。1はビームサーチを行わないことを意味します。  \n",
        "\n",
        "temperature (float, optional, defaults to 1.0) – 次のトークンの確率をモジュール化するために使用される値です。温度（デフォルト1、推奨0.7〜1.0）ボルツマン分布のパラメータ。小さい値ではランダムな補完が減り，0では決まりきった繰り返しの文になる。大きい値ではより様々な補完がされる。  \n",
        "\n",
        "top_k (int, optional, defaults to 50) – top-k-filteringのために保持する最高確率の語彙トークンの数です。確率が大きめな候補からサンプリングしてランダム性を導入する際の候補を何個にするか。40が一般的に良い値  \n",
        "\n",
        "top_p (float, optional, defaults to 1.0) – 生成テキストを累積確率に制限 (0で制限なし) float < 1に設定すると、top_p以上の確率を持つ最も確率の高いトークンのみが生成のために保持されます。  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d#top-p-(nucleus)-sampling  \n",
        "\n",
        "repetition_penalty (float, optional, defaults to 1.0) – 反復ペナルティのパラメータです。1.0はペナルティなし。すでに生成された単語や文脈に属する単語にペナルティを与えるために使用することができます。反復防止にはかなり効果的ですが、異なるモデルやユースケースには非常に敏感なようで、議論がある。  \n",
        "\n",
        "pad_token_id (int, optional) – PADトークンを指定\n",
        "\n",
        "bos_token_id (int, optional) – bosトークンを指定\n",
        "\n",
        "eos_token_id (int, optional) – eosトークンを指定\n",
        "\n",
        "length_penalty (float, optional, defaults to 1.0) – 長さに対する指数関数的なペナルティ。1.0はペナルティがないことを意味します。1.0未満の値を設定すると、モデルは短い配列を生成するようになり、1.0以上の値を設定すると、モデルは長い配列を生成するようになります。\n",
        "\n",
        "no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、そのサイズのngramはすべて一度しか発生しません。最も一般的な n-grams ペナルティは、すでに見た n-gramsを作る可能性のある次の単語の確率を 0 に手動で設定することで、n-gramsが 2 回出現しないようにするものです。  \n",
        "\n",
        "encoder_no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、encoder_input_idsに出現したそのサイズのすべてのngramは、decoder_input_idsには出現しません。\n",
        "\n",
        "bad_words_ids (List[List[int]], optional) – 生成してはいけないトークンのidのリスト。トークンのIDは以下で確認  \n",
        "tokenizer(bad_word, add_prefix_space=True).input_ids  \n",
        "\n",
        "num_return_sequences (int, optional, defaults to 1) – バッチ内の各要素について、独立して計算された戻り値の配列の数(返却される結果の数)。返されるべき最高得点のBeamの数を設定します。ただし、num_return_sequences <= num_beams とします。\n",
        "\n",
        "max_time (float, optional, defaults to None) – 計算の実行を許可する最大時間を秒単位で指定します。割り当てられた時間が経過しても、生成は現在のパスを終了します。  \n",
        "\n",
        "attention_mask (torch.LongTensor of shape (batch_size, sequence_length), optional) – パディングされたトークンのインデックスに対してアテンションを行わないようにするためのマスクです。マスクの値は [0, 1] で、マスクされていないトークンには 1、マスクされたトークンには 0 です。提供されていない場合は、パッドトークンをマスクするinput_idsと同じ形のテンソルがデフォルトになります。attentionで予測するための配列を作るので、マスクすると候補に出なくなる。入力シーケンスに対して同じ長さで指定する\n",
        "\n",
        "decoder_start_token_id (int, optional) – エンコーダ・デコーダモデルがbosとは異なるトークンでデコードを開始した場合、そのトークンのid。\n",
        "\n",
        "use_cache – (bool, optional, defaults to True): 過去の最後のキー／バリューの注目度（モデルに該当する場合）を利用して、デコーディングを高速化するかどうか。  \n",
        "\n",
        "num_beam_groups (int, optional, defaults to 1) – num_beamsを分割するグループの数（ビームの異なるグループ間の多様性を確保するため）。\n",
        "\n",
        "diversity_penalty (float, optional, defaults to 0.0) – この値は、ある時点で他のグループのビームと同じトークンを生成した場合、ビームのスコアから差し引かれます。なお、ダイバーシティペナルティは、グループビーム検索が有効な場合にのみ有効です。  \n",
        "\n",
        "prefix_allowed_tokens_fn – (Callable[[int, torch.Tensor], List[int]], optional):提供された場合、この関数は、各ステップで許可されたトークンのみにビーム検索を制約します。提供されない場合、制約は適用されません。この関数は2つの引数をとります：バッチID batch_id と input_id です。これは、バッチID batch_idと以前に生成されたトークンinput_idsを条件として、次の生成ステップで許可されたトークンのリストを返さなければなりません。この引数は、「自己回帰的実体検索」で説明されているように、接頭辞を条件とした制約付き生成に役立ちます。  \n",
        "\n",
        "output_attentions (bool, optional, defaults to False) – すべてのアテンションレイヤーのアテンションテンソルを返すかどうか。  \n",
        "\n",
        "output_hidden_states (bool, optional, defaults to False) – すべてのレイヤーの隠れた状態を返すかどうか。  \n",
        "\n",
        "output_scores (bool, optional, defaults to False) – 予測スコアを返すかどうか。  \n",
        "\n",
        "return_dict_in_generate (bool, optional, defaults to False) – 単なるタプルではなく、ModelOutputを返すかどうか。  \n",
        "\n",
        "forced_bos_token_id (int, optional) – decoder_start_token_idの後に、最初に生成されるトークンとして強制的に使用するトークンのidです。mBARTのような多言語モデルで、最初に生成されるトークンがターゲット言語のトークンである必要がある場合に便利です。（一番最初に生成される単語を指定してしまう。）  \n",
        "\n",
        "forced_eos_token_id (int, optional) – max_lengthに達したときに、最後に生成されたトークンとして強制的に使用するトークンのidです。(最後をわかりやすくして、途中で切られたのを知らせる)  \n",
        "\n",
        "remove_invalid_values (bool, optional) – 生成方法がクラッシュするのを防ぐために、モデルの可能性のあるnanとinfの出力を削除するかどうか。remove_invalid_valuesを使うと生成が遅くなることに注意してください。  \n",
        "\n",
        "synced_gpus (bool, optional, defaults to False) – max_lengthまでwhileループを続けて実行するかどうか  \n",
        "\n",
        "最新の研究により、単純な Beam Search や Greedy Search が同じ単語列の繰り返しを発生させてしまうのは、decoding に問題があるのではなくモデルの学習自体に問題があるとされています。また、Top-K や Top-p のようなサンプリングによる decoding であってもそうした単語列の繰り返しは発生しうるそうです。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIarpN7BN1HJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisPBDCsN1OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9350b35c-1165-4d86-cc6e-2da675685a0f"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のフック</s> ボディに打ち込んでいく<unk> 、そして、ショートレンジのパンチ。<unk> が入ってきてもパンチで応戦して、逆に押し返しているのが<unk> ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> 少しふらついた!<unk>!少し効いているか!効いている!右のストレート!さあ今度は<unk> の逆襲!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> もうちょっと<unk> 、スタミナ厳しいですから気持ちでいいので お休みしますがまだ入るとのこと。 体力ものこっていますし。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> 左のボディをいれてきた<unk>!<unk>!ロープ際!一気に攻勢を強めていきます<unk> です。 さあ、ここまでは優勢に試合を進めている<unk> ですが、</s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> パロディ30秒に一番近いを返さない。 バビロン。 ひらりひらり入った幼い。 残り10秒を切りました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> 赤コーナーよりサカタケントがこれから入場をして来ようとしています。 16戦7勝7敗2分け神奈川渥美所属の選手で今回がランカーへの初めての頂点という風に高田がかかっています。</s>\n",
            "左のフック</s> 赤コーナーよりサカタケントがこれから入場をして来ようとしています。 16戦7勝7敗2分け神奈川渥美所属の選手で今回がランカーへの初めての頂点という風に高田がかかっています。</s>\n",
            "左のフック</s> 赤コーナーよりサカタケントがこれから入場をして来ようとしています。 16戦7勝7敗2分け神奈川渥美所属の選手で今回がランカーへの初めての頂点という風に高田がかかっています。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS8z5iRxN1Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c00cea7-d33f-407b-8a44-c2949b29b53c"
      },
      "source": [
        "input = tokenizer.encode(\"ラッシュラッシュ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ラッシュラッシュ</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手はそれを狙っている様子もありますかね。 今のところは狙ってない感じですかね。そこへボディを打ち込んでいく<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s>\n",
            "ラッシュラッシュ</s> <unk> 選手はそれを狙っている様子もありますかね。 今のところは狙ってない感じですかね。そこへボディを打っていくのは<unk> ですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手はそれを狙っている様子もありますかね。 今のところは狙ってない感じですかね。そこへボディを打っていくのは<unk> ですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s>\n",
            "ラッシュラッシュ</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkkVVbvvsWG",
        "outputId": "66ae0315-73ad-4699-9c95-5ee2d358822d"
      },
      "source": [
        "input = tokenizer.encode(\"ダウン\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ダウン</s> <unk> 。 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s>\n",
            "ダウン</s> <unk> 。 どんどん前に出てきがパンチが出ないでみんなそれぞれの山頂開始しました教えてきたがすごい簡単ものラウンドなかなかパンチが出せなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 第5ラウンドが終了しました。 いかがでしたか今のラウンド。お互い<unk> 選手はワンツーがめっちゃヒットし、<unk> 選手はこのボディとフック、左右、いろんな角度のついた感じで、ではい確認してましたね</s>\n",
            "ダウン</s> <unk> 。 第6ラウンドが終了しました。 いかがでしたか今のラウンド。お互い<unk> 選手はワンツーがめっちゃヒットし、<unk> 選手はこのボディとフック、左右、いろんな角度のついた感じで、ではい確認してましたね</s>\n",
            "ダウン</s> <unk> ました。<unk> 。 どんどん前に出てきがパンチが出ないでみんなそれぞれの山頂開始しました教えてきたがすごい簡単ものラウンドなかなかパンチが出せなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 さあ、この辺りの入りですけれども、<unk> Cもまずは丁寧にボクシング文だっていきたいですし、<unk> 選手も丁寧にボクシング文だっていきたいですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 どんどん前に出てきがパンチが出ないでみんなそれぞれの山頂開始しました教えてきたがすごい簡単ものラウンドなかなかパンチが出せなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 左のフックがコンビネーションを打つ。 両者ともにしっかりガードを固めている平安山を固めながら中に入ろうとする動き出された1通取って。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 どんどん前に出てきがパンチが出ないでみんなそれぞれの山頂開始しました教えてきたがすごい簡単ものラウンドなかなかパンチが出せなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> 。 どんどん前に出てきがパンチが出ないでみんなそれぞれの山頂開始しました教えてきたがすごい簡単ものラウンドなかなかパンチが出せなかった。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUggffnvsdo",
        "outputId": "246ededf-cffa-40b2-e750-badf56ebaa34"
      },
      "source": [
        "input = tokenizer.encode(\"あーとここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "あーとここで倒れた</s> 少しふらついた!左!少しふらついた!最後は左のボディ!やはりパンチには威力があります!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 少しふらついた!左!少しふらついた!最後は左のボディ!やはり両者の戦い!距離の測り愛といったところでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n",
            "あーとここで倒れた</s> 左のスタンドも残り2秒に対していくような感じです連絡ありました第一第二組のストレートが顔面を捉えて生地が少し負担するというシーンがありましたが先ほどの右のストレート顔面を捉えたちんですただ倒れずに堪えて逆にパンチを返していくという終盤の <unk> らんどでん舌舌舌付きジムから今日デビュー戦を迎えることになりました厚地とアレス大阪府堺市の出身地元大阪でのスタートとなり。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah8NV8oNwuO8",
        "outputId": "44f58db6-6437-4283-8270-53fb7c5c970a"
      },
      "source": [
        "input = tokenizer.encode(\"ここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.85, temperature=0.7, min_length=64, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ここで倒れた</s> 少しふらついた! やはり前に出ますね。 ただ引く感じではないと思うので、最終ラウンド、打ち合い覚悟で前に出てくるでしょう。 ジャブを打って、最後のスタミナを振り絞ってという最終ラウンドになりますかね。 やっぱり最終ラウンドになってもしっかりしているのでよ</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リング中央には神がいます。 今リングインです。 浜田麻里奈エスペランサジム所属です日本女子フライ級8位の選手今回は階級を下げての下げての今回試合となりました減量期間は少し長かったんですが試合のために減量頑張れたと甘いものが好きなんだとただそれを我慢するのが辛いんですがダイエットになるから意外と楽しめたと語っていましたがまだです。</s>\n",
            "ここで倒れた</s> 倒れなかった選手はそれに対して何が大事になってきますか川畑さんランドのあのやりにくいボクシングハイができたらいいかなと思いますね特に今流れを一度持ってかれそうになった後に切り替えるそのクレバーさもありましたよしっかり相手を見て試合をしてたと思いますこれが3杯目ということになりましたてらさきかずきただ頭を使うボクシングその磨いてきた練習の一環というのはこの試合も示せたようにも思いますがいかがでしょうか僕はすごくそう思いますね。</s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 倒れなかった選手も大概痛いですよね。立てないですよね。立てないですね。ただ、ただ倒れなかった選手も大概痛いですよね。立てないですね。ただ、倒れなかった選手もだいぶ効いていると思うので、このまま行けたらいいと思います。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リング中央には第4ラウンドのゴングが鳴りました。 58キロ契約で行われています8回戦今成田行きたい木村テミンの一戦は第5ラウンドに移ります。 第5ラウンド docomo チャレンジャーの続きは距離を詰めに行きますただ岩川はそれをかわしていく。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リング中央には第4ラウンドのゴングが鳴りました。 56キロ契約で行われています8回戦今成田行きたい木村テミンの一戦は第5ラウンドに移ります。 第5ラウンド このラウンドまずは慎重に見てるなーって感じですねえもうちょっとおバカで飲むことなんですけどもこれちょっとずっと続けてたらポイント取れないと思うんで次の次。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 踏み込んでいって右のフック!左のストレート右のフック!右のストレート<unk> も返していく! 第7ラウンドまで終了しています。残すところはあと2ラウンド。あと2ラウンドということになります。 <unk> 選手は中盤にかけて<unk> 勝利、このコロナ禍での応援に来てくださる方に対して、面白い試合をしたいと、言っていますが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リング中央には秘密ですがそれはダウンタウンで一緒にしてパンツが飛んできました三つだタウンセブン立ち上がって試合再開ただダウンを奪ったのは今成。 第6ラウンドが終了しました前半戦終了やはり実績のある選手同士の戦いハイレベルな戦いとなっています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リング中央には秘密ですがそれはダウンタウンで一緒にしてパンツが飛んできました三つだタウンセブン立ち上がって試合再開ただダウンを奪ったのは今成。 さあ6ラウンドです試合は後半戦。 今リングインです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> 少しふらついた!やはり前に出ますね。<unk> 選手も引く感じではないと思うので、最終ラウンド、打ち合い覚悟で前に出てくるでしょう。 ジャブを打って、最後のスタミナを振り絞ってという最終ラウンドになりますかね。気持ちの勝負だと思います。気持ちの勝負なのか、それともしっかりパンチを当てていくのか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXts53TaxMU1",
        "outputId": "930b34f2-9987-46f8-df6c-63e7b54de91f"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上から連打</s> 早くもコンビネーション!<unk> も返していきます!両者相打つ!右!右のジャブ!<unk> も返していきます!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くも浪速のターミネーター48なるか連れて行ってますよね今も千鳥届いたでしょうかになってますね藤井はどうするかで画像これから出していこうというのがそのうち終わりをやらは狙っていますそれでも藤井はジャムなども含めて出して行こうとしています左利きにかけてパンツを出していてコンビネーション1割を狙っているそんなシーンもありましたこの砂利はこの集団は誰が出しましたそのパンチもご覧いただきたいと思いますが。</s>\n",
            "ガードの上から連打</s> 早くも中村!左のジャブ!これも左! 今成も左のジャブを繰り出していきます。 第4ラウンドももう残り1分を切っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くも15秒が経過しました。 <unk> がこのラウンドは飛び込んでいきましたね。 そうですね。 <unk> C動きも良くなってきたんじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くも<unk> 腸!左!効いたか?効いたか?ただ打ち返していきます。 右のストレートフックギミックが出ていますが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くもコンビネーションだボディに打ち込んでいきます小西みかとも感じてるから第2ラウンド終了です。 第2ラウンドも福井のパンチが顔面を捉える殿ました後はれんどのシーンボディに顔面に。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くもコンビネーション!<unk>!ロープ際!追い詰めた!両者の戦い! 最後は気持ちの勝負だ!両者の戦い!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くも<unk> 腸!左のストレート!<unk> 腸だけで気持ちよくなりますか?気持ちいいですね。 感じてる以上に感じてる以上に感じているボディです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くもコンビネーション!<unk>!ノックアウト勝利で<unk> の試合終了です!両者の戦いは三者三様取ろうという結果になりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 早くもコンビネーション!<unk> Cも返して行きます!両者相打つ!右!右のジャブ!左のフック!右のストレート<unk> Cの左!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj4_nF4yxt4q",
        "outputId": "c8e9976c-e0f1-4c93-a867-73600c613c55"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたノックアウトです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 倒れたまま第4ラウンドが終了しました。 左のフック!ボディに打ち込んでいく<unk>!ロープ際! <unk> が勝利を収めました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> 倒れたまま大丈夫でしょうかこれから担架で運ばれるかというところです。 今リングサイドで様子を見ています高橋義生第4ラウンドが終了しました結構盛り上がったと思いますこのまま展開を続けていくんでしょうか。</s>\n",
            "倒れたノックアウトです。</s> 両者ともに意地と意地のぶつかり合いパンチの打ち合いになっている気持ちの勝負だ気持ちの勝負念願のプロのリングお互いに意地と意地のぶつかり合いファミリーのプレート試合を止めました。</s> </s> </s>\n",
            "倒れたノックアウトです。</s> お互いにパンチを打つこれはスリップですが付きました。 両者ともに左のジャブを起点としてこれも左のジャブを仕込んでいきました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErD4cWrSyard",
        "outputId": "bc5b810f-b1fc-4f2c-d48f-c8606f2298d4"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたKOです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "倒れた<unk> です。</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 やはりハードパンチャーですから一発大きいのがあればかなりの威力がありますこの赤コーナーの満田です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 やはりハードパンチャーですから一発大きいのがあればかなりの威力がありますこの赤コーナーの満田です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングというところ。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 非常にタフな選手寮選手です。 ボディガードの前で構えているのはその<unk> です。ノックアウト勝利で飾っていますやりチャドを出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> やはりこの序盤で目立って負けている相手に対して、ここまでは試合を支配している<unk> です。 今リングインです。 <unk> C女子アトム級王座決定戦第2ラウンドです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 倒れ込んでしまったから今リングインです。<unk> が勝利を収めました。女子フライ級4回戦、<unk> 対<unk> の試合は判定へと移っていきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 会場からは大きな拍手、女子フライ級4回戦ファイナルラウンドへと入っていきます。 左のジャブ、残り2分間という戦いになります。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 負ければ引退も視野に入ると話していました<unk> 、負けられない一戦が始まろうとしています。 いい目をしていますね。覚悟が決まったようないい顔をしています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> さあまず序盤から前に出ようというのは池上の方ですかねそうですねあの元々グイグイいくスタイルだと思うんでやっぱりあの今回は選手があの足を使っていい加減池上選手がちょっと追いかける展開になりそうです金の方は今日に関しては point out どちらかと言うとましを取って使って距離を取ってそんな形では判定勝ちでも構わないというようなボクシングでもとにかく8歳ということをね話してましたけれどもはいあそんな感じでいいと思いますね今見てる感じこううまいこと足使ってで左軸にボクシング組み立てれてると思うんで。</s>\n",
            "倒れた<unk> です。</s> 倒れてから試合開始までが早いですね。 今のところは久保が攻めているようですね。 さあ第2ラウンドが始まりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjlq2F41zC2j",
        "outputId": "aec3fcec-88e0-464e-a818-458b1831c279"
      },
      "source": [
        "input = tokenizer.encode(\"右のフックから左右連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のフックから左右連打</s> 少しボディが効いているか動きが遅くなってそれでも右のフックを打っています。 スイッチしましたね住吉右のフックボディに受けましたもう一度直して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 少しボディが効いているか動きが遅くなってそれでも右のフックを打っています。 スイッチしましたね住吉右のフックボディに受けましたもう一度直して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> コンビネーションも見えてきました。<unk> もこの距離は許さないというな構えを見せます。 第3ラウンドもかなり少なくなってきています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 自分の距離で戦えているのは山中でしょうかが入りました。 第3ラウンドは少し静か落ち着いて様子を見ているといったところでしょうかストレート少し頭が回りましたさあ第3ラウンド残り10秒の赤いスライムスレートコンビネーション見せてきましたこの後に起こる病気ってパンチ値するか下の子です第3ラウンド終了です落ち着いた試合展開で優先とは思いません山中すみれです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> <unk> 選手はそれを狙っている様子もありますかね。 第4ラウンドは少し静かな試合になりましたが。 <unk> C動きも良くなってきたんじゃないですか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングというところ。</s>\n",
            "右のフックから左右連打</s> 少しボディが効いているか動きが遅くなってそれでも右のフックを打っています。 スイッチしましたね住吉右のフックボディに受けましたもう一度直して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlzLienS-drA",
        "outputId": "fbbd4d03-2b37-4c5e-aee9-d67111b397b8"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上からボディー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ガードの上からボディー</s> 左のジャブで<unk> Cは真正ジムに移籍してこれが初戦ということになります。 <unk> 会長に何としてもお礼を言いたい、そういった思いで戦うこの一戦です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ガードの上からボディー</s> 左で距離を取りますヒヤリ。 第3ラウンドももう残り1分を切っています右この独特な右がありますよな晴れ。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> 左で距離を取りますヒヤリ。 第3ラウンドももう残り1分を切っています右この独特な右がありますよな晴れ。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> 左のジャブで<unk> Cは真正ジムに移籍してこれが初戦ということになります。 <unk> 会長に何としてもお礼を言いたい、そういった思いで戦うこの一戦です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ガードの上からボディー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sQQtEbd-pXu",
        "outputId": "8d22db22-25a7-4696-a1c7-06eaa5f4381d"
      },
      "source": [
        "input = tokenizer.encode(\"ジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ジャブ</s> ストレート <unk> 左の方にとって行きました。 次のストレートを打つ満田の画面取れたか少し聞いたか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> ストレート <unk> とどうでしょうかて画像出していますが頭のてっぺん高いですよねあれがやはりハードパンチャーですから一発大きいのがあればかなりの威力がありますこの赤コーナーの満田です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> ストレート <unk> みき みき <unk> この辺りの戦略というのはどうでしょうか? 今のところはそうですね、<unk> 選手が自分のペースに持って行こうとしたら、<unk> は逆に押し返してしまって、逆に押し返されてこそ、<unk> 選手のチャンピオンとしての真価が問われると思うんです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> ストレート <unk> を打って行きます。 少し距離を詰めていったこの両者、第3ラウンド流れが変わるかもしれません。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> ストレート 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 左のストレート右のフックを返していきます。コンビネーションコンビネーションを見せてくれ富士リズムを取りながら左大きく振ってきました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> ストレート <unk> みき みき みき <unk> この辺りのパンチの交差点の残り香の魅力ですよねそうですねはい非常にオーソドックスなタイプですねあのアマチュアの時にはしっかり練習を積んだというこの<unk> ですがプロのリングになってからはなかなか勝てなかったこの<unk> を自分の実力を証明するためにプロのリングで戦っていきたいそう語っていました<unk> です。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW_A-HPXOvhl",
        "outputId": "49d12acd-f6f4-40b1-b6a9-a2cfcd24ffb6"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブ</s> サウスポーにとってちょっと嫌な打ち下ろしの左ですね。 そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> プレッシャーをかけているのは山中です。ボディに打ち込んでいく。持ち味は左のフックと話しているこの山中ですが、まだ教育写真のここまでは出来ませんありませんね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> やはりこの辺りは手数を増やしていく。大切なのはどういったところですか? フットワークが良い選手なので、しっかり足を使って、<unk> 選手の動きを<unk> して寄っていくといいと思うんですけど</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> やはりこの辺りは手数を増やしていく。大切なのはどういったところですか? フットワークが良い選手なので、しっかり足を使って、<unk> 選手の動きを<unk> して寄っていくといいと思うんですけど</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> やはりこの辺りは手数を増やしていく。大切なのはどういったところですか? フットワークが良い選手なので、しっかり足を使って、<unk> 選手の動きを<unk> して寄っていくといいと思うんですけど</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> やはりこの辺りは手数を増やしていく。大切なのはどういったところですか? フットワークが良い選手なので、しっかり足を使って、<unk> 選手の動きを<unk> して寄っていくといいと思うんですけど</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> サウスポーにとって非常に嫌なジャブですね。サウスポーはこう左のジャブを突かないといけないっていう風に思ってると思うんですが、そこを左のボディがうまく合わせているので、たまんないですよね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> サウスポーにとってちょっと嫌な打ち下ろしの左ですね。どうしても右のフックを打っていきたくなるサウスポーにとってはちょっと嫌な打ち下ろしの左ですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> 何かじっとしたから伺いたいような変な感じですね動きたいんとこなんですかねそうですねやっぱりポイントを稼ぐには愛するという音ボクシングもしなければいけないと言っておりました五十嵐なりますがさあ第4ラウンドに入ります10番にかけて左のジャブそして左のフックというパンチを出していきました左のボディお花って言ったときょうへいちょっと変則的にモーションを変えましたねこの五十嵐です上昇に芝も対応してきているこの両者の戦い。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2BheEMBO0Md",
        "outputId": "822bb18a-1f88-4e68-b994-09a4042934de"
      },
      "source": [
        "input = tokenizer.encode(\"右のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のジャブ</s> いきなり少し変化がありましたか。まだ立てませんは来た。 いきなり第2ラウンド終了です。非常に激しい打ち合いになっている。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> いきなり少し変化がありましたでしょうか第2ラウンド終了です。 今のところは谷山がダウンを奪いましたから攻め続けた谷山が第2ラウンドでダウンを奪っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> いきなり少し変則的な動きになりましたが。。 <unk> ですかね。 今のところはそうだと思います。 ただ、<unk> の持ち味は左のフックと話しているので、そこがコンビネーションの一つのポイントになっています。</s>\n",
            "右のジャブ</s> いきなり少し変則的なステップになりましたが。。 <unk> ですかね。 今のところはそうだと思います。 今のところは<unk> がプレスをかけていこうというところですが右のジャブを打っています。</s> </s> </s>\n",
            "右のジャブ</s> いきなり少し変則的な動きになりましたが。。 <unk> ですかね。 今のところはそうだと思います。 今のところは<unk> がプレスをかけていこうというところですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> チリチリとプレッシャーをかけるきらりのストレートを返していきます。 プレッシャーをかけているのは渡辺の方ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> 左のジャブを繰り出してきたのは長いですが持ち味は左のフックと話しているこの長いですがまだ教育写真のここまでは出来ませんありませんねまだ出てきたこのぐらいの距離です。</s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> 左のジャブを繰り出してきたのは長いですが持ち味は左のフックと話しているこの長いですがまだ教育写真のここまでは出来ませんありませんねまだ出てきたこのぐらいの距離です。</s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> チリチリとプレッシャーをかけるきらりのストレートを返していきます。 第3ラウンドもどんどん前に出るプレッシャーを与えているのは竹島海斗。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> いきなりコンビネーションだしてきましたね。 今のは立方体と戻さなかった雑になりすぎているので、足が止まっている。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0lu5HoO28H",
        "outputId": "bedf870a-fc8c-4cce-8357-f714a878cf2c"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブからワンツー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のジャブからワンツー</s> コンビネーションを見せました。 やはり先手を取っているのは<unk> ですかね。 今のところはそうだと思います。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> やはりパンチには威力があります。<unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングというところ。</s>\n",
            "左のジャブからワンツー</s> やはり左のジャブを打っていきます。 どうでしょうかこの辺りの戦略というのはどうでしょうか?</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> やはり左のジャブを打って行きます藤原ですコンビネーションもここは出てきましたなかなか前に出られない22歳の富田大貴しかし前には出させないと言う前には来させないというこの国からです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> コンビネーションを見せました。 この若い<unk> 選手ですが、アマチュアの実績は十分という中で、落ち着いて試合展開、ここまでできているのは<unk> でしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> 右のストレート左のボディへと入っていきました小西みかとも感じてるから第2ラウンド終了です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> 試合序盤からハグが積極的に前に行きますこのサウスポーの選手赤のグローブ昨年の2月にデビューしそこで判定負けを喫してしまった泡とみなされた。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> プレッシャーをかけているのは山中です。ボディに打ち込んでいく。苦しい展開になっています。コンビネーションをもらって前に出ることが少し減ってきています<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijuN8PUdPaik",
        "outputId": "b57bc33d-08da-473c-eb5a-b8e5c07de1dc"
      },
      "source": [
        "input = tokenizer.encode(\"左のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "左のアッパー</s> <unk> 選手はそれを狙っている様子もありますかね。 第4ラウンド終了です。 いきなり左。うまいですね。思い切ってうってきたところを、どうですか今のところは<unk> も返して行きます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> トロフィーはここ早く外したいところですよね。 ここも体を当てて行きます残り30秒た世界タイトル初防衛世界タイトル初挑戦軍配が上がるのはどちらか気持ちを見せ続けてきた時そのチャレンジャーお迎え行ってきた今から10月が前に出る月が前に出る。</s>\n",
            "左のアッパー</s> リーペペレースボディに打ち込んでいく<unk> 、そして、ショートレンジのパンチ。<unk> が入ってきてもパンチで応戦して、逆に押し返しているのが<unk> ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ボディに打ち込んでいく<unk> 、そして、ショートレンジのパンチ。<unk> が入ってきてもパンチで応戦して、逆に押し返しているのが<unk> ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ボディに打ち込んでいく<unk> 、そして、ショートレンジのパンチ。<unk> が入ってきてもパンチで応戦して、逆に押し返しているのが<unk> ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> チリチリとプレッシャーをかけるきらりのストレートボディを返していたのはウルフルズ。 両者ともにまだ静かに立ち上がり距離を測っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ボディに打ち込んでいく<unk> 、そして、ショートレンジのパンチ。<unk> が入ってきてもパンチで応戦して、逆に押し返しているのが<unk> ですかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbGej1nPuhc",
        "outputId": "579ad859-b5fd-4e6f-b8b9-926a4d2c46ce"
      },
      "source": [
        "input = tokenizer.encode(\"右のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "右のアッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> <unk> 選手はそれを狙っている様子もありますかね。 今のところはそうだと思います。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがありますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 素晴らしいボディを見せてくれました。<unk> です。 第4ラウンド、ここまで激しい打ち合いになっている両者の戦い。ワンツー!こちらもボディで返していく<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 俺が見たが果たしてこれが見えているのかどうかこれもしたいですかねそうですねまうち割の狙ってると思いますその辺り口終り確実に捉えている堀川 mm これじゃんちにもいいこれね影響を与えてくればいいなあその辺りの戦いぶりになっていますこの辺りで左のジャブを出していくところ見ながらの右ですか左の方に入れてきました左のジャブから左のジャブを返していきますさすがはベテランこの58.2匹のボディストレートそして距離を取ってそしてまた距離を詰めていくというこの堀川です入野ジャブから左に書いてきました冨田です大丈夫そうだったそしてこんなにじわじわと攻めて行ってそれから離れてこれたとすると本当にやりにくさは感じてるでしょうねちょっとますねもしかしたいのに。</s>\n",
            "右のアッパー</s> ジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> <unk> 選手はそれを狙っている様子もありますかね。 第4ラウンド終了です。 ちょっと様子見という第4ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがありますかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 俺はガードを固めながら右のフックを打っていきます。 左のストレート左のフックですね。 非常にタフな選手寮選手です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> フットワークを生かして相手を翻弄しようとする<unk> です。 第3ラウンドも残り1分を切っています。 <unk> は依然としてこの速いこのジャブを左右・ワンツーと出しておきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj7ohJTrRIeW",
        "outputId": "2b69a5c7-9366-4f27-fded-c196e1fa94ec"
      },
      "source": [
        "input = tokenizer.encode(\"アッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左ボディをやってきました長いです先ほどからリズムが予算良くなってきたという話もありました。 んじゃないかなとは思いますねこの辺りのこの上下の打ち分けその入る時のスタンドさんのストレートハイ。</s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左ボディをやってきました<unk> です。 <unk> がどう攻めていくか。 第7ラウンドまで終了しています。 二人の一戦は最終ラウンドへと移っていきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "アッパー</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtGG0ILqRVbY",
        "outputId": "a4e74e36-28cb-4310-9cac-a90bc9dbfdba"
      },
      "source": [
        "input = tokenizer.encode(\"カウンター\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングというところ。</s>\n",
            "カウンター</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 今のところはそこら辺の距離感も非常に左のジャブで返していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> みかとはよく寝つきを良くしていたんで吐いたりやりにくい距離感がピンボケするのではいよく言われる距離感が難しいというところなんですけども野上さんその辺りはいかがですかそうですねやはり常にオーソドックスと戦うことが多いのでそれを散歩に切り替えられたりっていうことはとてもやりにくい状況だと思いますもちろん鈴木奈々イモトのスイッチとは今回が初対戦になります第一ランドまもなくゴングというところ。</s>\n",
            "カウンター</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piw5DZQ3UdGW",
        "outputId": "467c504f-10c1-47d3-ab3a-09c6eccc4d81"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> いつも通りボクシングをして、いつも通り勝利をつかみたいと伺っていました。 <unk> です。 いつも通り練習通りのボクシングをし、いつも通りの勝利を掴みたいと伺っていました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n",
            "試合展開</s> ピリピリとしたこの空気感とコンプレックスです本当にお互いがボクシング人生にかけているって言うならない選手は試合の序盤にダウンを奪われましたが第5ラウンドあたりからね自分の距離で戦えるようになる自分だけじゃないかもしれないですけどもあいつの強さで自分のペースに持って行こうとしているのがちょっと見えるかなと僕は思いまなるほど本来はもう着く距離を取り込んだと思う。</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_uAXTaYirD",
        "outputId": "cdf9586b-1774-45b3-9139-297b9e3a14b5"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "試合展開について</s> 少し<unk> 腸を出してきたのでたですはじめの一歩の左上あたりの部分が赤くなっていますでしょうか距離は少し詰まっていますでしょうかはじめの一歩の左上あたりの部分が赤くなっています。</s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいのでどんどん前に出ていくたいですね。 そこにボディを打ち込んでいく<unk> 。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいのでどんどん前に出ていくスタミナ勝負に持ち込みたいと語っていました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいので お休みしますがまだ入るとのこと。 体力ものこっていますし。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいのでほしいですねそうですね私も残り10秒、最後力を振り絞ってほしいですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいので お休みしますがまだ入るとのこと。 体力ものこっていますし。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいので お休みしますがまだ入るとのこと。 体力ものこっていますし。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいのでどんどん前に出ていくたいですね。そうしないと<unk> はどんどん自分のペースに持って行こうとするので。</s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいのでどんどん前に出ていくたいですね。そうしないと<unk> はどんどん自分のペースになってしまいます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 少し<unk> 、スタミナ厳しいですから気持ちでいいので お休みしますがまだ入るとのこと。 体力ものこっていますし。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS9RLYW2YehG",
        "outputId": "f8e1e2a6-de97-46f9-d667-c5184af8e59e"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシング\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボクシング</s> 左のジャブが伸びている。 第3ラウンドももう残り1分を切っています右のストレート。 さあ山中さんあい最後は今それと出しましたねまだ石巻市に来てますね。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 まずは飛び出したのが<unk> です。 やはり元アマチュアの選手なので距離を取って隙のない選手と思っていたのですが<unk> ですが、そういった展開、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 まずは飛び出したのが<unk> です。 やはり元アマチュアの選手なので距離を取って隙のない選手と思っていたのですが<unk> ですが、そういった展開、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 まずは飛び出したのが<unk> です。 やはり元アマチュアの選手なので距離を取って隙のない選手と思っていたのですが<unk> ですが、そういった展開、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 まずは飛び出したのが<unk> です。 これまで3戦は2勝1分け、対する<unk> は6戦2勝。2勝のうち一つは<unk> 勝利、3連敗からの2連勝もありました。</s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 <unk> 対<unk> の一戦が始まってすぐにゴングが鳴り響きました。 赤いボクシンググローブが<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ボクシング</s> 56キロ契約で行われます8回戦。 まずは飛び出したのが<unk> です。 やはり元アマチュアの選手なので距離を取って隙のない選手と思っていたのですが<unk> ですが、そういった展開、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 左のジャブを打って行きました青いグローブ能登元恭平ラスそして中川は今両手を広げて胸を見せるような仕草を見せました左のボディお花って言ったときょうへい中川も左のジャブを繰り出していきます。</s>\n",
            "ボクシング</s> キャリアは10年。 <unk> 。 さあ、これがデビュー戦です。 <unk> C女子アトム級王座決定戦、今試合が始まりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDqzn1wXWXro",
        "outputId": "a57b3685-ae21-46b3-c2b4-f22f1286f899"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手今の顔もそうなんですが、落ち着いていますね。さあ行くぞみたいな顔してたんで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8DlV3bYWia9",
        "outputId": "e322a865-355c-4d7d-ba5e-9937ea665e51"
      },
      "source": [
        "input = tokenizer.encode(\"今回のボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "今回のボクシングの試合について</s> <unk> 選手は前回の試合がキャリア初の黒星そして初の<unk> 勝利でしたそこからの再起戦ということになります今日の試合も前回と同じ気持ちで試合に臨んでくるでしょうか<unk> 選手。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> チャンピオンの岩川もかなり肩で息していますからたださんこれやはり岩川にとってもなかなか厳しいラウンドになってますよねそうですねあの体のサイズが違う中でチャンピオンやっぱりもうちょっとショートレンジャーあの出していくっていうなんか工作がないとパーク in は覚えてきたので慌てていくっていう必要性はありませんあの鈴木選手のあの本当にあのボディはい中でその中のストーリーの一つのパンチがないのではいちょっと鈴木選手が戦い方に今ちょっと揉まれて言ってるのになるほどですから鈴木が体を密着させる前の所で何か一つこの距離感という間でしたねこの避けていく中で何かチャンピオンは一つビックパンチが欲しいところ。</s>\n",
            "今回のボクシングの試合について</s> <unk> は本当に落ち着いて試合展開、ただ強気のボクシングを見せている。 中盤にかけてその<unk> に対して、キャリアのある元日本チャンピオンですから<unk> は。どのような展開を見せていくべきでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> は本当に落ち着いて試合展開、自分のボクシングを取り戻そうという思いが強い選手ですね。 <unk> 選手はリラックスして、しっかり相手を見てるなと思いました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> 選手はジムでも愛されキャラでお出かけスタイル的に気持ちが強い選手なので相手の選手もききっちに来て。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> は何もさせてくれなかったという感じじゃないですかね。 今のところはそうだと思います。 今この<unk> 選手の立場から見ると、その元世界チャンピオンという間この肩書き、そしてこれからの可能性、というところも感じてるでしょうね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> は本当に落ち着いて試合展開、ただ強気のボクシングを見せている。 中盤にかけてその<unk> に対して、キャリアのある元日本チャンピオンですから<unk> は。どのような展開を見せていくべきでしょうか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> <unk> はいつも通りいつも通りのボクシングをしたいと話していました。 今回、<unk> が日本タイトル初挑戦となります。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5QCrAIYJxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145a7502-a44d-495f-b450-165ddf9e896d"
      },
      "source": [
        "input = tokenizer.encode(\"かなりのダメージを受けている\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "かなりのダメージを受けている</s> <unk> 選手はそれを狙っている様子もありますかね。 サウスポーにとってちょっと嫌な打ち下ろしの左ストレートでくるんですけど、そこが長所であり、あえて狙うならばそこが短所になるので、</s>\n",
            "かなりのダメージを受けている</s> さあ。 今出ている距離を詰めてパンチを出せるかの時は新治ボディにパンチを集めている開き状態をやって距離を詰めて高橋をしても左を出していく。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> そこへボディを打つ!<unk>! やはりパンチには威力があります。<unk> です。 今成もパンチを返してきたらどんどん前に出ていく、ボディです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> そこまで考えて両者ともに飛び出したこの両者の戦い。 パンチの打ち合いになりました。 右のフックから右のストレート、これが一つ鍵になってきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> そこまで考えてコンビネーションを打つと。。 少しパンチが大きくなっているが、返して行きます<unk> です。ワンツーと打っていく。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> 先ほどの神ですこの右のストレート中に入り込もうというところです。 今リングギアに追い込まれていますが何とかならなかったですね危なかったところでしたいやここは両者。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> <unk> です。 さあ、まず序盤から前に出ようというのは<unk> ですかね。 そうですね、もうちょっと<unk> 自身も予想外なことをされたのかなっていう感じにはなりますけども。</s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> そこまで考えて両者ともにジャブを打っていきます。 右のストレート右のボディへと入っていきました小西みかとも感じてるから第2ラウンド終了です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> そこまで考えて両者ともにパンツを売っているここまでの戦いです。 少し第2ラウンド高いから変わりました少し第2ラウンド高いから変わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> さあこれが再起戦ということになります。 今出ている距離を詰めてパンチを出せるかの時は新治ボディにパンチを集めている開き状態をやって距離を詰めて高橋をしても左を出していく。</s> </s> </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-W9pewlyv6x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}