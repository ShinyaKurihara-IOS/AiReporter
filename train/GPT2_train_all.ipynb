{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train_all.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHfd98YKpAa"
      },
      "source": [
        "以下の設定になっていること  \n",
        "\n",
        "ランタイム > ランタイムのタイプを変更  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  \n",
        "\n",
        "編集 > ノートブックの設定  \n",
        "ハードウェアアクセラレータ：GPU  \n",
        "ランタイムの仕様：ハイメモリ  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAZUtGFrIE0N",
        "outputId": "903a45f5-9235-41bb-ef2b-00a307e10d0f"
      },
      "source": [
        "# googleドライブをマウント\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "# 表示されるリンクをクリックして、アクセスを許可して、最後に表示される文字列を以下の入力欄に入れる"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT6Phq7IINCz",
        "outputId": "8fddb2af-9419-41a6-d36e-8f371c4fbccc"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c7WZHPnIqSv",
        "outputId": "e66e1697-eeb9-4b9c-f313-84da90c67d17"
      },
      "source": [
        "# importで使う必要があるので、インストールがランタイム切れるごとに必要\n",
        "# インストール後にランタイムの再起動を行わないとT5Tokenizerが見つからない\n",
        "# メニュー「ランタイム → ランタイムを再起動」で「Google Colab」を再起動\n",
        "\n",
        "# ドライブに保存してるものでインストール\n",
        "!pip install -e transformers\n",
        "\n",
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/work/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.4.2) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n",
            "Collecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 9.6 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (4.6.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1) (0.3.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Installing collected packages: xxhash, tqdm, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "Successfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWjtNtvnRTPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea50934b-20a2-453b-cad8-6d5e4d4d9561"
      },
      "source": [
        "# 作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/work/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tepd4-4WCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZtynWz0u4IT"
      },
      "source": [
        "CLM（Causal Language Modeling）: GPT、GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iKmxERxt7Io"
      },
      "source": [
        "・model_name_or_path: モデルのチェックポイント（モデルを最初から学習しない場合）  \n",
        "・model_type: モデルの種別（モデルを最初から学習する場合）  \n",
        "・config_name: コンフィグ名（model_nameと同じでない場合）  \n",
        "・tokenizer_name: トークナイザー名（model_nameと同じでない場合）  \n",
        "・cache_dir: キャッシュフォルダ  \n",
        "・use_fast_tokenizer: Fastトークナイザーを使用するかどうか  \n",
        "・model_revision: 使用するモデルの特定のバージョン  \n",
        "・use_auth_token: 「transformers-cli login」の実行時に生成されたトークンを使用するかどうか  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l0KKz6auJXj"
      },
      "source": [
        "・dataset_name: データセット名  \n",
        "・dataset_config_name: データセットのコンフィグ名  \n",
        "・train_file: 学習データ（テキストファイル）  \n",
        "・validation_file: 検証データ（テキストファイル）  \n",
        "・overwrite_cache: キャッシュの上書き  \n",
        "・validation_split_percentage: 学習データから使われる検証データの割合（検証データがない場合）  \n",
        "・max_seq_length: トークン化後の最大合計入力シーケンス長  \n",
        "・preprocessing_num_workers: 前処理に使用するプロセス数  \n",
        "・block_size: トークン化後のオプションの入力シーケンス長  \n",
        "・max_train_samples: 学習データの最大数  \n",
        "・max_val_samples: 検証データの最大数  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9LIvEaz6Pst"
      },
      "source": [
        "    # GPT2のモデルファイルを指定\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    # 学習ファイル\n",
        "    --train_file=train.txt \\\n",
        "    # 評価データファイル\n",
        "    --validation_file=train.txt \\\n",
        "    # トレーニングを実施する\n",
        "    --do_train \\\n",
        "    # 評価を実施する\n",
        "    --do_eval \\\n",
        "    # 学習回数（エポック数）\n",
        "    --num_train_epochs=30 \\\n",
        "    # チェックポイントの保存間隔\n",
        "    --save_steps=5000 \\\n",
        "    # チェックポイントの保持数\n",
        "    --save_total_limit=3 \\\n",
        "    # T5Tokenizer.model_max_length=1024をチャンクサイズとして使用するかblock_sizeで指定するかを設定する（設定しないとtokenizerの超大なサイズから1024になる）メモリに乗るように調整する必要がある（バッチサイズとの兼ね合い）\n",
        "    --block_size=512 \\\n",
        "    # GPU1つあたりの学習バッチサイズ\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    # GPU1つあたりの評価バッチサイズ\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    # モデルとチェックポイントの出力先\n",
        "    --output_dir=output/ \\\n",
        "    # 出力先の上書きの許可\n",
        "    --overwrite_output_dir=True \\\n",
        "    # T5Tokenizerで高速化ライブラリがあれば使用する\n",
        "    --use_fast_tokenizer=False\n",
        "\n",
        "### 学習する回数\n",
        "エポック数 * (データ文字数 / ブロックサイズ):1つの学習データ / バッチサイズ  \n",
        "    エポック数 = 1  \n",
        "    データ文字数 = 22142  \n",
        "    データの文字数だと数字が合わないので、tokenizeした結果の形態素数による？\n",
        "    ブロックサイズ = 512  \n",
        "    データ文字数 / ブロックサイズ = 43  \n",
        "    バッチサイズ = 1  \n",
        "    Total optimization steps = 29  \n",
        "\n",
        "\n",
        "バッチサイズ = 2  \n",
        "バッチサイズを2にすることで、トータルの実行回数が減っている  \n",
        "Total optimization steps = 15  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zS2rIpZIfXB",
        "outputId": "88ae215b-8932-4e36-a9ff-c828efc52433"
      },
      "source": [
        "%%time\n",
        "\n",
        "# ファインチューニングの実行\n",
        "!python ./transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train_all_ent.txt \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs=500 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --block_size=256 \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --output_dir=output_all/ \\\n",
        "    --overwrite_output_dir=True \\\n",
        "    --use_fast_tokenizer=False"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/07/2021 11:46:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/07/2021 11:46:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=output_all/, overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=500.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep07_11-46-25_90d824060d5a, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=5000, save_total_limit=3, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=output_all/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
            "Downloading: 2.57kB [00:00, 3.05MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset text/default-b8ce191bbe380d35 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-b8ce191bbe380d35/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-b8ce191bbe380d35/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1386] 2021-09-07 11:46:28,212 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9um28sik\n",
            "Downloading: 100% 799/799 [00:00<00:00, 683kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-09-07 11:46:28,479 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|file_utils.py:1393] 2021-09-07 11:46:28,479 >> creating metadata file for /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:463] 2021-09-07 11:46:28,480 >> loading configuration file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4f2220bae6b6e6e9de761491c836858e9a4a297b9d8a58071e454c4dc03b5463.52b718e4a0d99e40aaaf447d3818b20b306909860ff4d3623cf948c2cc5c7570\n",
            "[INFO|configuration_utils.py:499] 2021-09-07 11:46:28,480 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": 4096,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1386] 2021-09-07 11:46:28,745 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp56tz1m2b\n",
            "Downloading: 100% 806k/806k [00:00<00:00, 1.25MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-09-07 11:46:29,852 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1393] 2021-09-07 11:46:29,852 >> creating metadata file for /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|file_utils.py:1386] 2021-09-07 11:46:30,369 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphkjtqros\n",
            "Downloading: 100% 153/153 [00:00<00:00, 144kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-09-07 11:46:30,625 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1393] 2021-09-07 11:46:30,625 >> creating metadata file for /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|file_utils.py:1386] 2021-09-07 11:46:30,884 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_7tgild4\n",
            "Downloading: 100% 282/282 [00:00<00:00, 280kB/s]\n",
            "[INFO|file_utils.py:1390] 2021-09-07 11:46:31,147 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|file_utils.py:1393] 2021-09-07 11:46:31,147 >> creating metadata file for /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-09-07 11:46:31,409 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/a7b51178c78979cf1a0a901647886cbf99626e1cb9a26eba30d4f59ac46ebf17.c0b735c65f40dff8596b5f699043bb29048036242443fea32b79a9dd8510ea96\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-09-07 11:46:31,409 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-09-07 11:46:31,409 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/42091916a8a40b3949b8a4f56ce63e437a166ae0e88d1d15546860c13bdc5ceb.9049458ebcd1cf666b7b0a046aa394597f12e611077571cfc86e0938f8675d82\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-09-07 11:46:31,409 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/797151d567a04d67dc126ddf3c4cc4779887653076191eaa7b08f73f4ff874a2.9c6d86638d0c8b0d39297c982899c13374e883f6e85a7c2c9baad32a40abf7dd\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-09-07 11:46:31,409 >> loading file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1386] 2021-09-07 11:46:31,727 >> https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmyv4jwt5\n",
            "Downloading: 100% 1.37G/1.37G [01:03<00:00, 21.6MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-09-07 11:47:35,569 >> storing https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|file_utils.py:1393] 2021-09-07 11:47:35,569 >> creating metadata file for /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1051] 2021-09-07 11:47:35,570 >> loading weights file https://huggingface.co/rinna/japanese-gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/535bf84748bb0fd4692430e45803b0a8cec51dc5f8581c46ebca3c5d470c75df.cc70e8c279faa70b5303f7802493074be521ae42d129355a708b119ca945c8cb\n",
            "[INFO|modeling_utils.py:1167] 2021-09-07 11:47:43,741 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1176] 2021-09-07 11:47:43,741 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 13/13 [00:00<00:00, 13.83ba/s]\n",
            "100% 13/13 [00:00<00:00, 21.82ba/s]\n",
            "[INFO|trainer.py:946] 2021-09-07 11:47:52,130 >> ***** Running training *****\n",
            "[INFO|trainer.py:947] 2021-09-07 11:47:52,130 >>   Num examples = 570\n",
            "[INFO|trainer.py:948] 2021-09-07 11:47:52,130 >>   Num Epochs = 500\n",
            "[INFO|trainer.py:949] 2021-09-07 11:47:52,130 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:950] 2021-09-07 11:47:52,130 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:951] 2021-09-07 11:47:52,130 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:952] 2021-09-07 11:47:52,130 >>   Total optimization steps = 71500\n",
            "{'loss': 3.0527, 'learning_rate': 4.9650349650349656e-05, 'epoch': 3.5}\n",
            "{'loss': 2.0173, 'learning_rate': 4.93006993006993e-05, 'epoch': 6.99}\n",
            "{'loss': 1.1658, 'learning_rate': 4.8951048951048956e-05, 'epoch': 10.49}\n",
            "{'loss': 0.6197, 'learning_rate': 4.86013986013986e-05, 'epoch': 13.99}\n",
            "{'loss': 0.3049, 'learning_rate': 4.825174825174825e-05, 'epoch': 17.48}\n",
            "{'loss': 0.1679, 'learning_rate': 4.7902097902097904e-05, 'epoch': 20.98}\n",
            "{'loss': 0.1025, 'learning_rate': 4.755244755244756e-05, 'epoch': 24.48}\n",
            "{'loss': 0.0747, 'learning_rate': 4.7202797202797204e-05, 'epoch': 27.97}\n",
            "{'loss': 0.0601, 'learning_rate': 4.685314685314686e-05, 'epoch': 31.47}\n",
            "{'loss': 0.0523, 'learning_rate': 4.6503496503496505e-05, 'epoch': 34.97}\n",
            "  7% 5000/71500 [1:14:12<16:23:42,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 13:02:04,776 >> Saving model checkpoint to output_all/checkpoint-5000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 13:02:05,336 >> Configuration saved in output_all/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 13:02:25,749 >> Model weights saved in output_all/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 13:02:26,068 >> tokenizer config file saved in output_all/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 13:02:30,039 >> Special tokens file saved in output_all/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 13:02:30,607 >> Copy vocab file to output_all/checkpoint-5000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 13:03:35,638 >> Deleting older checkpoint [output_all/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 0.0474, 'learning_rate': 4.615384615384616e-05, 'epoch': 38.46}\n",
            "{'loss': 0.0438, 'learning_rate': 4.5804195804195805e-05, 'epoch': 41.96}\n",
            "{'loss': 0.0441, 'learning_rate': 4.545454545454546e-05, 'epoch': 45.45}\n",
            "{'loss': 0.039, 'learning_rate': 4.5104895104895105e-05, 'epoch': 48.95}\n",
            "{'loss': 0.0367, 'learning_rate': 4.475524475524476e-05, 'epoch': 52.45}\n",
            "{'loss': 0.0353, 'learning_rate': 4.4405594405594406e-05, 'epoch': 55.94}\n",
            "{'loss': 0.0338, 'learning_rate': 4.405594405594406e-05, 'epoch': 59.44}\n",
            "{'loss': 0.0328, 'learning_rate': 4.370629370629371e-05, 'epoch': 62.94}\n",
            "{'loss': 0.0314, 'learning_rate': 4.335664335664335e-05, 'epoch': 66.43}\n",
            "{'loss': 0.031, 'learning_rate': 4.300699300699301e-05, 'epoch': 69.93}\n",
            " 14% 10000/71500 [2:29:32<15:12:02,  1.12it/s][INFO|trainer.py:1558] 2021-09-07 14:17:24,591 >> Saving model checkpoint to output_all/checkpoint-10000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 14:17:24,596 >> Configuration saved in output_all/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 14:17:29,557 >> Model weights saved in output_all/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 14:17:29,562 >> tokenizer config file saved in output_all/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 14:17:29,565 >> Special tokens file saved in output_all/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 14:17:29,572 >> Copy vocab file to output_all/checkpoint-10000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 14:18:02,963 >> Deleting older checkpoint [output_all/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 0.0296, 'learning_rate': 4.265734265734266e-05, 'epoch': 73.43}\n",
            "{'loss': 0.0299, 'learning_rate': 4.230769230769231e-05, 'epoch': 76.92}\n",
            "{'loss': 0.0293, 'learning_rate': 4.195804195804196e-05, 'epoch': 80.42}\n",
            "{'loss': 0.0301, 'learning_rate': 4.1608391608391614e-05, 'epoch': 83.92}\n",
            "{'loss': 0.0283, 'learning_rate': 4.125874125874126e-05, 'epoch': 87.41}\n",
            "{'loss': 0.0271, 'learning_rate': 4.0909090909090915e-05, 'epoch': 90.91}\n",
            "{'loss': 0.027, 'learning_rate': 4.055944055944056e-05, 'epoch': 94.41}\n",
            "{'loss': 0.0289, 'learning_rate': 4.020979020979021e-05, 'epoch': 97.9}\n",
            "{'loss': 0.0261, 'learning_rate': 3.986013986013986e-05, 'epoch': 101.4}\n",
            "{'loss': 0.0257, 'learning_rate': 3.9510489510489516e-05, 'epoch': 104.9}\n",
            " 21% 15000/71500 [3:44:03<13:56:55,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 15:31:55,401 >> Saving model checkpoint to output_all/checkpoint-15000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 15:31:55,405 >> Configuration saved in output_all/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 15:31:59,630 >> Model weights saved in output_all/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 15:31:59,636 >> tokenizer config file saved in output_all/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 15:31:59,640 >> Special tokens file saved in output_all/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 15:32:01,744 >> Copy vocab file to output_all/checkpoint-15000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 15:32:29,471 >> Deleting older checkpoint [output_all/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 0.0252, 'learning_rate': 3.916083916083916e-05, 'epoch': 108.39}\n",
            "{'loss': 0.0257, 'learning_rate': 3.8811188811188816e-05, 'epoch': 111.89}\n",
            "{'loss': 0.0251, 'learning_rate': 3.846153846153846e-05, 'epoch': 115.38}\n",
            "{'loss': 0.0244, 'learning_rate': 3.811188811188811e-05, 'epoch': 118.88}\n",
            "{'loss': 0.0247, 'learning_rate': 3.776223776223776e-05, 'epoch': 122.38}\n",
            "{'loss': 0.0249, 'learning_rate': 3.741258741258741e-05, 'epoch': 125.87}\n",
            "{'loss': 0.0243, 'learning_rate': 3.7062937062937064e-05, 'epoch': 129.37}\n",
            "{'loss': 0.0246, 'learning_rate': 3.671328671328672e-05, 'epoch': 132.87}\n",
            "{'loss': 0.0264, 'learning_rate': 3.6363636363636364e-05, 'epoch': 136.36}\n",
            "{'loss': 0.0241, 'learning_rate': 3.601398601398602e-05, 'epoch': 139.86}\n",
            " 28% 20000/71500 [4:58:27<12:41:12,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 16:46:19,635 >> Saving model checkpoint to output_all/checkpoint-20000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 16:46:19,639 >> Configuration saved in output_all/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 16:46:25,940 >> Model weights saved in output_all/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 16:46:25,945 >> tokenizer config file saved in output_all/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 16:46:25,948 >> Special tokens file saved in output_all/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 16:46:25,954 >> Copy vocab file to output_all/checkpoint-20000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 16:46:52,592 >> Deleting older checkpoint [output_all/checkpoint-5000] due to args.save_total_limit\n",
            "{'loss': 0.0235, 'learning_rate': 3.566433566433567e-05, 'epoch': 143.36}\n",
            "{'loss': 0.0232, 'learning_rate': 3.531468531468531e-05, 'epoch': 146.85}\n",
            "{'loss': 0.0222, 'learning_rate': 3.4965034965034965e-05, 'epoch': 150.35}\n",
            "{'loss': 0.0226, 'learning_rate': 3.461538461538462e-05, 'epoch': 153.85}\n",
            "{'loss': 0.022, 'learning_rate': 3.4265734265734265e-05, 'epoch': 157.34}\n",
            "{'loss': 0.0222, 'learning_rate': 3.391608391608392e-05, 'epoch': 160.84}\n",
            "{'loss': 0.0216, 'learning_rate': 3.356643356643357e-05, 'epoch': 164.34}\n",
            "{'loss': 0.021, 'learning_rate': 3.321678321678322e-05, 'epoch': 167.83}\n",
            "{'loss': 0.0285, 'learning_rate': 3.2867132867132866e-05, 'epoch': 171.33}\n",
            "{'loss': 0.0209, 'learning_rate': 3.251748251748252e-05, 'epoch': 174.83}\n",
            " 35% 25000/71500 [6:12:54<11:26:22,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 18:00:47,013 >> Saving model checkpoint to output_all/checkpoint-25000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 18:00:47,017 >> Configuration saved in output_all/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 18:00:51,526 >> Model weights saved in output_all/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 18:00:51,530 >> tokenizer config file saved in output_all/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 18:00:51,533 >> Special tokens file saved in output_all/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 18:00:51,540 >> Copy vocab file to output_all/checkpoint-25000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 18:01:25,254 >> Deleting older checkpoint [output_all/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 0.0207, 'learning_rate': 3.216783216783217e-05, 'epoch': 178.32}\n",
            "{'loss': 0.0196, 'learning_rate': 3.181818181818182e-05, 'epoch': 181.82}\n",
            "{'loss': 0.0192, 'learning_rate': 3.146853146853147e-05, 'epoch': 185.31}\n",
            "{'loss': 0.022, 'learning_rate': 3.111888111888112e-05, 'epoch': 188.81}\n",
            "{'loss': 0.0187, 'learning_rate': 3.0769230769230774e-05, 'epoch': 192.31}\n",
            "{'loss': 0.0182, 'learning_rate': 3.0419580419580425e-05, 'epoch': 195.8}\n",
            "{'loss': 0.0172, 'learning_rate': 3.0069930069930068e-05, 'epoch': 199.3}\n",
            "{'loss': 0.0173, 'learning_rate': 2.972027972027972e-05, 'epoch': 202.8}\n",
            "{'loss': 0.0162, 'learning_rate': 2.9370629370629372e-05, 'epoch': 206.29}\n",
            "{'loss': 0.0162, 'learning_rate': 2.9020979020979022e-05, 'epoch': 209.79}\n",
            " 42% 30000/71500 [7:27:25<10:15:52,  1.12it/s][INFO|trainer.py:1558] 2021-09-07 19:15:18,133 >> Saving model checkpoint to output_all/checkpoint-30000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 19:15:18,138 >> Configuration saved in output_all/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 19:15:23,983 >> Model weights saved in output_all/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 19:15:23,987 >> tokenizer config file saved in output_all/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 19:15:23,991 >> Special tokens file saved in output_all/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 19:15:23,996 >> Copy vocab file to output_all/checkpoint-30000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 19:15:52,024 >> Deleting older checkpoint [output_all/checkpoint-15000] due to args.save_total_limit\n",
            "{'loss': 0.0152, 'learning_rate': 2.8671328671328672e-05, 'epoch': 213.29}\n",
            "{'loss': 0.0141, 'learning_rate': 2.8321678321678326e-05, 'epoch': 216.78}\n",
            "{'loss': 0.0133, 'learning_rate': 2.7972027972027976e-05, 'epoch': 220.28}\n",
            "{'loss': 0.012, 'learning_rate': 2.762237762237762e-05, 'epoch': 223.78}\n",
            "{'loss': 0.0118, 'learning_rate': 2.7272727272727273e-05, 'epoch': 227.27}\n",
            "{'loss': 0.0107, 'learning_rate': 2.6923076923076923e-05, 'epoch': 230.77}\n",
            "{'loss': 0.0101, 'learning_rate': 2.6573426573426574e-05, 'epoch': 234.27}\n",
            "{'loss': 0.0101, 'learning_rate': 2.6223776223776224e-05, 'epoch': 237.76}\n",
            "{'loss': 0.0098, 'learning_rate': 2.5874125874125877e-05, 'epoch': 241.26}\n",
            "{'loss': 0.009, 'learning_rate': 2.5524475524475528e-05, 'epoch': 244.76}\n",
            " 49% 35000/71500 [8:41:58<8:59:39,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 20:29:50,564 >> Saving model checkpoint to output_all/checkpoint-35000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 20:29:50,569 >> Configuration saved in output_all/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 20:29:55,076 >> Model weights saved in output_all/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 20:29:55,080 >> tokenizer config file saved in output_all/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 20:29:55,083 >> Special tokens file saved in output_all/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 20:29:56,860 >> Copy vocab file to output_all/checkpoint-35000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 20:30:26,181 >> Deleting older checkpoint [output_all/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 0.0092, 'learning_rate': 2.5174825174825178e-05, 'epoch': 248.25}\n",
            "{'loss': 0.009, 'learning_rate': 2.4825174825174828e-05, 'epoch': 251.75}\n",
            "{'loss': 0.0093, 'learning_rate': 2.4475524475524478e-05, 'epoch': 255.24}\n",
            "{'loss': 0.0081, 'learning_rate': 2.4125874125874125e-05, 'epoch': 258.74}\n",
            "{'loss': 0.0081, 'learning_rate': 2.377622377622378e-05, 'epoch': 262.24}\n",
            "{'loss': 0.0084, 'learning_rate': 2.342657342657343e-05, 'epoch': 265.73}\n",
            "{'loss': 0.0077, 'learning_rate': 2.307692307692308e-05, 'epoch': 269.23}\n",
            "{'loss': 0.0074, 'learning_rate': 2.272727272727273e-05, 'epoch': 272.73}\n",
            "{'loss': 0.0076, 'learning_rate': 2.237762237762238e-05, 'epoch': 276.22}\n",
            "{'loss': 0.0079, 'learning_rate': 2.202797202797203e-05, 'epoch': 279.72}\n",
            " 56% 40000/71500 [9:56:27<7:46:22,  1.13it/s][INFO|trainer.py:1558] 2021-09-07 21:44:20,025 >> Saving model checkpoint to output_all/checkpoint-40000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 21:44:20,029 >> Configuration saved in output_all/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 21:44:25,050 >> Model weights saved in output_all/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 21:44:25,055 >> tokenizer config file saved in output_all/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 21:44:25,058 >> Special tokens file saved in output_all/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 21:44:25,064 >> Copy vocab file to output_all/checkpoint-40000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 21:44:54,935 >> Deleting older checkpoint [output_all/checkpoint-25000] due to args.save_total_limit\n",
            "{'loss': 0.0074, 'learning_rate': 2.1678321678321677e-05, 'epoch': 283.22}\n",
            "{'loss': 0.0074, 'learning_rate': 2.132867132867133e-05, 'epoch': 286.71}\n",
            "{'loss': 0.0074, 'learning_rate': 2.097902097902098e-05, 'epoch': 290.21}\n",
            "{'loss': 0.0072, 'learning_rate': 2.062937062937063e-05, 'epoch': 293.71}\n",
            "{'loss': 0.0071, 'learning_rate': 2.027972027972028e-05, 'epoch': 297.2}\n",
            "{'loss': 0.007, 'learning_rate': 1.993006993006993e-05, 'epoch': 300.7}\n",
            "{'loss': 0.0067, 'learning_rate': 1.958041958041958e-05, 'epoch': 304.2}\n",
            "{'loss': 0.0071, 'learning_rate': 1.923076923076923e-05, 'epoch': 307.69}\n",
            "{'loss': 0.0066, 'learning_rate': 1.888111888111888e-05, 'epoch': 311.19}\n",
            "{'loss': 0.0068, 'learning_rate': 1.8531468531468532e-05, 'epoch': 314.69}\n",
            " 63% 45000/71500 [11:10:56<6:32:52,  1.12it/s][INFO|trainer.py:1558] 2021-09-07 22:58:48,640 >> Saving model checkpoint to output_all/checkpoint-45000\n",
            "[INFO|configuration_utils.py:314] 2021-09-07 22:58:48,644 >> Configuration saved in output_all/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-07 22:58:53,212 >> Model weights saved in output_all/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-07 22:58:54,470 >> tokenizer config file saved in output_all/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-07 22:58:54,473 >> Special tokens file saved in output_all/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-07 22:58:54,480 >> Copy vocab file to output_all/checkpoint-45000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-07 22:59:20,733 >> Deleting older checkpoint [output_all/checkpoint-30000] due to args.save_total_limit\n",
            "{'loss': 0.0069, 'learning_rate': 1.8181818181818182e-05, 'epoch': 318.18}\n",
            "{'loss': 0.0067, 'learning_rate': 1.7832167832167836e-05, 'epoch': 321.68}\n",
            "{'loss': 0.0065, 'learning_rate': 1.7482517482517483e-05, 'epoch': 325.17}\n",
            "{'loss': 0.0066, 'learning_rate': 1.7132867132867133e-05, 'epoch': 328.67}\n",
            "{'loss': 0.0068, 'learning_rate': 1.6783216783216786e-05, 'epoch': 332.17}\n",
            "{'loss': 0.0064, 'learning_rate': 1.6433566433566433e-05, 'epoch': 335.66}\n",
            "{'loss': 0.0065, 'learning_rate': 1.6083916083916083e-05, 'epoch': 339.16}\n",
            "{'loss': 0.0063, 'learning_rate': 1.5734265734265734e-05, 'epoch': 342.66}\n",
            "{'loss': 0.0064, 'learning_rate': 1.5384615384615387e-05, 'epoch': 346.15}\n",
            "{'loss': 0.0062, 'learning_rate': 1.5034965034965034e-05, 'epoch': 349.65}\n",
            " 70% 50000/71500 [12:25:20<5:19:08,  1.12it/s][INFO|trainer.py:1558] 2021-09-08 00:13:12,897 >> Saving model checkpoint to output_all/checkpoint-50000\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 00:13:12,901 >> Configuration saved in output_all/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 00:13:17,439 >> Model weights saved in output_all/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 00:13:17,558 >> tokenizer config file saved in output_all/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 00:13:17,561 >> Special tokens file saved in output_all/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 00:13:17,568 >> Copy vocab file to output_all/checkpoint-50000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-08 00:13:51,202 >> Deleting older checkpoint [output_all/checkpoint-35000] due to args.save_total_limit\n",
            "{'loss': 0.0065, 'learning_rate': 1.4685314685314686e-05, 'epoch': 353.15}\n",
            "{'loss': 0.0062, 'learning_rate': 1.4335664335664336e-05, 'epoch': 356.64}\n",
            "{'loss': 0.0061, 'learning_rate': 1.3986013986013988e-05, 'epoch': 360.14}\n",
            "{'loss': 0.0063, 'learning_rate': 1.3636363636363637e-05, 'epoch': 363.64}\n",
            "{'loss': 0.0061, 'learning_rate': 1.3286713286713287e-05, 'epoch': 367.13}\n",
            "{'loss': 0.0061, 'learning_rate': 1.2937062937062939e-05, 'epoch': 370.63}\n",
            "{'loss': 0.006, 'learning_rate': 1.2587412587412589e-05, 'epoch': 374.13}\n",
            "{'loss': 0.0061, 'learning_rate': 1.2237762237762239e-05, 'epoch': 377.62}\n",
            "{'loss': 0.0059, 'learning_rate': 1.188811188811189e-05, 'epoch': 381.12}\n",
            "{'loss': 0.0063, 'learning_rate': 1.153846153846154e-05, 'epoch': 384.62}\n",
            " 77% 55000/71500 [13:39:51<4:03:07,  1.13it/s][INFO|trainer.py:1558] 2021-09-08 01:27:43,886 >> Saving model checkpoint to output_all/checkpoint-55000\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 01:27:43,890 >> Configuration saved in output_all/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 01:27:48,479 >> Model weights saved in output_all/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 01:27:48,483 >> tokenizer config file saved in output_all/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 01:27:50,189 >> Special tokens file saved in output_all/checkpoint-55000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 01:27:50,196 >> Copy vocab file to output_all/checkpoint-55000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-08 01:28:18,070 >> Deleting older checkpoint [output_all/checkpoint-40000] due to args.save_total_limit\n",
            "{'loss': 0.0061, 'learning_rate': 1.118881118881119e-05, 'epoch': 388.11}\n",
            "{'loss': 0.0059, 'learning_rate': 1.0839160839160838e-05, 'epoch': 391.61}\n",
            "{'loss': 0.006, 'learning_rate': 1.048951048951049e-05, 'epoch': 395.1}\n",
            "{'loss': 0.0058, 'learning_rate': 1.013986013986014e-05, 'epoch': 398.6}\n",
            "{'loss': 0.0058, 'learning_rate': 9.79020979020979e-06, 'epoch': 402.1}\n",
            "{'loss': 0.0059, 'learning_rate': 9.44055944055944e-06, 'epoch': 405.59}\n",
            "{'loss': 0.0059, 'learning_rate': 9.090909090909091e-06, 'epoch': 409.09}\n",
            "{'loss': 0.0057, 'learning_rate': 8.741258741258741e-06, 'epoch': 412.59}\n",
            "{'loss': 0.0058, 'learning_rate': 8.391608391608393e-06, 'epoch': 416.08}\n",
            "{'loss': 0.0056, 'learning_rate': 8.041958041958042e-06, 'epoch': 419.58}\n",
            " 84% 60000/71500 [14:54:21<2:50:01,  1.13it/s][INFO|trainer.py:1558] 2021-09-08 02:42:13,596 >> Saving model checkpoint to output_all/checkpoint-60000\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 02:42:13,602 >> Configuration saved in output_all/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 02:42:18,134 >> Model weights saved in output_all/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 02:42:18,139 >> tokenizer config file saved in output_all/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 02:42:18,142 >> Special tokens file saved in output_all/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 02:42:18,149 >> Copy vocab file to output_all/checkpoint-60000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-08 02:42:45,427 >> Deleting older checkpoint [output_all/checkpoint-45000] due to args.save_total_limit\n",
            "{'loss': 0.0057, 'learning_rate': 7.692307692307694e-06, 'epoch': 423.08}\n",
            "{'loss': 0.0056, 'learning_rate': 7.342657342657343e-06, 'epoch': 426.57}\n",
            "{'loss': 0.0058, 'learning_rate': 6.993006993006994e-06, 'epoch': 430.07}\n",
            "{'loss': 0.0055, 'learning_rate': 6.643356643356643e-06, 'epoch': 433.57}\n",
            "{'loss': 0.0057, 'learning_rate': 6.2937062937062944e-06, 'epoch': 437.06}\n",
            "{'loss': 0.0055, 'learning_rate': 5.944055944055945e-06, 'epoch': 440.56}\n",
            "{'loss': 0.0055, 'learning_rate': 5.594405594405595e-06, 'epoch': 444.06}\n",
            "{'loss': 0.0055, 'learning_rate': 5.244755244755245e-06, 'epoch': 447.55}\n",
            "{'loss': 0.0055, 'learning_rate': 4.895104895104895e-06, 'epoch': 451.05}\n",
            "{'loss': 0.0054, 'learning_rate': 4.5454545454545455e-06, 'epoch': 454.55}\n",
            " 91% 65000/71500 [16:08:48<1:36:03,  1.13it/s][INFO|trainer.py:1558] 2021-09-08 03:56:41,095 >> Saving model checkpoint to output_all/checkpoint-65000\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 03:56:41,100 >> Configuration saved in output_all/checkpoint-65000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 03:56:45,842 >> Model weights saved in output_all/checkpoint-65000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 03:56:45,846 >> tokenizer config file saved in output_all/checkpoint-65000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 03:56:45,848 >> Special tokens file saved in output_all/checkpoint-65000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 03:56:45,855 >> Copy vocab file to output_all/checkpoint-65000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-08 03:57:16,624 >> Deleting older checkpoint [output_all/checkpoint-50000] due to args.save_total_limit\n",
            "{'loss': 0.0054, 'learning_rate': 4.195804195804197e-06, 'epoch': 458.04}\n",
            "{'loss': 0.0053, 'learning_rate': 3.846153846153847e-06, 'epoch': 461.54}\n",
            "{'loss': 0.0054, 'learning_rate': 3.496503496503497e-06, 'epoch': 465.03}\n",
            "{'loss': 0.0054, 'learning_rate': 3.1468531468531472e-06, 'epoch': 468.53}\n",
            "{'loss': 0.0054, 'learning_rate': 2.7972027972027974e-06, 'epoch': 472.03}\n",
            "{'loss': 0.0053, 'learning_rate': 2.4475524475524477e-06, 'epoch': 475.52}\n",
            "{'loss': 0.0053, 'learning_rate': 2.0979020979020983e-06, 'epoch': 479.02}\n",
            "{'loss': 0.0054, 'learning_rate': 1.7482517482517485e-06, 'epoch': 482.52}\n",
            "{'loss': 0.0053, 'learning_rate': 1.3986013986013987e-06, 'epoch': 486.01}\n",
            "{'loss': 0.0053, 'learning_rate': 1.0489510489510491e-06, 'epoch': 489.51}\n",
            " 98% 70000/71500 [17:23:19<22:11,  1.13it/s][INFO|trainer.py:1558] 2021-09-08 05:11:11,518 >> Saving model checkpoint to output_all/checkpoint-70000\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 05:11:11,523 >> Configuration saved in output_all/checkpoint-70000/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 05:11:16,310 >> Model weights saved in output_all/checkpoint-70000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 05:11:16,315 >> tokenizer config file saved in output_all/checkpoint-70000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 05:11:16,317 >> Special tokens file saved in output_all/checkpoint-70000/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 05:11:16,323 >> Copy vocab file to output_all/checkpoint-70000/spiece.model\n",
            "[INFO|trainer.py:1625] 2021-09-08 05:11:48,730 >> Deleting older checkpoint [output_all/checkpoint-55000] due to args.save_total_limit\n",
            "{'loss': 0.0052, 'learning_rate': 6.993006993006994e-07, 'epoch': 493.01}\n",
            "{'loss': 0.0052, 'learning_rate': 3.496503496503497e-07, 'epoch': 496.5}\n",
            "{'loss': 0.0054, 'learning_rate': 0.0, 'epoch': 500.0}\n",
            "100% 71500/71500 [17:46:07<00:00,  1.28it/s][INFO|trainer.py:1129] 2021-09-08 05:34:00,285 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 63968.1545, 'train_samples_per_second': 1.118, 'epoch': 500.0}\n",
            "100% 71500/71500 [17:46:07<00:00,  1.12it/s]\n",
            "[INFO|trainer.py:1558] 2021-09-08 05:34:00,657 >> Saving model checkpoint to output_all/\n",
            "[INFO|configuration_utils.py:314] 2021-09-08 05:34:00,660 >> Configuration saved in output_all/config.json\n",
            "[INFO|modeling_utils.py:837] 2021-09-08 05:34:05,161 >> Model weights saved in output_all/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1896] 2021-09-08 05:34:05,165 >> tokenizer config file saved in output_all/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1902] 2021-09-08 05:34:05,168 >> Special tokens file saved in output_all/special_tokens_map.json\n",
            "[INFO|tokenization_t5.py:287] 2021-09-08 05:34:05,174 >> Copy vocab file to output_all/spiece.model\n",
            "[INFO|trainer_pt_utils.py:656] 2021-09-08 05:34:05,178 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,295 >>   epoch                      =      500.0\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   init_mem_cpu_alloc_delta   =        1MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   init_mem_gpu_alloc_delta   =     1307MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   init_mem_gpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_mem_cpu_peaked_delta =      126MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_mem_gpu_alloc_delta  =     3849MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_mem_gpu_peaked_delta =     4657MB\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_runtime              = 63968.1545\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_samples              =        570\n",
            "[INFO|trainer_pt_utils.py:661] 2021-09-08 05:34:05,296 >>   train_samples_per_second   =      1.118\n",
            "CPU times: user 5min 57s, sys: 1min, total: 6min 58s\n",
            "Wall time: 17h 48min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtu4AvG5Ik-p"
      },
      "source": [
        "# from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "# # トークナイザーとモデルの準備\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"output/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xllJ0rN9Fuyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f489ae94-0380-480b-ad7c-faa1e0cee487"
      },
      "source": [
        "# もちろんだが、Autoでも直指定でも同じ結果にはなっている\n",
        "# https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
        "from transformers import T5Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# GPU判定\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"output_all/\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(32000, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbp9X4mv2yJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt93bvvwv22j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_K7EOdMPVZ",
        "outputId": "6d0fe409-adcb-4d06-b8cb-94ae4992747e"
      },
      "source": [
        "# 推論\n",
        "# https://huggingface.co/blog/how-to-generate\n",
        "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate\n",
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のジャブ これがデビュー戦緊張はしているがどんどん前に出て行きたい\n",
            "左のジャブ これがここから突破口を音\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがここから突破口を音\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがデビュー戦。\n",
            "左のジャブ これがデビュー戦緊張はしているがどんどん前に出て行きたい\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmB5AQ9pQoyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9d64fe-38fb-4442-cb27-5be15b07abe8"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のフック これがデビュー戦。\n",
            "左のフック ここはボディに打ち込んでいきます。\n",
            "左のフック ここから大金星あげてくるのか\n",
            "左のフック これがデビュー戦。\n",
            "左のフック ここはボディに打ち込んでいます\n",
            "左のフック アッパーからersonのパンチがnimal_artを捉えるosition_ocation。\n",
            "左のフック 第2ラウンドはersonがダウンを覚えました\n",
            "左のフック アッパーからersonのパンチへと変わっていきます\n",
            "左のフック ここはちょっと距離を詰めていこうというこのerson。\n",
            "左のフック ここはちょっと硬いですね\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-VUAXfoEE9K",
        "outputId": "8ba242bd-1a33-4346-c0b9-a8f65e3a5bb3"
      },
      "source": [
        "input = tokenizer.encode(\"ボディ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n",
            "ボディ 右の誰かぐらっときた\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zPc_EPFEJi_",
        "outputId": "b7a3d26a-9521-4499-df1f-3254e4104394"
      },
      "source": [
        "input = tokenizer.encode(\"右のストレート\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "右のストレート これがデビュー戦。\n",
            "右のストレート ここはガード固めて。\n",
            "右のストレート これがデビュー戦。\n",
            "右のストレート これがデビュー戦。\n",
            "右のストレート ここはちょっと気味で右が入りました。\n",
            "右のストレート これがデビュー戦。\n",
            "右のストレート これがデビュー戦。\n",
            "右のストレート これがデビュー戦。\n",
            "右のストレート これがデビュー戦緊張はしているがどんどん前に出て行きたい\n",
            "右のストレート これがデビュー戦。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHIHS0H7EMe1",
        "outputId": "49e54ef2-a096-4710-a34d-cb84c54fc928"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ガードの上から eaponを打っていくのは守りerson。\n",
            "ガードの上から こちらもデビュー戦portで一花咲かせるか藤川遥29歳。\n",
            "ガードの上から どんどん落ち込んでいている\n",
            "ガードの上から eaponを打って行きます\n",
            "ガードの上から どんどん落ち込んでいている\n",
            "ガードの上から eaponを打っていくのは守りerson。\n",
            "ガードの上から 森青葉。\n",
            "ガードの上から eaponを打って行きますが。\n",
            "ガードの上から どんどん落ち込んでいている\n",
            "ガードの上から eaponを打っていくのは平井大\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4H6oqsAKrlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxas9R2LKrt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiAl1wGnF6kf"
      },
      "source": [
        "tokenizerの中身を確認  \n",
        "\\<s\\>の意味合いを表示  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HYhyTw7Ea7-",
        "outputId": "8daf467e-f137-4f24-960e-71f20db23a62"
      },
      "source": [
        "# model.generateの結果はtokenizerのindexベクトル\n",
        "output[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    9,  5682,    10, 12276,     2,     9,     0,    20,  2115,    18,\n",
              "         5456,  2199,     7,    80,     0,    10,   819,     8,     2,     2,\n",
              "            2,     2,     2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym78sIbQFIMn",
        "outputId": "0e4f0dcc-3a88-4653-d17f-f3ce7328ac4b"
      },
      "source": [
        "# 記号の意味\n",
        "tokenizer.all_special_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '</s>', '<unk>', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNCWHrgWG9bY",
        "outputId": "3574002a-6b91-4aab-b276-e7e660b76436"
      },
      "source": [
        "# 記号に対応するindex\n",
        "tokenizer.all_special_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 0, 5, 3, 4, 6]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCg-F5fAFlaa"
      },
      "source": [
        "{\"bos_token\": \"\\<s\\>\", \"eos_token\": \"\\</s\\>\", \"unk_token\": \"<unk>\", \"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\", \"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\"}  \n",
        "bos_token: 文の先頭（Begin of sequence token）  \n",
        "eos_token: 文のおしり（End of Sequence token）  \n",
        "unk_token: IDに変換できない文字（Unknown token）  \n",
        "sep_token: 文と文を区切り目（The separator token）  \n",
        "pad_token: パッディング（The token used for padding）  \n",
        "cls_token: 分類用（cls_token）  \n",
        "mask_token: マスク（The token used for masking values）  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI5xE9Myg0WR"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode  \n",
        "sequences : torch.Tensorの配列を入力値として指定  \n",
        "  トークン化された入力IDのリスト  \n",
        "skip_special_tokens : デコード時に特殊なトークンを削除するかどうか(eos_tokenとかを消す)(デフォルト:False)  \n",
        "clean_up_tokenization_spaces : トークン化スペースをクリーンアップするかどうか(デフォルト:True)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqZhkicXiBP8"
      },
      "source": [
        "* https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode  \n",
        "\n",
        "text (str, List[str] or List[int]) – 入力文字列  \n",
        "\n",
        "text_pair (str, List[str] or List[int], optional) – ペアとなるもう一つを入力する場合のオプション  \n",
        "\n",
        "add_special_tokens (bool, optional, defaults to True) – 上記で定義していない特別なトークンをモデルに適用するか\n",
        "\n",
        "padding (bool, str or PaddingStrategy, optional, defaults to False) –パディングして入力シーケンスを揃える場合  \n",
        "\n",
        "truncation (bool, str or TruncationStrategy, optional, defaults to False) –逆に長過ぎる場合に、一定の長さに揃える場合\n",
        "\n",
        "max_length (int, optional) –トランケーション・パディングで使用するオプション\n",
        "\n",
        "stride (int, optional, defaults to 0) – max_lengthで切り捨てられたのを調整する  \n",
        "\n",
        "is_split_into_words (bool, optional, defaults to False) – 単語分割が既にされている場合True\n",
        "\n",
        "pad_to_multiple_of (int, optional) – 指定された値の倍数になるようにシーケンスをパッドする  \n",
        "\n",
        "return_tensors (str or TensorType, optional) – python整数のリストの代わりにテンソルを返す  \n",
        "'tf': Return TensorFlow tf.constant objects.  \n",
        "'pt': Return PyTorch torch.Tensor objects.  \n",
        "'np': Return Numpy np.ndarray objects.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZXgdwCNECfn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paoQ0wFiNsyp"
      },
      "source": [
        "* https://huggingface.co/blog/how-to-generate  \n",
        "* https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.generation_utils.GenerationMixin.generate  \n",
        "* https://note.com/npaka/n/n5d296d8ae26d  \n",
        "* https://note.com/npaka/n/n96dde45fdf8d  \n",
        "\n",
        "### GPT2LMHeadModel.generateのオプションを確認  \n",
        "\n",
        "input_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) – 入力シーケンス  \n",
        "\n",
        "max_length (int, optional, defaults to model.config.max_length) – 生成されるシーケンスの最大長を指定（学習に使用した256の長さが良さそう）\n",
        "\n",
        "max_new_tokens (int, optional, defaults to None) – 現在のトークン数に関係なく、生成されるシーケンスの最大長を指定\n",
        "\n",
        "min_length (int, optional, defaults to 10) – 生成されるシーケンスの最小の長さ\n",
        "\n",
        "do_sample (bool, optional, defaults to False) – 単語予測にサンプリングを入れてランダム性を導入する。（デフォルトは greedy decoding の生成）  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d  \n",
        "\n",
        "early_stopping (bool, optional, defaults to False) – ビーム探索で、num_beams個の文が生成された時点で、ビーム探索を終了するかどうか  \n",
        "\n",
        "num_beams (int, optional, defaults to 1) – ビームサーチを行うビームの数。1はビームサーチを行わないことを意味します。  \n",
        "\n",
        "temperature (float, optional, defaults to 1.0) – 次のトークンの確率をモジュール化するために使用される値です。温度（デフォルト1、推奨0.7〜1.0）ボルツマン分布のパラメータ。小さい値ではランダムな補完が減り，0では決まりきった繰り返しの文になる。大きい値ではより様々な補完がされる。  \n",
        "\n",
        "top_k (int, optional, defaults to 50) – top-k-filteringのために保持する最高確率の語彙トークンの数です。確率が大きめな候補からサンプリングしてランダム性を導入する際の候補を何個にするか。40が一般的に良い値  \n",
        "\n",
        "top_p (float, optional, defaults to 1.0) – 生成テキストを累積確率に制限 (0で制限なし) float < 1に設定すると、top_p以上の確率を持つ最も確率の高いトークンのみが生成のために保持されます。  \n",
        "https://zenn.dev/hellorusk/articles/1c0bef15057b1d#top-p-(nucleus)-sampling  \n",
        "\n",
        "repetition_penalty (float, optional, defaults to 1.0) – 反復ペナルティのパラメータです。1.0はペナルティなし。すでに生成された単語や文脈に属する単語にペナルティを与えるために使用することができます。反復防止にはかなり効果的ですが、異なるモデルやユースケースには非常に敏感なようで、議論がある。  \n",
        "\n",
        "pad_token_id (int, optional) – PADトークンを指定\n",
        "\n",
        "bos_token_id (int, optional) – bosトークンを指定\n",
        "\n",
        "eos_token_id (int, optional) – eosトークンを指定\n",
        "\n",
        "length_penalty (float, optional, defaults to 1.0) – 長さに対する指数関数的なペナルティ。1.0はペナルティがないことを意味します。1.0未満の値を設定すると、モデルは短い配列を生成するようになり、1.0以上の値を設定すると、モデルは長い配列を生成するようになります。\n",
        "\n",
        "no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、そのサイズのngramはすべて一度しか発生しません。最も一般的な n-grams ペナルティは、すでに見た n-gramsを作る可能性のある次の単語の確率を 0 に手動で設定することで、n-gramsが 2 回出現しないようにするものです。  \n",
        "\n",
        "encoder_no_repeat_ngram_size (int, optional, defaults to 0) – int > 0に設定すると、encoder_input_idsに出現したそのサイズのすべてのngramは、decoder_input_idsには出現しません。\n",
        "\n",
        "bad_words_ids (List[List[int]], optional) – 生成してはいけないトークンのidのリスト。トークンのIDは以下で確認  \n",
        "tokenizer(bad_word, add_prefix_space=True).input_ids  \n",
        "\n",
        "num_return_sequences (int, optional, defaults to 1) – バッチ内の各要素について、独立して計算された戻り値の配列の数(返却される結果の数)。返されるべき最高得点のBeamの数を設定します。ただし、num_return_sequences <= num_beams とします。\n",
        "\n",
        "max_time (float, optional, defaults to None) – 計算の実行を許可する最大時間を秒単位で指定します。割り当てられた時間が経過しても、生成は現在のパスを終了します。  \n",
        "\n",
        "attention_mask (torch.LongTensor of shape (batch_size, sequence_length), optional) – パディングされたトークンのインデックスに対してアテンションを行わないようにするためのマスクです。マスクの値は [0, 1] で、マスクされていないトークンには 1、マスクされたトークンには 0 です。提供されていない場合は、パッドトークンをマスクするinput_idsと同じ形のテンソルがデフォルトになります。attentionで予測するための配列を作るので、マスクすると候補に出なくなる。入力シーケンスに対して同じ長さで指定する\n",
        "\n",
        "decoder_start_token_id (int, optional) – エンコーダ・デコーダモデルがbosとは異なるトークンでデコードを開始した場合、そのトークンのid。\n",
        "\n",
        "use_cache – (bool, optional, defaults to True): 過去の最後のキー／バリューの注目度（モデルに該当する場合）を利用して、デコーディングを高速化するかどうか。  \n",
        "\n",
        "num_beam_groups (int, optional, defaults to 1) – num_beamsを分割するグループの数（ビームの異なるグループ間の多様性を確保するため）。\n",
        "\n",
        "diversity_penalty (float, optional, defaults to 0.0) – この値は、ある時点で他のグループのビームと同じトークンを生成した場合、ビームのスコアから差し引かれます。なお、ダイバーシティペナルティは、グループビーム検索が有効な場合にのみ有効です。  \n",
        "\n",
        "prefix_allowed_tokens_fn – (Callable[[int, torch.Tensor], List[int]], optional):提供された場合、この関数は、各ステップで許可されたトークンのみにビーム検索を制約します。提供されない場合、制約は適用されません。この関数は2つの引数をとります：バッチID batch_id と input_id です。これは、バッチID batch_idと以前に生成されたトークンinput_idsを条件として、次の生成ステップで許可されたトークンのリストを返さなければなりません。この引数は、「自己回帰的実体検索」で説明されているように、接頭辞を条件とした制約付き生成に役立ちます。  \n",
        "\n",
        "output_attentions (bool, optional, defaults to False) – すべてのアテンションレイヤーのアテンションテンソルを返すかどうか。  \n",
        "\n",
        "output_hidden_states (bool, optional, defaults to False) – すべてのレイヤーの隠れた状態を返すかどうか。  \n",
        "\n",
        "output_scores (bool, optional, defaults to False) – 予測スコアを返すかどうか。  \n",
        "\n",
        "return_dict_in_generate (bool, optional, defaults to False) – 単なるタプルではなく、ModelOutputを返すかどうか。  \n",
        "\n",
        "forced_bos_token_id (int, optional) – decoder_start_token_idの後に、最初に生成されるトークンとして強制的に使用するトークンのidです。mBARTのような多言語モデルで、最初に生成されるトークンがターゲット言語のトークンである必要がある場合に便利です。（一番最初に生成される単語を指定してしまう。）  \n",
        "\n",
        "forced_eos_token_id (int, optional) – max_lengthに達したときに、最後に生成されたトークンとして強制的に使用するトークンのidです。(最後をわかりやすくして、途中で切られたのを知らせる)  \n",
        "\n",
        "remove_invalid_values (bool, optional) – 生成方法がクラッシュするのを防ぐために、モデルの可能性のあるnanとinfの出力を削除するかどうか。remove_invalid_valuesを使うと生成が遅くなることに注意してください。  \n",
        "\n",
        "synced_gpus (bool, optional, defaults to False) – max_lengthまでwhileループを続けて実行するかどうか  \n",
        "\n",
        "最新の研究により、単純な Beam Search や Greedy Search が同じ単語列の繰り返しを発生させてしまうのは、decoding に問題があるのではなくモデルの学習自体に問題があるとされています。また、Top-K や Top-p のようなサンプリングによる decoding であってもそうした単語列の繰り返しは発生しうるそうです。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIarpN7BN1HJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisPBDCsN1OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b65c7e-9c44-4d5a-f33a-b02af2275e12"
      },
      "source": [
        "input = tokenizer.encode(\"左のフック\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のフック</s> ここはちょっと<unk> 気味で右が入りました。 この<unk> の右というのはちょっと速いですね。 俺を仕留めてていきたいとイメージでしょうか</s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンドは残り10秒です。 さあ距離を詰めてきた、この第3ラウンド。</s> </s>\n",
            "左のフック</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 左のジャブあたりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはちょっと<unk> 気味で右が入りました。 この<unk> の右というのはちょっと速いですね。ちょっと早いですね。 俺を仕留めてていきたいとイメージでしょうか</s>\n",
            "左のフック</s> ここはちょっと<unk> 気味で右が入りました。 第2ラウンドは少し静かな戦いになりましたが。 第2ラウンド終了です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいきます。 ストレートネック ただここはバックステップを使って未来はClothing避けていきます</s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終了です。 ここまで激しい打ち合いになっている両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終了です。 ここまで激しい打ち合いになっている両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはちょっと<unk> 気味で右が入りました。 この<unk> の右というのはちょっと速いですね。 ちょっと早いですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のフック</s> ここはちょっと<unk> 気味で右が入りました。 第2ラウンドは少し静かな試合になりましたが。 第3ラウンドです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS8z5iRxN1Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b526443-51b2-45d4-de30-ac1b570d9e26"
      },
      "source": [
        "input = tokenizer.encode(\"ラッシュラッシュ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ラッシュラッシュ</s> これありますからね。 ガード上げて距離を詰めていく<unk> erson。 右のジャブ。 そっからこの次の展開を考えているこの<unk> octrine_<unk> ethod_<unk> therなんですがどうですか、これ距離感というの<unk> itle_<unk> therどう思いますか?</s>\n",
            "ラッシュラッシュ</s> すごいですね。 はい。 ここはジャブを打っていって、残り2ラウンド。 ここは最終ラウンドです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> すごいですね。 はい。 ここは右のジャブ。 このジャブを打っていって、最後のラウンドに持って行きました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> ここはジャブを打っていけばきた子は<unk> _<unk> umberでも食った気しないとは右の<unk> eaponと左の服が溶けて連絡あったうちらになっている第5ラウンド最後打ち合いになりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> これありますからね。 ガード上げて距離を詰めていく<unk> erson。 右のジャブ。 これやはり<unk> ersonの方が尻上がりに来る感じがしますね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> ここは両者。 お互いの気持ちがぶつかります。 <unk> nimal_<unk> artが交錯します。右、右をいれてくる。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> ここは両者。 お互いの気持ちがぶつかります。 <unk> nimal_<unk> artが交錯します。 右、右をいれてくる。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> すごい期待できませんね はい楽しみが広がりますね さあ残り10秒です。 未来だいきです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> それが終わりました。 先ほどのシーン。 右の<unk> eaponが<unk> nimal_<unk> artをとらえました。 今のシーンです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ラッシュラッシュ</s> ここは両者。 お互いの気持ちがぶつかります。 <unk> nimal_<unk> artが交錯します。 右、右をいれてくる。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkkVVbvvsWG",
        "outputId": "e4057793-ae34-4f18-e721-1bf16ae80a63"
      },
      "source": [
        "input = tokenizer.encode(\"ダウン\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ダウン</s> <unk> ersonです。 ここは右のフック。 ダウンですね。 こういうの長い左のジャブを伸ばしていく<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> そこまでは<unk> osition_<unk> ocationがどんどん中に入っていきましたが。。。 そこの表情ですよ。 これもうこれなんか狙ってる感じの<unk> nimal_<unk> artしますよね</s>\n",
            "ダウン</s> ここは美味しいですね。 いろいろ残りも入れていきました。 本人もそれを意識しだしてるんじゃないすかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> そこまでは圧倒していたんですが最後の。。。 ここはバックステップ少しダウンを奪いましたが。。。 ただここはバックステップも残り30秒。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> nimal_<unk> artがこれは試合をひっくり返したと言ってもいいでしょうかあのまま走り流れ込んでいたらあるいは<unk> ersonだったかもしれませんが見事にその力でもってきました。</s> </s> </s> </s> </s>\n",
            "ダウン</s> ここは美味しいですね。 いろいろ残りも入れていきました。 本人もそれを意識しだしてるんじゃないすかね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> nimal_<unk> artが曲がりましたが。。。。 聞いたと見ればパンチを集める力 そしてパンチを集める体が浮きました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> これがデビュー戦。 アマチュア戦績57.43勝14敗実績は十分にきゅうレビュー。 今日はプロの舞台でどのような戦いぶりを見せるか。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> nimal_<unk> artが曲がりましたが。。。。。 聞いたと見ればパンチを集める力 素晴らしいものがありました 第2ラウンドでダウンを奪われた映画ですが。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ダウン</s> <unk> nimal_<unk> artが曲がりましたが。。。 聞いたと見ればパンチを集める力 今日はしっかり周りを取って。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUggffnvsdo",
        "outputId": "840057bc-811c-4213-d956-0c9df8cdce73"
      },
      "source": [
        "input = tokenizer.encode(\"あーとここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "あーとここで倒れた</s> かなり体力が限界かと思われたんですが<unk> erson声を出しながら前に出ます。 試合は序盤からどんどん行っている</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> ま復帰後はま復帰後初勝利およそ4年ぶりの勝利ということになりました 一方の<unk> ature_Colorコーナーの永遠ですが<unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 担架で運ばれて行きました。 まあしかし<unk> ersonにとっては前回が復帰後初勝利そして日本ランクも手にした一戦になりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 担架で運ばれて行きました。 これで当たればというところですが第5ラウンドが終了しましたのファイナルラウンド1ラウンドのみとなります</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> すでに第2ラウンドが終了しています。 まずは<unk> imeのパンチラ<unk> nimal_<unk> artを捉えるシーン。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> すでに第2ラウンドが終了しています。 まずは<unk> imeです。 まあ幼い<unk> erson<unk> osition_<unk> ocationですから甘いですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> やはり前に出ようとすると右の<unk> eaponが<unk> nimal_<unk> artを捉えました しっかり中に踏み込んで。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 担架で運ばれるという形になりました。 まもなく第2ラウンド終了のゴングですか。 第2ラウンドは<unk> ersonのパンチな<unk> nimal_<unk> artをクリーンヒットしました</s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> 担架で運ばれるという形になりました。 これアプリタイミングがいいんですかね? はい あの意味<unk> osition_<unk> ocationが前に入ろうとしたところにみんな入りましたね。</s> </s> </s> </s> </s> </s> </s>\n",
            "あーとここで倒れた</s> もうひと勝負すれどこの<unk> nimal_<unk> artをとらえました やはりあの勝ちを重ねるにつれに行くにつれだんだんその<unk> portの楽しさというよりは10月みたいのが増えていくてもよくありますよね</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah8NV8oNwuO8",
        "outputId": "5c5c5953-70ef-4ffe-ffca-35c44d5e6215"
      },
      "source": [
        "input = tokenizer.encode(\"ここで倒れた\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.85, temperature=0.7, min_length=64, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ここで倒れた</s> が、まだ大丈夫だとそういう機会にしたいと言っていましたからその寮に一礼をして。 まずは<unk> ature_Colorコーナーの<unk> ersonです。 まず入場してきたのが<unk> ature_Colorコーナー<unk> erson35歳です。 まず入場してきたのは<unk> ature_Colorコーナー<unk> erson35歳です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> と思ったら今度は右の<unk> eaponが<unk> nimal_<unk> artをとらえました なかなかない中に入っていけるかどうか岩崎圭介は前回よりもすごく<unk> nimal_<unk> artを使ってない入らせない<unk> portしてますね そうですね あのやっぱりお互いにクリーンヒットがないんでここで栗拾い形になりましたね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> ということですが。 まみてる以上に対峙するとはいあれ思った以上に遠いなって物が溜まりにくいのかなと思いますね 実際、<unk> roduct_<unk> therの<unk> port見ててもそのスピードと手数の多さにちょっと真ん中が先週ママ対応しきれてないなーっていうのは思いましたほど</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> ということですが。 まこの試合は1年4ヶ月ぶりの復帰戦のことになります。 リハビリ期間はどうしようかと。 やっぱりもう1年4ヶ月ぶりなんでね、 精神的にもやっぱりかなりダメージはあるでしょうね。 ただ家行っても倒れない家行っても倒れない家行ってもですね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> が、再び世界への道を目指すというこの試合です。 試合は後半戦に入っていきます。 コーナーウイルス<unk> ame_<unk> therもっと前に出ると友人<unk> からの声を聞いてどんどん前に出ていけるかどうか理系かお出かけなかったらどこへ持って行けという声を聞いてどんどん前に出て行ったうるうるした畳ダウンで立ち上がることができるとハンマーハンマーで立ち上がって再開2度のダウンを奪った上でメールしとくから見たことなかったよ止めましたがやはり第3ラウンド割合が終わりました。</s>\n",
            "ここで倒れた</s> が、もう<unk> requency直さないといけないそうですねそうですね また動き出されたなという感じでしょうか そうですね <unk> erson<unk> itle_<unk> therのあのもう<unk> requency直さないといけないという風に周りが話していたので、やっぱりそれを掴む形で試合動かしたいですね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> が、<unk> roduct_<unk> therがどんどんパンチを出してプレッシャーを与えていきます。 プレッシャーを与えているのは<unk> roduct_<unk> therです。 ただ、<unk> ersonもパンチには威力があります。 <unk> ersonにはカウンターの切れ味取りところではここまで続きが優勢に進めても一撃で流れを変えるそんな力がありますよね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> が、もう<unk> requency直さないといけないそうですねそうですね また動き出されたとしたらどういう風な対策をとるのがいいんですか? そうですね もう立てないと思います。 このラウンドは<unk> ersonがダウンを覚えました。 ただ、<unk> ersonはこの試合、気合十分でやってまいりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> が、再び世界を目指すと言う。 その前に<unk> octrine_<unk> ethod_<unk> therを目指しましたが。 前に出た時にパンチが当たる感じがしました。 ただ、少しパンチをもらっているシーンもありました。 試合は後半戦に入っていきます。 第5ラウンドです。 先ほど、<unk> ersonがダウンしました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ここで倒れた</s> が、<unk> roduct_<unk> therがどんどんパンチを出してプレッシャーを与えていきます。 まず中に入っていこうという<unk> ersonです。 右のジャブ。 まずボディに打ち込んでいきます。 そして中に入ろうという<unk> ersonです。 左のジャブ。 まず中に入ってくる<unk> ersonに対して右のジャブそれからフットワークを使って距離を取っていきたいぐらいです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXts53TaxMU1",
        "outputId": "d431df2e-2ad8-4abc-df14-9f6a4b6a7b80"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上から連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 リング中央<unk> nimal_<unk> artの後ろには<unk> imeが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ガードの上からでもしっかり手数を出して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 リング中央には秘密ですがそれは<unk> _<unk> therで一緒にしてClothingが飛んできました</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 リング中央<unk> nimal_<unk> artの横にあるのは<unk> _<unk> ersonです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 リング中央には秘密ですがそれは<unk> _<unk> therで一緒にしてClothingが飛んできました</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> <unk> octrine_<unk> ethod_<unk> ther。。。 ボディに打ち込んでいます。 ストレートネック <unk> octrine_<unk> ethod_<unk> therの上からでもしっかり手数を出して。</s>\n",
            "ガードの上から連打</s> <unk> octrine_<unk> ethod_<unk> ther。 リング中央には秘密ですがそれは<unk> _<unk> therで一緒にしてClothingが飛んできました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 リング中央には秘密ですがそれは<unk> _<unk> therで一緒にしてClothingが飛んできました</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ガードの上からでもしっかり手数を出して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上から連打</s> <unk> octrine_<unk> ethod_<unk> ther。 リング中央には秘密ですがそれは<unk> _<unk> therで一緒にしてClothingが飛んできました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj4_nF4yxt4q",
        "outputId": "ccf83c88-db81-44d0-d943-9e20d384fdf1"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたノックアウトです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "倒れたノックアウトです。</s> ここまで2勝1敗1分け、神田桃子。 今リングインです。 <unk> choolでは<unk> osition_<unk> ocationを務めてきました</s> </s>\n",
            "倒れたノックアウトです。</s> これで戦績は1勝6敗3分け。 いまだ価値がありません。 神田桃子34歳。 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> これこの試合の明暗を分けました。 何それ。 何それ。 今この<unk> ersonもパンチを返して行きました</s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> ここまで4.4勝日本拳法で培ったと乗り鉄左のボディにも入れてきましたが堂々としますね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> これで終わりましたか今のですね やっぱりそうですねはいそして最後のダウンのシーンです これが本当かっぽい<unk> portしてますよね</s>\n",
            "倒れたノックアウトです。</s> これで終了です判定は3体で落ち着いた試合展開を見せています<unk> です。 ここまでの試合手数では間違いなく<unk> ersonです</s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> これで終わりでしょうか。 これで戦績は1勝6敗2分け。 神田桃子34歳。 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> これで終わりましたか今のです やっぱりそうですねはいそして藤井ステーション江藤会長もかなり<unk> nimal_<unk> artを使っています</s> </s> </s>\n",
            "倒れたノックアウトです。</s> これで終わりましたか今のですね。 これ見てやっぱり<unk> erson<unk> itle_<unk> therの距離なんでしょうかね</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れたノックアウトです。</s> ここまでは<unk> osition_<unk> ocationに体を密着させられシーンが続いていますが一撃で試合を終わらせるを持ってようと思っています。</s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErD4cWrSyard",
        "outputId": "4fd77848-69b3-42be-9d79-2dd580c2a9c1"
      },
      "source": [
        "input = tokenizer.encode(\"倒れたKOです。\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "倒れた<unk> です。</s> ここは右のストレート。 これ右の<unk> octrine_<unk> ethod_<unk> ther打ってきました <unk> ersonです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> これ終了。 やっぱりもう10年ぐらいになる選手なので様々な経験をされていますしたりしたから追う者にとってはあの憧れる存在だと思います</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> ここは右のボディ。 やはり右のフックが当たっています。 倒れたまま、というのもありますよね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> そこに対して右のカウンターを狙っています。 相手の家終わりを狙っている強いていたミラクルハンマー。 その辺りがまあここから終盤にかけてポイントになってくるんですがどうでしょうね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> ここは右のボディ。 やはり入ってきたところに右を合わせる力はこのカウンターパンチですねそういったところが一つか気になってくる。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> 試合は終盤に入ってきます。第5ラウンドです。序盤からどんどんパンチを打っているのは<unk> です。 ただ、<unk> もパンチを返していきます。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> ここは右のストレート。 今リング中央です。 左のボディを打ち返していったのは<unk> です。 第6ラウンドが終了しました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> これ終了ですが。。。 やっぱりもう10年ぐらいになる選手なので様々な経験をされていますしたりしたから追う者にとってはあの憧れる存在だと思います</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "倒れた<unk> です。</s> これ序盤からどんどんパンチを打っているすごいデビュー戦になりますね。 キャリア49戦目、35歳の<unk> 、35歳の<unk> osition_<unk> ocationに勝ってどんどん自分のレベルを上げていくんだという風に語っていました。</s>\n",
            "倒れた<unk> です。</s> ここは右のボディ。 やはり右のフックが当たっています。 伊賀パンチが<unk> nimal_<unk> artを捉えるという神が増えてきましたね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjlq2F41zC2j",
        "outputId": "3707c27d-7f1a-4a9f-b0ea-0ff7384096d9"
      },
      "source": [
        "input = tokenizer.encode(\"右のフックから左右連打\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "右のフックから左右連打</s> これがデビュー戦。 ボディに打ち込んでいます。 リングは中央です。 第2ラウンド終了です。 先ほどの神です</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> これがデビュー戦です。 パンチをもらっているシーンもありましたが、ただ冷静に試合展開を進めているのは<unk> ersonですかね。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> しかしここは両者。 パンチラ工作シーンです。 左の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 <unk> ersonはガード上げて中に入り込もうとするところ右のボディ。</s>\n",
            "右のフックから左右連打</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 <unk> ersonはガード上げて中に入り込もうとするところ右のボディ。</s>\n",
            "右のフックから左右連打</s> おまけまあこれは来るわ<unk> nimal_<unk> art<unk> ature_Colorするよね答えありませんよね。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> すごいパンチが入りました。 第2ラウンド終了です。 ここまで激しい打ち合いになっている両者の戦い。 第2ラウンドです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> 。 ボディに打ち込んでいます。 ストレートネック 第4ラウンド終了です。 ここまで激しい打ち合いになっている両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> これがデビュー戦。 第3ラウンドは落ち着いた試合展開。 右のストレートパンチを打ちながら距離の測り愛となりましたら1ラウンドです</s> </s> </s> </s> </s> </s> </s>\n",
            "右のフックから左右連打</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 <unk> nimal_<unk> artを捉える<unk> erson</s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlzLienS-drA",
        "outputId": "8d1eb03f-f4e7-4079-b9ea-9fa4a6602352"
      },
      "source": [
        "input = tokenizer.encode(\"ガードの上からボディー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ガードの上からボディー</s> <unk> ersonは打ち負けていません 第2ラウンド終了です。 この大量何度もお互い手数を出していきました。 岩崎圭介は<unk> ersonの動きに動じずに自分のペースを掴んでパンチを当てられるかそう試合前か立っていました</s>\n",
            "ガードの上からボディー</s> <unk> ersonにしてみてもなかなか休めないさあ右の<unk> eaponを打ってきました 中に入っていけるかどうか<unk> ersonです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonです <unk> erson。 激しい打ち合いになっている。 右のジャブ。 ここはガードを固めています<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonでも動きを止められない<unk> erson。 中に入っていけるかどうか<unk> erson。 パンチを返してきたら中に入ってくる<unk> ersonに対して右のカウンターを狙っています</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonにしてきます。 中に入っていけるかどうか<unk> ersonです。 上下の打ち分け<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonです <unk> erson。 激しい打ち合いになっている。 距離が近くなってからその形の<unk> nimal_<unk> artのフック。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonにしてみます。 リングは中央です。 左のボディ降りていったところに右のボディ。 やはり相手も左のフックを警戒してるところがあるかもしれません</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonにしてきます。 中に入っていけるかどうか<unk> ersonです。 距離を詰めて距離を詰めていくのは守り<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonでも動き出されたら手数を増やしていきたいとそう語っていましたけれども打たれながらもどんどん前に出たい奴らです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ガードの上からボディー</s> <unk> ersonにしてみます。 中に入っていけるかどうか<unk> ersonです。 上下の打ち分け<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sQQtEbd-pXu",
        "outputId": "bb85b26d-f376-438a-e582-d2997aa9b9aa"
      },
      "source": [
        "input = tokenizer.encode(\"ジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ジャブ</s> この辺りは持ち味を十分というところです。ただ距離を取ってみると。。。 ジャブを打っていって、いきなり距離を詰めても次第に力の左のボディです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この辺りは持ち味を十分というところです。ただ距離を取ってみると。。。 <unk> ocation_<unk> therのフックを見せましたが大ちゃん残り10秒。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この<unk> roduct_<unk> therどうでしょうか? それとかよりも今少し後ろに体重が移動しました。 左の<unk> roduct_<unk> ther!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この<unk> roduct_<unk> therどうでしょうか? それとかよりも今少し後ろに体重が移動しました。 左の<unk> roduct_<unk> ther!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この<unk> roduct_<unk> therどうでしょうか? それとかよりも今少し後ろに体重が移動しました。 左の<unk> roduct_<unk> ther!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この<unk> roduct_<unk> therどうでしょうか? それとかよりも今少し後ろに体重が移動しました。 左の<unk> roduct_<unk> ther!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この辺りは持ち味を十分というところです。 ただ距離を取ってみると。 第3ラウンドは残り10秒です。 張り込んでここで第3ラウンド終了です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この辺りは持ち味を十分というところです。ただ距離を詰めていく<unk> です。 右のストレートか左のジャブを打ってきました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ジャブ</s> この辺りは持ち味を十分というところです。 ただ距離を詰めていく<unk> ersonです。 左のジャブから右の<unk> octrine_<unk> ethod_<unk> ther、これが一つ鍵になってきます。</s>\n",
            "ジャブ</s> この辺りは持ち味を十分というところです。 ただ距離を取ってみると。 第3ラウンドは残り10秒です。 張り込んでここで第3ラウンド終了です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW_A-HPXOvhl",
        "outputId": "f8f959d8-3068-4f18-90db-8734f6576197"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のジャブ</s> これがデビュー戦。 緊張はしているもののしっかり技術を持っているのは<unk> ersonです。 アマチュア戦績57.43勝14敗実績は十分にきゅうレビュー</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。落ち着いた試合展開を見せている<unk> です。 序盤からパンチを打っているんですが、最後の。。。 左の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。 デビュー戦とは思えない落ち着き。落ち着いた試合展開を見せています<unk> です。 序盤からパンチを打っていく<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。落ち着いた試合展開を見せている<unk> です。序盤からしっかり距離を取って、どんどんパンチを打っているのは<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。落ち着いた試合展開を見せている<unk> です。 序盤からパンチを打っているんですが、最後の。。。 左の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。 <unk> rovinceのCity出身24歳浜口良性です。 <unk> choolボクシング部から真正ジムで今日デビュー戦を迎えました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 ジャブの打ち合い。 右の<unk> octrine_<unk> ethod_<unk> ther!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めていくのは守り<unk> erson。 左のジャブから右の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s>\n",
            "左のジャブ</s> これがデビュー戦。落ち着いた試合展開を見せている<unk> です。 第3ラウンドです。 試合は中盤戦に入っていきます。 序盤の話にもありましたが、右の<unk> roduct_<unk> therのフックを見せました<unk> です。</s>\n",
            "左のジャブ</s> これがデビュー戦。 緊張はしているがどんどん前に出て行きたい。 ジャブの打ち合い。 右のストレートジーンズ</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2BheEMBO0Md",
        "outputId": "a3b463ef-eafa-4daf-f2aa-c263551ef790"
      },
      "source": [
        "input = tokenizer.encode(\"右のジャブ\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めていくのは<unk> ersonです。 左のジャブから右2で入れていく<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めていくのは<unk> ersonです。 まずは<unk> ersonですが、左のジャブを打って行きました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めるだけではなくて<unk> nimal_<unk> artはクールに行きたいんだけど話を完了していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めるだけではなくて<unk> nimal_<unk> artはクールに行きたいんだけど話を完了していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これが一つ<unk> ersonの形ではあります 距離を詰める<unk> ersonです <unk> ature_ColorClothingClothingそして<unk> ature_ColorClothingです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これが一つ<unk> ersonの形ではあります 距離を詰める<unk> ersonです <unk> ature_ColorClothingClothingそして<unk> ature_ColorClothingです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めるだけではなくて<unk> nimal_<unk> artはクールに行きたいんだけど話を完了していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めるだけじゃなくて<unk> nimal_<unk> artはクールに行きたいんだけど話を完了していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めるだけではなくて<unk> nimal_<unk> artはクールに行きたいんだけど話を完了していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のジャブ</s> これがここから突破口を音 <unk> ersonです。 ただ距離を詰めていくのは守り<unk> erson。 守り<unk> ersonはこれまでロングレンジでの攻防左のジャブを打って右の<unk> octrine_<unk> ethod_<unk> therへと展開していくこのボクシングです</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0lu5HoO28H",
        "outputId": "1a691dce-3679-43c5-d3f2-cfed6278f663"
      },
      "source": [
        "input = tokenizer.encode(\"左のジャブからワンツー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のジャブからワンツー</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 パンチをもらっているシーンもありましたが、ただ冷静に試合展開を進めているのは<unk> ですかね。</s>\n",
            "左のジャブからワンツー</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 序盤からパンチを打っているんですが、最後の。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> この辺りのパンチの交差点の残り香の魅力ですね。 ここは右のボディから左お互い別の種類の集中力っていうのは本当にありませんね</s>\n",
            "左のジャブからワンツー</s> この辺りのパンチの交差点の残り香の魅力ですね。 これですね。 さあ残り10秒です。</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 序盤からパンチを打っているんですが、最後の。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> この辺りのパンチの交差点の残り香の魅力ですね。 ここは右のボディから左<unk> ersonはこの左を狙ってますかあってますね</s> </s>\n",
            "左のジャブからワンツー</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 パンチをもらっているシーンもありましたが、ただ冷静に試合展開を進めているのは<unk> ですかね。</s>\n",
            "左のジャブからワンツー</s> これがデビュー戦。 落ち着いた試合展開を見せている<unk> です。 序盤からパンチを打っているんですが、最後の。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> ここはちょっと<unk> 気味で右が入りました。 この<unk> の右というのはちょっと速いですね。 ちょっと早いですね</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のジャブからワンツー</s> この辺りのパンチの交差点の残り香の魅力ですね。 第2ラウンドも残り10秒です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijuN8PUdPaik",
        "outputId": "87ee5d89-2ff7-45ce-c0d3-d7be9ffa5f6c"
      },
      "source": [
        "input = tokenizer.encode(\"左のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 第3ラウンドの後半。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ここまで激しい打ち合いになっている両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 ガードを固めていますので状態を揺らしてパンチを<unk> していく<unk> 。 ただここもパンチを返していくのは<unk> ersonです</s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終了です。 先ほどの神です この右のストレート中に入り込もうというところです。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "左のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbGej1nPuhc",
        "outputId": "cec46aba-987e-44ab-8652-6392b236262a"
      },
      "source": [
        "input = tokenizer.encode(\"右のアッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "右のアッパー</s> ここはちょっと<unk> 気味で右が入りました。 この<unk> の右というのはちょっと速いですね。 俺を仕留めてていきたいとイメージでしょうか</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 左のジャブから効果ボディを連打。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 左のジャブから効果ボディを連打。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 ガードを固めていますが。 ボディに打ち込んでいます。 ストレートネック 残り2秒。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 勝ったのは<unk> erson35歳です</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 第3ラウンドの後半にかけて、両者のパンチが交錯するというシーンがありました。</s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ここまで激しい打ち合いになっている両者の戦い。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 ボディに打ち込んでいく<unk> ersonです。 右のジャブから左の<unk> eaponへとつなげていきました</s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "右のアッパー</s> ここはボディに打ち込んでいきます。 リングは中央です。 第3ラウンド終わりました。 ちょっと様子見という第1ラウンドから第二ラウンド、様子見ながら拳を交えながら、というところがあります</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj7ohJTrRIeW",
        "outputId": "2a83efe2-b92b-470c-eba1-18dcd9a97510"
      },
      "source": [
        "input = tokenizer.encode(\"アッパー\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! 今リングイニです。 中村京太郎<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これも<unk> nimal_<unk> artをとらえた。 左のジャブから右の<unk> octrine_<unk> ethod_<unk> ther!</s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 左のボディ!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これも<unk> nimal_<unk> artをとらえた。 右のボディ!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! 今強いパンチが当たるそんなラウンドになりましたね。 第3ラウンドは少し静かな試合になりましたが。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これも<unk> nimal_<unk> artをとらえた。 左のジャブから右の<unk> octrine_<unk> ethod_<unk> ther!</s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! 今強いパンチが入りました。 ガードの上からでもしっかり手数を出して。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! 第6ラウンドは静かな戦いになっている依頼もそれとえらいね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これも<unk> nimal_<unk> artをとらえた。 右のボディ!</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "アッパー</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これも<unk> nimal_<unk> artをとらえた。 中に入っていきたい<unk> ersonです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtGG0ILqRVbY",
        "outputId": "18bf455b-8b8b-4e46-f848-22f28bee7b6c"
      },
      "source": [
        "input = tokenizer.encode(\"カウンター\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 いいですねサウスポー。 第3ラウンド終わりました。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 いいシャブが出ています。 右の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ガードを固めながら右の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ガードを固めて距離を詰めていくのは守り<unk> erson。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 いいシャブが出ています。 右の<unk> octrine_<unk> ethod_<unk> ther。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の息を切っています。 まこの試合に向けてますアマチュアでの実績は十分という<unk> 、ここまでは落ち着いた試合展開。 ただ序盤からどんどん手数を出して、自分の強みはスタミナだと語っていました</s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 第6ラウンドは静かな戦いになっている依頼もそれとえらいね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 伊賀パンチがまあこれは来るわ<unk> nimal_<unk> art<unk> ature_Colorするよね答えありませんよね。</s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 第6ラウンドは残り10秒です。 オリエン仕込んで行くナイスボディという声も上がる。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "カウンター</s> 右の<unk> octrine_<unk> ethod_<unk> ther少し体力的にもきつくなってきたところでしょうか。 あと一ラウンドなので、気合いで頑張ってほしいですね。</s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Piw5DZQ3UdGW",
        "outputId": "aff1ffea-d031-4486-bbe1-e8a3986194a3"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "試合展開</s> 中盤にかけて手数を増やしてきた<unk> ersonです。 ただここもフットワークを生かして体が入れ替わります。 左の<unk> octrine_<unk> ethod_<unk> ther。</s>\n",
            "試合展開</s> 中盤にかけて<unk> ersonが手数を出していく中、カウンターで未来の<unk> eaponを打ち返していく。 ただ、<unk> ersonもパンチを返していく。</s> </s> </s> </s>\n",
            "試合展開</s> 中盤にかけて<unk> ersonがダウンを覚えましたが大丈夫でしょうかでのパンチ集めて行きました。 一方の設備ですが念願の初勝利に向けて第1ラウンドの立ち上げになります</s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 ただここは細かいパンチボディ店<unk> 。</s> </s> </s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ここはガードを固めています<unk> です。 中に入っていきたい<unk> です。</s> </s> </s> </s> </s>\n",
            "試合展開</s> 中盤にかけて<unk> ersonがダウンを覚えましたが大丈夫でしょうかでのパンチ集めて行きました。 試合は終盤に入ってきます</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther! これがデビュー戦緊張はしているがどんどん前に出て行きたい。 その通りの試合展開になっています</s> </s> </s> </s> </s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 中に入っていきたい<unk> です。</s> </s> </s> </s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther。 ここはガードを固めています<unk> です。 中に入っていきたい<unk> です。</s> </s> </s> </s> </s>\n",
            "試合展開</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 中に入っていきたい<unk> です。</s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di_uAXTaYirD",
        "outputId": "c8ff3dd4-144d-4820-89a0-7bb344faf232"
      },
      "source": [
        "input = tokenizer.encode(\"試合展開について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはしっかり中に入っていきたい。 ガードを固めながら中に入ろうとするのは<unk> ersonです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> <unk> erson<unk> itle_<unk> therはこういう選手だったというのを今日の試合でやはりもう<unk> requency皆に分かってもらいたくないそういう気持ちで望んでいます</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 まず中に入っていこうという<unk> です。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここは少し<unk> nimal_<unk> artが止まってきたか<unk> osition_<unk> ocation。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> もう<unk> requency<unk> erson<unk> itle_<unk> therが話していたのは<unk> osition_<unk> ocationが増しているのはある程度では<unk> ersonには負けてないぞどんどんこのスピードで押していけ</s>\n",
            "試合展開について</s> 序盤の<unk> nimal_<unk> artのペイントプラスこの今のこのこの右からいきなり起こさないというのあったらもっとねあのいい展開にたかが<unk> osition_<unk> ocationにとって良い展開になっていますね</s> </s> </s> </s>\n",
            "試合展開について</s> もう<unk> requency<unk> erson<unk> itle_<unk> therが話していたのは<unk> osition_<unk> ocationが増しているのはある程度では<unk> ersonには負けてないぞどんどんこのスピードで押していけ</s>\n",
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはしっかり中に入っていきたい。 ガードを固めてるのは<unk> ersonです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! まず中に入っていこうというこのはなぶさんに対して右のジャムなどを送り出しながら距離を取ろうというこの下町たこの試合のを見て。</s> </s> </s> </s> </s>\n",
            "試合展開について</s> 右の<unk> octrine_<unk> ethod_<unk> ther! ここはガードを固めています<unk> です。 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS9RLYW2YehG",
        "outputId": "f4e67c80-804d-4639-bec1-657021eb349d"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシング\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ボクシング</s> グローブがこれがデビュー戦という<unk> erson22歳<unk> rovinceのCity出身です。 2017年に<unk> requencyプロのライセンスを取得しましたが仕事や家庭の事情で試合ができなかった</s>\n",
            "ボクシング</s> 35歳で6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています<unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳にして6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳にして6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳で6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 引退してからの<unk> port人生。 今振り返れば。 あの<unk> がこんなに勝つとは思っていませんでしたと本人は話していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳で6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳で6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳で6年ぶりの復帰戦とありましたがまぁ若いのある港にラウンド<unk> で敗れています <unk> erson<unk> itle_<unk> ther</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシング</s> 35歳で20年以上この<unk> portと向き合ってきた。 その<unk> が返す! これが若さの勢い! いいラッシュですね。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDqzn1wXWXro",
        "outputId": "7ffc9bff-dc77-487d-8734-dfa8e7def72b"
      },
      "source": [
        "input = tokenizer.encode(\"ボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ボクシングの試合について</s> <unk> erson<unk> itle_<unk> therがいない<unk> osition_<unk> ocationの試合展開とは違った落ち着いた左のジャブを見せているなという印象ですが第4ラウンドが始まりました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね ただ前に出ようとする少し明日から前に出ようとする<unk> ersonはどういった展開を見せるか残り10秒となって。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね 一年半ぶりの復帰戦ということになりますかね まあ その辺りのブランクについては<unk> erson試合前に話していました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね この試合に向けてますかね ただ一つ不安なのが体だけじゃなくて気持ちの強さを見せたいと話していた中でやはり序盤の立ち上がり左のジャブが当たるのかどうかというところ。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> <unk> erson<unk> itle_<unk> therがすごくいいボクシングを見せましたね。 すごい接近戦がうまいなと思います。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね 前に出た方がパンチを集める力っていうのはすごくある <unk> osition_<unk> ocationの<unk> erson<unk> itle_<unk> therはその元選手キャリアキャリアとかも好きだしすごい楽しみです。</s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですねすごい楽しみです。 まあでも相手がアマチュアキャリア豊富なんで、どこまで通用するのかなっていうのがちょっと、僕は見たいと思っています。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね この試合に向けてますかね ただやはりサウスポーてるところで距離感がまた大右というとこだ違うわけですよね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "ボクシングの試合について</s> そうですね 最初のこの。 やはり序盤から<unk> ishに買ってくれば勢いに乗ってないClothingの肉か対する逆さまな自らが来て数を出して主導権を握っていきたいと話していた中で思わぬ形で<unk> ish消し</s>\n",
            "ボクシングの試合について</s> そうですね 前に出た方がパンチが多いかなと思うんで <unk> ocation_<unk> therの動きながらじりじりと飛び込んでいくのは止めた方がいいですね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8DlV3bYWia9",
        "outputId": "40c8ad4b-1722-446d-92b7-e32bb15adc66"
      },
      "source": [
        "input = tokenizer.encode(\"今回のボクシングの試合について\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "今回のボクシングの試合について</s> 自分の<unk> port人生の中でも<unk> ank良かったので今回は<unk> で勝ちたいと語っていました<unk> ersonです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生の中でも<unk> ank良かったので、まあこの時は買って何とかならんち無理をしないというところが自分の強みだと語っていたんですが、序盤からどんどん自分の<unk> port人生の中でも<unk> ank良かったなという試合になっています。</s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生は<unk> requency負けなしだから絶対に負けられないという話でいました。 この試合に向けてます</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生は<unk> rovinceにかかっているんだと語っていた<unk> ersonですが序盤からどんどん自分のペースに持って行こうとする<unk> erson</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生の中でも<unk> ank良かったので今回は<unk> で勝ちたいという話をしていました。 その辺りの<unk> erson<unk> itle_<unk> therの気持ち何とか<unk> ature_Colorコーナーの<unk> erson<unk> itle_<unk> ther</s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生は間違いないですねという風に語っていました大場浩平です。 さあまず序盤から前に出ようというのは<unk> ersonの方も予想していたでしょうか</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生の中でも<unk> ank良かったので今回は絶対に<unk> で勝ちたいという話をしていました。 幼いです</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生は20年。 ただ<unk> portが好きでここまで続けてこられたのは<unk> ersonそしてこれから<unk> ersonを続けるためには勝たなければいけないとそういう風に語りました</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分のこれまでやってきたこと。そしてこれから挑戦していきたいことを話していました。 まずは序盤からしっかり自分の<unk> portを進めてきて、</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "今回のボクシングの試合について</s> 自分の<unk> port人生の中でも<unk> ank良かったのでこのまま続けられる。 そう語っていました<unk> ersonです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5QCrAIYJxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9e7e38-3fd4-427b-85dd-0c409f2d7b3c"
      },
      "source": [
        "input = tokenizer.encode(\"かなりのダメージを受けている\", return_tensors=\"pt\").to(device)\n",
        "output = model.generate(input, do_sample=True, top_k=40, top_p=0.95, temperature=0.7, min_length=32, max_length=256, num_return_sequences=10)\n",
        "# result = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "result = tokenizer.batch_decode(output)\n",
        "for ret in result:\n",
        "  print(ret)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 ただ第4ラウンドもパンチを出しているのは<unk> ersonです。</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから再起戦となる。 キャリア49戦目、35歳の<unk>! 負けよりかそれほど高くないんですが場所学校1年生から空手キックボクシング小学校5年から<unk> portを始めました</s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 ただ第4ラウンドもパンチを出しているのは<unk> ersonです ただ第4ラウンドは<unk> ersonもパンチを返して行きました</s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 そうですね巻き返していけるかどうかですね? だいぶダメージは効いていると思います。</s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 まずは<unk> ersonです。 ただこれまでの戦いはま復帰後の初勝利が絶対条件という風に語っていました</s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 パンチをもらっているシーンもありましたがどんどん前に出て行くね</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 パンチをもらっているシーンもありましたが、ただ冷静に試合展開を進めているのは<unk> ersonですかね。</s> </s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから再起戦となる。 キャリア49戦目、35歳の<unk>! 負ければ引退も視野に入ると話していました<unk> 、負けられない一戦が始まろうとしています。</s> </s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから後半戦に入っているところにつながった巻き返しを見せるんでしょうか。 ただ第4ラウンドもパンチを出しているのは<unk> ersonです 落ち着いて一発を狙っているのが<unk> erson。</s> </s> </s>\n",
            "かなりのダメージを受けている</s> ここから再起戦となる。 キャリア49戦目。 負けよりかそれほど高くないんですが場所学校1年生から空手キックボクシング小学校5年から<unk> portを始めました</s> </s> </s> </s> </s> </s> </s> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-W9pewlyv6x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}